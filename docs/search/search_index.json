{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Intel Retail Documentation","text":"<p>Welcome to the Intel Retail organization.</p>"},{"location":"index.html#learn-more-about-our-features","title":"Learn more about our features","text":"<p>Architecture</p> <p>Performance Tools</p> <p>Use Cases</p> <p>Releases</p>"},{"location":"releasenotes.html","title":"Releases","text":"<p>Release v1.0.1</p> <p>Release v1.5.0</p> <p>Release v2.0.0</p> <p>Release v2.1.0</p> <p>Release v3.0.0</p>"},{"location":"Architecture/pipelines.html","title":"Intel Retail","text":""},{"location":"Architecture/pipelines.html#repositories","title":"Repositories","text":"<p>In release v3.0.0 Intel-retail modules have been organized into logical repositories. By taking advantage of Github submodules different modules can be referenced from other repositories. For example, performance tools are being used by retail-use-cases and automated-self-checkout. Rather than duplicating and maintaining performance tools between the two repositories we linked the latest performance tools release as a submodule. </p> <p></p>"},{"location":"Architecture/pipelines.html#frameworks","title":"Frameworks","text":""},{"location":"Architecture/pipelines.html#openvino","title":"OpenVINO","text":"<p>OpenVINO is an open source toolkit provided by Intel to assist with running AI and ML on Intel hardware. The tools include a portable inference engine that is compatible with different Intel hardware platforms. The code can be found on the OpenVINO Github and examples can be ran with OpenVINO Jupyter Notebooks.</p> <p>OpenVINO provides some pre-trained models for quick development and testing through the OpenVINO Model Zoo. OpenVINO also supports converting models through they Model Conversion Process</p> <p>Details about the latest version can be found in the OpenVINO Release Notes.</p>"},{"location":"Architecture/pipelines.html#dlstreamer-pipeline","title":"DLStreamer Pipeline","text":"<p>Rather than working directly with the OpenVINO APIs our solutions offers more practical ways to interface with OpenVINO. One method is using Intel DLStreamer. This solution provides a no code way based on GStreamer and OpenVINO to deploy, process, and output a pipeline. </p> <p>The diagrams show how we take advantage of Docker, Docker Compose, and environment variable files to pre-package DLStreamer based pipelines for our use cases. Leveraging Environment Variables allows users to modify properties on the fly when different configurations are required.</p> <p></p>"},{"location":"Architecture/pipelines.html#openvino-model-serverovms","title":"OpenVINO Model Server(OVMS)","text":"<p>Another solution is the OpenVINO Model Server(OVMS). OVMS is a model hosting server that hosts inference models through a set of APIs. Unlike DLStreamer this solution requires developers to write code for pre and post processing model inference results. The advantage is the additional control developers have over their inference processing. Another benefit is the distribution of inference workloads between multiple servers either locally or remotely.</p>"},{"location":"Architecture/pipelines.html#openvino-model-serverovms-pipeline-with-grpc-remote-procedure-call","title":"OpenVINO Model Server(OVMS) Pipeline with gRPC (Remote Procedure Call)","text":"<p>There are two methods for running your inference through OVMS. The more flexible method has the client use gRPC (Remote Procedure Call) to request inference results from OVMS. By providing the proper input type and format the client can push inference compute to OVMS. OVMS can be local or on a remote system as long as the requested model is supported. This provides great flexibility with only minor latency increase. </p> <p>The gRPC interface supports c/c++, python, and go. A python example is located in our retail-use-cases gRPC python. The diagram show how the Docker Compose will deploy the client and OVMS. </p> <p></p>"},{"location":"Architecture/pipelines.html#openvino-model-serverovms-pipeline-with-c-api","title":"OpenVINO Model Server(OVMS) Pipeline with C API","text":"<p>When performance is more important than flexibility a developer can use the C API to bypass the gRPC interface and reduce latency. currently this method is only supported for c/c++ and required client/OVMS to both be deployed in a single Docker container. and example of a C API pipeline can be found in retail-use-cases gst_capi. Similar to DLStreamer the Docker Compose only launches a single container per pipeline now that the client/OVMS directly connect through the C API.</p> <p></p>"},{"location":"Architecture/pipelines.html#performance","title":"Performance","text":"<p>More details about benchmarking pipelines can be found on the Performance Tools Page.</p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html","title":"Distributed Architecture","text":"<ul> <li>Status</li> <li>Decision</li> <li>Context</li> <li>Proposed Design</li> <li>Consequences</li> <li>References</li> </ul>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#decision","title":"Decision","text":""},{"location":"Architecture/v2.0.0/distributed-architecture.html#context","title":"Context","text":"<p>To a wider variety of computer vision use cases in the real world a distributed inference architecture is required for deployment and scale. To achieve this, OpenVINO Model Server (OVMS) will be used for server side inferencing as part of the architecture design. The new architecture will lose some inference throughput but gain flexibility and scale.</p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#proposed-design","title":"Proposed Design","text":"<p>Using OpenVINO Model Server (OVMS) pipeline, workloads can be distributed between different services. For our solution a single system and remote server setup will be supported.</p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#single-system-setup","title":"Single System Setup","text":"<p>The single system solution will launch both the OVMS client and OVMS server on the same system as Docker containers. The local network can be used for communication between the Docker containers. The profile launcher program will load the profile configs and environment variables form a local data volume. The computer vision models will also be located on a local data volume. The models can be downloaded using the provided scripts or manually by the user.</p> <p></p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#remote-server-setup","title":"Remote Server Setup","text":"<p>The remote serve set will launch the same OVMS client and OVMS server containers but on two different systems. These systems can be on the same network on in remote locations as long as the systems can communicate through the network. This will require additional security or a direct connection from client to server. Similar to the single system the profile launcher will load the profile configs and environment variables from a data volume. In this case the data volume can be a local copy or a remote copy of those files. On the server the computer vision models will be in a data volume. Unlike the profile config and environment files these must be located on the server in a data volume. This is to prevent any unwanted changes to the computer vision model when it is located in a remote location. </p> <p></p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#running-the-pipeline","title":"Running the Pipeline","text":"<p>The profile launching program will start a pre-configured OVMS client and OVMS server. Run Pipeline documentation covers the parameter details and how to configure different input sources.</p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#applicable-repos","title":"Applicable Repos","text":"<p>automated-self-checkout</p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#consequences","title":"Consequences","text":"<p>Unlike DLStreamer there will be some latency to call OVMS through gRPC. This will results in a slightly lower stream density for systems. We will however support a wider range and combination of models since the inferencing will be abstracted into the OpenVINO Model Server.</p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#references","title":"References","text":"<p>https://docs.openvino.ai/2023.0/ovms_what_is_openvino_model_server.html https://github.com/openvinotoolkit/model_server</p>"},{"location":"Architecture/v2.0.0/multiple-ovms-json-config.html","title":"Multiple OpenVINO Model Server Config JSON","text":"<ul> <li>Status</li> <li>Decision</li> <li>Context</li> <li>Proposed Design</li> <li>Consequences</li> <li>References</li> </ul>"},{"location":"Architecture/v2.0.0/multiple-ovms-json-config.html#decision","title":"Decision","text":""},{"location":"Architecture/v2.0.0/multiple-ovms-json-config.html#context","title":"Context","text":"<p>Currently, we use same config.json for all instances of OpenVINO Model Server(OVMS) pipelines, which leads to some issue regarding the device mounting for OVMS server: see issue intel-retail/automated-self-checkout#322. So we need a way to have multiple or unique config json file per OVMS instance.</p>"},{"location":"Architecture/v2.0.0/multiple-ovms-json-config.html#proposed-design","title":"Proposed Design","text":"<p>Move the current device update logic from <code>run.sh</code> with the config.json file into profile-launcher in Golang. When profile-launcher about to launch a new instance of OVMS server, it then produces a unique config json file name for that instance of OVMS server.</p> <p>For example, we can use the Docker container name of that OVMS server like ovms_server0, or ovms_server1,...etc to be appended into the config json as part of the file name (e.g. config_ovms_server0.json).</p> <p>One example golang code for updating json file target_device and producing a new config.json is shown below: <pre><code>// ----------------------------------------------------------------------------------\n// Copyright 2023 Intel Corp.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n//  Unless required by applicable law or agreed to in writing, software\n//  distributed under the License is distributed on an \"AS IS\" BASIS,\n//  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n//  See the License for the specific language governing permissions and\n//  limitations under the License.\n//\n// ----------------------------------------------------------------------------------\n\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"log\"\n    \"os\"\n    \"reflect\"\n)\n\ntype OvmsConfig struct {\n    ModelList []ModelConfig `json:\"model_config_list\"`\n}\n\ntype ModelConfig struct {\n    Config map[string]interface{} `json:\"config\"`\n}\n\nfunc main() {\n    updateConfig(\"CPU\")\n}\n\nfunc updateConfig(device string) {\n    contents, err := os.ReadFile(\"config_template.json\")\n    if err != nil {\n        err = fmt.Errorf(\"Cannot read json config %v\", err)\n    }\n\n    var data OvmsConfig\n    err = json.Unmarshal(contents, &amp;data)\n    if err != nil {\n        log.Fatalf(\"failed to unmarshal configuration file configuration.yaml: %v\", err)\n    }\n\n    fmt.Println(reflect.TypeOf(data.ModelList))\n\n    for _, model := range data.ModelList {\n        // fmt.Println(model)\n        model.Config[\"target_device\"] = device\n        fmt.Println(model.Config[\"target_device\"])\n        fmt.Println(\"!!!!!!!!!!!!\")\n        fmt.Println(model.Config)\n    }\n\n    // convert to struct\n    updateConfig, err := json.Marshal(data)\n    if err != nil {\n        log.Fatalf(\"could not marshal config to JSON: %v\", err)\n    }\n    _ = os.WriteFile(\"config_ovms_server0.json\", updateConfig, 0644)\n}\n</code></pre></p> <p>This step is done before the profile-launcher calling the <code>start_ovms_server.sh</code>script.</p> <p>In the profile-launcher we also set the correct  environment variable values for the <code>start_ovms_server.sh</code>script to use.  For example, <code>OVMS_MODEL_CONFIG_JSON</code> to be the unique config json file name that was produced from the above example.</p> <p>For clean-up, we can do deletion of the config json files when <code>make clean-all</code> is called or <code>make clean-ovms-server</code> is called.</p>"},{"location":"Architecture/v2.0.0/multiple-ovms-json-config.html#references","title":"References","text":"<ul> <li>https://docs.openvino.ai/2023.0/ovms_what_is_openvino_model_server.html</li> <li>https://github.com/openvinotoolkit/model_server</li> <li>see issue Classification profile crashed when run the 2nd instance switch from CPU to GPU.0 automated-self-checkout#322</li> </ul>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html","title":"Performance Benchmarking","text":"<ul> <li>Status</li> <li>Decision</li> <li>Context</li> <li>Proposed Design</li> <li>Consequences</li> <li>References</li> </ul>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#decision","title":"Decision","text":""},{"location":"Architecture/v2.0.0/performance_benchmarking.html#context","title":"Context","text":"<p>To assist customers we will provide a set of performance Docker containers to measure the performance of their pipelines. The performance Docker containers will need to be supported on most modern Intel hardware. The output will also need to be formatted and presented to customers as a hardware recommendation.</p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#proposed-design","title":"Proposed Design","text":""},{"location":"Architecture/v2.0.0/performance_benchmarking.html#benchmark-script","title":"Benchmark Script","text":"<p>The benchmark script is designed to help determine the performance needs for a specific pipeline profile. The script will run a designated pipeline profile and can either replicate that pipeline profile a specific number of times or continue to replicate until a performance target is reached.</p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#performance-tools","title":"Performance tools","text":"<p>sysstat: System CPU utilization free: System memory usage iotop: System Disk read and write data igt-gpu-tools: Integrated GPU utilization Intel XPU Manage: Discrete GPU utilization Intel Performance Counter Monitor: System power usage</p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#input-source-types","title":"Input Source Types","text":"<p>For performance inputs we support RTSP video streams, USB camera, Intel\u00ae RealSense\u2122 Camera, and video files. For longer benchmarking runs its' recommended to use a video loop with an RTSP stream for inference result consistency. As an option an RTSP Camera Simulator is provided with the performance script.</p> <p>Input Source Types</p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#specified-number-of-pipelines","title":"Specified Number of Pipelines","text":"<p>If you are looking to test a specific number of pipelines on different hardware SKUs the <code>--pipelines</code> parameter can be used. This parameter will start the specified number of pipelines </p> <p>Specified Number of Pipelines</p> <p></p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#consolidated-results","title":"Consolidated Results","text":"<p>To make reading results easier, a consolidation script has been provided. This script will work with a single or multiple runs of the specified number of pipelines. Details about this process are found in Benchmark Specified Number of Pipelines</p> <pre><code>make consolidate ROOT_DIRECTORY=&lt;output dir&gt;\n</code></pre>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#stream-density","title":"Stream Density","text":"<p>The stream density parameter can be used to find the maximum number of pipelines at a target frames per second (FPS) on a specific hardware SKU. By setting the <code>--stream_density</code> parameter to the desired FPS the script will continue to create pipelines until the average pipelines FPS falls below the desired FPS. The script will provide a detailed log to show each pipeline FPS during the test run. This option provides a method for testing the top performance when introducing a new pipeline or hardware SKU.</p> <p>Stream Density</p> <p></p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#applicable-repos","title":"Applicable Repos","text":"<p>automated-self-checkout</p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#consequences","title":"Consequences","text":"<p>Having a generic and scalable set of performance Docker containers will allow customers to test a wide range of pipelines and hardware setups without extensive configuration of their systems. The flexibility will bring faster time to market and better hardware decision making by customers.</p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#references","title":"References","text":"<p>Pipeline Benchmarking</p>"},{"location":"Architecture/v2.0.0/profile-launcher.html","title":"Distributed Architecture","text":"<ul> <li>Status</li> <li>Decision</li> <li>Context</li> <li>Proposed Design</li> <li>Consequences</li> <li>References</li> </ul>"},{"location":"Architecture/v2.0.0/profile-launcher.html#decision","title":"Decision","text":""},{"location":"Architecture/v2.0.0/profile-launcher.html#context","title":"Context","text":"<p>Depending on the underlying pipeline architecture you may only require one Docker container or you may require many Docker containers. Specifically OVMS has two methods for running: gRPC which uses remote inference calls from a client to server and Capi which does the inferencing locally. Additionally other methods such as GStreamer are run in a single container. The profiles should be able to accommodate both use cases.</p>"},{"location":"Architecture/v2.0.0/profile-launcher.html#proposed-design","title":"Proposed Design","text":"<p>Update the profile to handle an array of configurations. This will allow the user to mix multiple Docker container configurations into a single profile.</p> <p>Each container configuration will contain the following information: - Docker image: The profile launcher will use as the target image - Environment file: Loaded into the container - Entrypoint script: Launch the desired start process - Input arguments: container or entrypoint script - Docker Volumes: Mounted to the container - Docker Networks: Connected to the container</p> <p></p>"},{"location":"Architecture/v2.0.0/profile-launcher.html#single-container-profile","title":"Single Container Profile","text":"<p>A single container profile will run a single Docker image using a single entrypoint script. This use case will be for pipelines that are self contained in a single container. Although the container can interact with other containers on the system the performance tools will only measure the performance of the single running container.</p> <p></p>"},{"location":"Architecture/v2.0.0/profile-launcher.html#multiple-container-profile","title":"Multiple Container Profile","text":"<p>A multiple container profile will run the array of containers defined in the profile config. Each container can have it's own entrypoint script even if they utilize the same base Docker image. The common profile will be the OpenVINO Model Server and client. In this case a OVMS container will contain the inference models defined in the config.json from the profile. Once the OVMS container is started the client will be launched and connect to the OVMS container. This will result in the inference workload being executed in a difference service which can be on the local system or in a remote location. </p> <p></p>"},{"location":"Architecture/v2.0.0/profile-launcher.html#applicable-repos","title":"Applicable Repos","text":"<p>automated-self-checkout</p>"},{"location":"Architecture/v2.0.0/profile-launcher.html#consequences","title":"Consequences","text":"<p>All profiles will need to be updated to use this new array structure. As a benefit common containers such as the OpenVINO Model Server can be shared between profiles.</p>"},{"location":"Architecture/v2.0.0/profile-launcher.html#references","title":"References","text":"<p>https://docs.openvino.ai/2023.0/ovms_what_is_openvino_model_server.html https://github.com/openvinotoolkit/model_server</p>"},{"location":"Architecture/v2.0.0/target-device.html","title":"Distributed Architecture","text":"<ul> <li>Status</li> <li>Decision</li> <li>Context</li> <li>Proposed Design</li> <li>Consequences</li> <li>References</li> </ul>"},{"location":"Architecture/v2.0.0/target-device.html#decision","title":"Decision","text":""},{"location":"Architecture/v2.0.0/target-device.html#context","title":"Context","text":"<p>The platform parameter is inconsistent with the target device being used in the pipeline. To be consistent with OVMS we want to use the target_device parameter in the script to match the target_device setting in the OVMS config file.</p>"},{"location":"Architecture/v2.0.0/target-device.html#proposed-design","title":"Proposed Design","text":"<p>Update the platform parameter to match the target_device standard used by OpenVINO Model Server. This will provide clarity to the device being used for the inferencing portion of the pipeline. The following are the acceptance criteria for the change.</p> <ul> <li>Replace platform parameter with target_device using CPU as the default device.</li> <li>Update the docker_run script to make it run with minimal changes to the profiles.</li> <li>Confirm that the benchmark script works with the target_device parameter update.</li> <li>Update unit tests</li> <li>Update documentation</li> <li>Convert $DEVICE to $TARGET_DEVICE for internal environment variables.</li> <li>Add option to use existing config file and not override all target_devices to support models with different target_device values.</li> </ul>"},{"location":"Architecture/v2.0.0/target-device.html#target-device-list","title":"Target Device list","text":"Device Parameter Description Links CPU CPU Use CPU only OVMS Parameters GPU GPU Use default GPU OVMS Parameters Specified GPU GPU.x Use a specific GPU. ex. GPU.0 = integrated GPU, GPU.1 = discrete Arc GPU OVMS Parameters Mixed Contifuration MULTI:x,y Use a combination of devices for inferencing ex. MULTI:CPU,GPU.1 will use the CPU and discrete Arc GPU for inferencing OVMS Parameters Automatic Device Selection AUTO Allow OpenVINO to automatically select the optimal device for inferencing Possibly depricated? Automatic Device Selection AUTO Allow OpenVINO to automatically select the optimal device for inferencing Possibly depricated? Heterogeneous Execution HETERO Allows OpenVINO to execute inference on multiple devices Heterogeneous Execution Heterogeneous Execution Priority HETERO:x,y Allows OpenVINO to execute inference on multiple devices and set the priority of device. ex. HETERO:CPU,GPU.1 will prioritize CPU and discrete Arc GPU for inferencing Heterogeneous Execution"},{"location":"Architecture/v2.0.0/target-device.html#applicable-repos","title":"Applicable Repos","text":"<p>automated-self-checkout</p>"},{"location":"Architecture/v2.0.0/target-device.html#consequences","title":"Consequences","text":"<p>Removing the platform parameter will break any existing test and benchmark scripts. The change will clarify which device you are targeting for the inference.</p>"},{"location":"Architecture/v2.0.0/target-device.html#references","title":"References","text":"<p>https://docs.openvino.ai/2023.0/ovms_what_is_openvino_model_server.html https://github.com/openvinotoolkit/model_server</p>"},{"location":"performance-tools/api-docs.html","title":"API Documentation for Benchmark Scripts","text":""},{"location":"performance-tools/api-docs.html#benchmark-package","title":"Benchmark Package","text":"<ul> <li>Copyright (C) 2024 Intel Corporation. *</li> <li>SPDX-License-Identifier: Apache-2.0</li> </ul>"},{"location":"performance-tools/api-docs.html#benchmark.convert_csv_results_to_json","title":"<code>convert_csv_results_to_json(results_dir, log_name)</code>","text":"<p>convert the csv output to json format for readability</p> <p>Parameters:</p> Name Type Description Default <code>results_dir</code> <p>directory containing the benchmark results</p> required <code>log_name</code> <p>first portion of the log filename to search for</p> required"},{"location":"performance-tools/api-docs.html#benchmark.docker_compose_containers","title":"<code>docker_compose_containers(command, compose_files=[], compose_pre_args='', compose_post_args='', env_vars=os.environ.copy())</code>","text":"<p>helper function to bring up or down containers using the provided params</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <p>valid docker compose command like \"up\" or \"down\"</p> required <code>compose_files</code> <p>list of docker compose files</p> <code>[]</code> <code>compose_pre_args</code> <p>string of arguments called before the command</p> <code>''</code> <code>compose_post_args</code> <p>string of arguments called after the command</p> <code>''</code> <code>env_vars</code> <p>environment variables to use in the shell for calling compose</p> <code>copy()</code> <p>Returns:</p> Name Type Description <code>stdout</code> <p>console output from Popen when running the command</p> <code>stderr</code> <p>console error from Popen when running the command</p> <code>returncode</code> <p>Popen return code</p>"},{"location":"performance-tools/api-docs.html#benchmark.main","title":"<code>main()</code>","text":"<p>runs benchmarking using docker compose for the specified pipeline</p>"},{"location":"performance-tools/api-docs.html#benchmark.parse_args","title":"<code>parse_args(print=False)</code>","text":"<p>parses the input arguments for the command line</p> <p>Parameters:</p> Name Type Description Default <code>print</code> <p>boolean on whether to print the help or not</p> <code>False</code> <p>Returns:</p> Name Type Description <code>None</code> <p>if print is True</p> <code>parser_object</code> <p>if the input arguments are parsed</p>"},{"location":"performance-tools/benchmark.html","title":"Computer Vision Pipeline Benchmarking","text":"<p>The provided Python-based script works with Docker Compose to get pipeline performance metrics like video processing in frames-per-second (FPS), memory usage, power consumption, and so on.</p>"},{"location":"performance-tools/benchmark.html#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Python environment v3.12.2</p> <p>Note</p> <p>This could be accomplished using Miniconda and creating a Python 3.12.2 env</p> </li> <li> <p>Python packages listed in performance-tools/benchmark-scripts/requirements.txt</p> <p>Note</p> <p>Use <code>pip install -r requirements.txt</code></p> </li> <li> <p>Docker</p> </li> <li>Docker Compose</li> <li>Make</li> <li>Git</li> <li> <p>Code from Retail Use Cases Repo and its submodule Performance Tools Repo</p> <p>Note</p> <p>To install the submodule, run <code>make update-submodules</code> from the root of the retail-use-cases repo.</p> </li> </ul>"},{"location":"performance-tools/benchmark.html#benchmark-a-cv-pipeline","title":"Benchmark a CV Pipeline","text":"<ol> <li>Build the benchmark container and change into the benchmark-scripts directory.</li> </ol> <p><pre><code>make build-benchmark-docker\ncd benchmark-scripts\n</code></pre> 2. Choose a CV pipeline from the Retail Use Cases Repo and note the file paths to the docker compose files. 3. Run the benchmarking script using the docker compose file(s) as inputs to the script (sample command shown below).</p> <pre><code>```bash\npython benchmark.py --compose_file ../../use-cases/gst_capi/add_camera-simulator.yml --compose_file ../../use-cases/gst_capi/add_gst_capi_yolov5_ensemble.yml\n```\n</code></pre> <p>Specific number of pipelines with single container </p> <p>Specific number of pipelines with OVMS and Client </p>"},{"location":"performance-tools/benchmark.html#benchmark-stream-density-for-cv-pipelines","title":"Benchmark Stream Density for CV Pipelines","text":"<p>Benchmarking a pipeline can also discover the maximum number of workloads or streams that can be run in parallel for a given target FPS. This information is useful to determine the hardware required to achieve the desired performance for CV pipelines.</p> <p>To run the stream density functionality use <code>--target_fps</code> and/or <code>--density_increment</code> as inputs to the <code>benchmark.py</code> script:</p> <pre><code> python benchmark.py  --retail_use_case_root ../../retail-use-cases --target_fps 14.95 --density_increment 1 --init_duration 40   --compose_file ../../retail-use-cases/use-cases/grpc_python/docker-compose_grpc_python.yml\n</code></pre> <p>where the parameters: - <code>target_fps</code> is the given target frames per second (fps) to achieve for maximum number of pipelines - <code>density_increment</code> is to configure the benchmark logic to increase the number of pipelines each time while trying to find out the maximum number of pipelines before reaching the given target fps. - <code>init_duration</code> is the initial duration period in second before pipeline performance metrics are taken</p> <pre><code>!!! Note\n    It is recommended to set --target_fps to a value lesser than your target FPS to account for real world variances in hardware readings.\n</code></pre> <p>Stream density with single container </p> <p>Stream density with OVMS and Client </p>"},{"location":"performance-tools/benchmark.html#modifying-additional-benchmarking-variables","title":"Modifying Additional Benchmarking Variables","text":""},{"location":"performance-tools/benchmark.html#change-power-profile","title":"Change Power Profile","text":"<ul> <li>For Ubuntu, follow this documentation to change the power profile.</li> <li>For Windows, follow this documentation to change the power mode.</li> </ul>"},{"location":"performance-tools/benchmark.html#developer-resources","title":"Developer Resources","text":""},{"location":"performance-tools/benchmark.html#python-testing","title":"Python Testing","text":"<p>To run the unit tests for the performance tools:</p> <pre><code>cd benchmark-scripts\nmake python-test\n</code></pre> <p>To run the unit tests and determine the coverage:</p> <pre><code>cd benchmark-scripts\nmake python-coverage\n</code></pre>"},{"location":"release-notes/v1-0-1.html","title":"1.0.1","text":"<p>Intel\u00ae Automated Self-Checkout Reference Package 1.0.0 is the first major release. This release includes all items required to run the vision checkout pipeline and benchmarking. For details on running the solution, refer to the Overview. </p>"},{"location":"release-notes/v1-0-1.html#new-features","title":"New Features","text":"Title Description Pipeline scripts Scripts that run the GStreamer-based vision checkout pipeline Benchmark scripts Scripts that start pipelines and system metrics based on parameters Docker* images Dockerized images for the pipeline and benchmark tools for code portability Set up Documentation Markdown files that include setup steps for initial use Unit tests Basic unit tests scripts for smoke testing Camera simulator Camera simulator script to simulate an RTSP stream using a media file Media downloader script Script to assist with downloading sample media for the camera simulator Model downloader script Script to assist with downloading the model files used for the pipelines"},{"location":"release-notes/v1-0-1.html#issues-fixed","title":"Issues Fixed","text":"Issue Number Description Link None Initial Release"},{"location":"release-notes/v1-0-1.html#known-issues","title":"Known Issues","text":"Issue Number Description Link 15 Pipeline core run on some HW is not producing inference results https://github.com/intel-retail/vision-self-checkout/issues/15 29 Unable to modify batch size from run script https://github.com/intel-retail/vision-self-checkout/issues/29"},{"location":"release-notes/v1-5-0.html","title":"1.5.0","text":"<p>Intel\u00ae Automated Self-Checkout Reference Package 1.5.0 is the second main release. This release includes bug fixes, feature enhancements, dockerization of the benchmarking tools, and OpenVINO Model Server support. For details on running the solution, refer to the Overview. </p>"},{"location":"release-notes/v1-5-0.html#new-features","title":"New Features","text":"Title Description OpenVINO Model Server OpenVINO Model Server support OpenVINO Model Server Pipelines Object detection pipelines using OpenVINO Model Server Benchmark scripts Dockerization Benchmark tools have been moved to Docker containers for more flexible deployment Github Build actions Code linting and security scans for pull requests"},{"location":"release-notes/v1-5-0.html#issues-fixed","title":"Issues Fixed","text":"Issue Number Description 41 Pipeline failure log 42 Create makefile docker commands 51 Optimized density script to reduce run time on high powered systems 55 Make performance / powersave mode configurable 57 Add debug option for docker-run.sh 58 Doc update with makefile 61 rename vision self checkout to automated self checkout 65 Update documentation to include OVMS pipelines 66 Add model download top level script 67 [Tech Debt] Make --workload work in any option/argument position when run benchmark.sh 75 docker-run.sh with wrong message when no --workload option is provided 77 XPU Manager not running on multiple GPUs 85 Fix ShellCheck issues in scripts 88 Incorrect instructions for building IGT in pipelinebenchmarking.md 91 format avc mp4 tag logic is inverted 96 For ovms workload getModels.sh not working when it is called by docker-run.sh from project base directory 99 Clean up some checked in dlstreamer models 100 Add cleaning ovms containers to makefile 105 benchmark pcm directory incorrect 109 igt path pointing to the incorrect directory causing the igt log to not be written 112 make CPU as default device for ovms pipeline 115 add dockerfile.bitModel to download bit models 119 pipelinesetup doc has incorrect link to models.list.yml 124 add ovms sample image download into run script 129 Update License to Apache 2.0 131 update mkdoc to navigate to OVMS doc 142 make build-ovms-server failed for 2nd time or later after removed the Docker image for rebuild"},{"location":"release-notes/v1-5-0.html#known-issues","title":"Known Issues","text":"Issue Number Description Link None"},{"location":"release-notes/v2-0-0.html","title":"2.0.0","text":"<p>Intel\u00ae Automated Self-Checkout Reference Package 2.0.0 is the next major release. This release includes bug fixes, feature enhancements, expansion of the OpenVINO Model Server use cases, implementation of gRPC and C API OVMS pipelines. For details on running the solution, refer to the Overview. </p>"},{"location":"release-notes/v2-0-0.html#new-features","title":"New Features","text":"Title Description OpenVINO Model Server gRPC OpenVINO Model Server support OpenVINO Model Server C API OpenVINO Model Server C API example Github Integration Test Action Nightly integration test action Docker Compose Pipeline Docker Compose version of the pipeline"},{"location":"release-notes/v2-0-0.html#issues-fixed","title":"Issues Fixed","text":"Issue Number Description 2.0 Issues Closed Github issues closed in the 2.0 release"},{"location":"release-notes/v2-0-0.html#known-issues","title":"Known Issues","text":"Issue Number Description Link None"},{"location":"release-notes/v2-1-0.html","title":"2.1.0","text":"<p>Intel\u00ae Automated Self-Checkout Reference Package 2.1.0 is minor maintenance release. This release includes bug fixes, feature enhancements, and a new yolov8 with efficientnet profile using OVMS C API. For details on running the solution, refer to the Overview. </p>"},{"location":"release-notes/v2-1-0.html#new-features","title":"New Features","text":"Title Description Yolov8 + Efficientnet C API Profile OVMS based yolov8 + efficientnet profile Sample video indexing Sample video indexing added for camera simulator container name consistency Batch size param for DLStreamer Add bach size parameter for DLStreamer profiles"},{"location":"release-notes/v2-1-0.html#issues-fixed","title":"Issues Fixed","text":"Issue Number Description 2.1 Issues Closed Github issues closed in the 2.1 release"},{"location":"release-notes/v2-1-0.html#known-issues","title":"Known Issues","text":"Issue Number Description Link None"},{"location":"release-notes/v3-0-0.html","title":"3.0.0","text":"<p>Intel\u00ae Retail organization 3.0.0 is major release. The release splits the Automated Self Checkout reference solution into multiple repositories for improved maintainability and scalability.</p>"},{"location":"release-notes/v3-0-0.html#new-features","title":"New Features","text":"Title Description new repositories Documentation Documentation, architecture, and requirements for Intel\u00ae retail repositories Automated Self Checkout Automated self checkout use case Retail Use Cases Retail use cases using DLStreamer and OpenVINO Model Server Performance Tools Performance tools for pipeline benchmarking"},{"location":"release-notes/v3-0-0.html#issues-fixed","title":"Issues Fixed","text":"Issue Number Description 3.0.0 Issues Closed Github issues closed in the 3.0.0 release"},{"location":"release-notes/v3-0-0.html#known-issues","title":"Known Issues","text":"Issue Number Description Link None"},{"location":"use-cases/use-cases.html","title":"Intel Retail Use Cases","text":"<p>Automated Self Checkout</p>"},{"location":"use-cases/automated-self-checkout/advanced.html","title":"Advanced Settings","text":""},{"location":"use-cases/automated-self-checkout/advanced.html#applying-environment-variablesev-to-run-pipeline","title":"Applying Environment Variables(EV) to Run Pipeline","text":"<p>EV can be applied in two ways:</p> <pre><code>1. As a Docker Compose environment parameter input \n2. In the env files\n</code></pre> <p>The input parameter will override the one in the env files if both are used.</p>"},{"location":"use-cases/automated-self-checkout/advanced.html#run-with-custom-environment-variables","title":"Run with Custom Environment Variables","text":"<p>Environment variables with make commands</p> <p>Example</p> <pre><code>make PIPELINE_SCRIPT=yolov5s_effnetb0.sh RESULTS_DIR=\"../render_results\"  run-render-mode\n</code></pre> <p>Environment variable with docker compose up</p> <p>Example</p> <pre><code>PIPELINE_SCRIPT=yolov5s_effnetb0.sh RESULTS_DIR=\"../render_results\" docker compose -f src/docker-compose.yml --env-file src/res/yolov5-cpu.env up -d\n</code></pre> <p>Note</p> <pre><code>The environment variables set like this are known as command line environment overrides and are applied to this run only.\nThey will override the default values in env files and docker-compose.yml.\n</code></pre>"},{"location":"use-cases/automated-self-checkout/advanced.html#editing-the-environment-files","title":"Editing the Environment Files","text":"<p>Environment variable files can be used to persist environment variables between deployments. There are three environment variables files with our default environment variables for Automated Self Checkout.</p> <pre><code>    - `src/gst.env` file for shared environment variables\n    - `src/yolov5-cpu.env` file for running pipeline on cpu only\n    - `src/yolov5-gpu.env` file for running pipeline in gpu or mixed CPU/GPU\n</code></pre> <p>After modifying or creating a new .env file you can load the .env file through docker compose up</p> <p>Example</p> <pre><code>docker compose -f src/docker-compose.yml --env-file src/res/yolov5-cpu.env up -d\n</code></pre>"},{"location":"use-cases/automated-self-checkout/advanced.html#environment-variables-evs","title":"Environment Variables (EVs)","text":"<p>The table below lists the environment variables (EVs) that can be used as inputs for the container running the inferencing pipeline.</p> Docker Compose EVsDocker Compose ParametersCommon EVsAutomated Self Checkout DLStreamer EVs <p>This list of EVs is for running through the make file or docker compose up</p> Variable Description Values <code>DEVICE_ENV</code> Path to device specific environment file that will be loaded into the pipeline container src/res/yolov5-gpu.env <code>DEVICE</code> for setting device to use for pipeline run \"GPU\", \"CPU\", \"AUTO\", \"MULTI:GPU,CPU\" <code>DOCKER_COMPOSE</code> The docker-compose.yml file to run src/docker-compose.yml <code>RETAIL_USE_CASE_ROOT</code> The root directory for Automated Self Checkout in relation to the docker-compose.yml .. <code>RESULTS_DIR</code> Directory to output results ../results <p>This list of parameters that can be set when running docker compose up</p> Variable Description Values <code>-v</code> Volume binding for containers in the Docker Compose -v results/:/tmp/results <code>-e</code> Override environment variables inside of the Docker Container -e LOG_LEVEL debug <p>This list of EVs is common for all profiles.</p> Variable Description Values <code>BARCODE_RECLASSIFY_INTERVAL</code> time interval in seconds for barcode classification Ex: 5 <code>BATCH_SIZE</code> number of frames batched together for a single inference to be used in gvadetect batch-size element 0, 1 <code>CLASSIFICATION_OPTIONS</code> extra classification pipeline instruction parameters \"\", \"reclassify-interval=1 batch-size=1 nireq=4 gpu-throughput-streams=4\" <code>DETECTION_OPTIONS</code> extra object detection pipeline instruction parameters \"\", \"gpu-throughput-streams=4 nireq=4 batch-size=1\" <code>GST_DEBUG</code> for running pipeline in gst debugging mode 0, 1 <code>LOG_LEVEL</code> log level to be set when running gst pipeline ERROR, INFO, WARNING, and more <code>OCR_RECLASSIFY_INTERVAL</code> time interval in seconds for OCR classification Ex: 5 <code>RENDER_MODE</code> for displaying pipeline and overlay CV metadata 1, 0 <code>PIPELINE_COUNT</code> Number of Automated Self Checkout Docker container instances to launch Ex: 1 <code>PIPELINE_SCRIPT</code> Pipeline script to run. yolov5s.sh, yolov5s_effnetb0.sh, yolov5s_full.sh <p>This list of EVs specifically supports the GST profile DLStreamer workloads.</p> Variable Description Values <code>DECODE</code> decoding element instructions for gst-launch to use Ex: \"decode bin force-sw-decoders=1\" <code>OCR_DEVICE</code> optical character recognition device \"CPU\", \"GPU\" <code>PRE_PROCESS</code> pre process command to add for inferencing \"pre-process-backend=vaapi-surface-sharing\", \"pre-process-backend=vaapi-surface-sharing pre-process-config=VAAPI_FAST_SCALE_LOAD_FACTOR=1\" <code>VA_SURFACE</code> use video analytics surface from the shared memory if applicable \"\", \"! \"video/x-raw(memory"},{"location":"use-cases/automated-self-checkout/automated-self-checkout.html","title":"Intel\u00ae Automated Self-Checkout Reference Package","text":""},{"location":"use-cases/automated-self-checkout/automated-self-checkout.html#overview","title":"Overview","text":"<p>As Computer Vision becomes more and more mainstream, especially for industrial &amp; retail use cases, development and deployment of these solutions becomes more challenging. Vision workloads are large and complex and need to go through many stages. For instance, in the pipeline below, the video data is ingested, pre-processed before each inferencing step, inferenced using two models - YOLOv5 and EfficientNet, and post processed to generate metadata and show the bounding boxes for each frame. This pipeline is just an example of the supported models and pipelines found within this reference.</p> <p></p> <p>Automated self-checkout solutions are complex, and retailers, independent software vendors (ISVs), and system integrators (SIs) require a good understanding of hardware and software, the costs involved in setting up and scaling the system, and the configuration that best suits their needs. Vision workloads are significantly larger and require systems to be architected, built, and deployed with several considerations. Hence, a set of ingredients needed to create an automated self-checkout solution is necessary. More details are available on the Intel Developer Focused Webpage and on this LinkedIn Blog</p> <p>The Intel\u00ae Automated Self-Checkout Reference Package provides critical components required to build and deploy a self-checkout use case using Intel\u00ae hardware, software, and other open-source software. This reference implementation provides a pre-configured automated self-checkout pipeline that is optimized for Intel\u00ae hardware. </p>"},{"location":"use-cases/automated-self-checkout/automated-self-checkout.html#next-steps","title":"Next Steps","text":"<p>To begin using the automated self-checkout solution you can follow the Getting Started Guide. </p>"},{"location":"use-cases/automated-self-checkout/automated-self-checkout.html#releases","title":"Releases","text":"<p>For the project release notes, refer to the GitHub* Repository.</p>"},{"location":"use-cases/automated-self-checkout/getting_started.html","title":"Getting Started","text":""},{"location":"use-cases/automated-self-checkout/getting_started.html#step-by-step-instructions","title":"Step by step instructions:","text":"<ol> <li> <p>Download the models using download_models/downloadModels.sh</p> <pre><code>make download-models\n</code></pre> </li> <li> <p>Update github submodules</p> <pre><code>make update-submodules\n</code></pre> </li> <li> <p>Download sample videos used by the performance tools</p> <pre><code>make download-sample-videos\n</code></pre> </li> <li> <p>Build the demo Docker image</p> <pre><code>make build\n</code></pre> </li> <li> <p>Start Automated Self Checkout using the Docker Compose file. The Docker Compose also includes an RTSP camera simulator that will infinitely loop through the sample videos downloaded in step 3.</p> <pre><code>make run-render-mode\n</code></pre> </li> <li> <p>Verify Docker containers</p> <p>Verify Docker images <pre><code>docker ps --format 'table{{.Names}}\\t{{.Status}}\\t{{.Image}}'\n</code></pre> Result: <pre><code>src-OvmsClientGst-1   Up 32 seconds\ncamera-simulator0     Up 4 minutes\ncamera-simulator      Up 4 minutes\n</code></pre></p> </li> <li> <p>Verify Results</p> <p>After starting Automated Self Checkout you will begin to see result files being written into the results/ directory. Here are example outputs from the 3 log files.</p> <p>gst-launch__gst.log <pre><code>/GstPipeline:pipeline0/GstGvaWatermark:gvawatermark0/GstCapsFilter:capsfilter1: caps = video/x-raw(memory:VASurface), format=(string)RGBA\n/GstPipeline:pipeline0/GstFPSDisplaySink:fpsdisplaysink0/GstXImageSink:ximagesink0: sync = true\nGot context from element 'vaapipostproc1': gst.vaapi.Display=context, gst.vaapi.Display=(GstVaapiDisplay)\"\\(GstVaapiDisplayGLX\\)\\ vaapidisplayglx0\", gst.vaapi.Display.GObject=(GstObject)\"\\(GstVaapiDisplayGLX\\)\\ vaapidisplayglx0\";\nProgress: (open) Opening Stream\nPipeline is PREROLLED ...\nPrerolled, waiting for progress to finish...\nProgress: (connect) Connecting to rtsp://localhost:8554/camera_0\nProgress: (open) Retrieving server options\nProgress: (open) Retrieving media info\nProgress: (request) SETUP stream 0\n</code></pre> <p>pipeline_gst.log <pre><code>14.58\n14.58\n15.47\n15.47\n15.10\n15.10\n14.60\n14.60\n14.88\n14.88\n</code></pre> <p>r_gst.jsonl <pre><code>{\"resolution\":{\"height\":1080,\"width\":1920},\"timestamp\":1}\n{\"objects\":[{\"detection\":{\"bounding_box\":{\"x_max\":1.0,\"x_min\":0.7868695002029238,\"y_max\":0.8493015899134377,\"y_min\":0.4422388975124676},\"confidence\":0.7139435410499573,\"label\":\"person\",\"label_id\":0},\"h\":440,\"region_id\":486,\"roi_type\":\"person\",\"w\":409,\"x\":1511,\"y\":478}],\"resolution\":{\"height\":1080,\"width\":1920},\"timestamp\":66661013}\n{\"objects\":[{\"detection\":{\"bounding_box\":{\"x_max\":1.0,\"x_min\":0.6974737628926411,\"y_max\":0.8381138710318847,\"y_min\":0.44749696271196093},\"confidence\":0.7188630104064941,\"label\":\"person\",\"label_id\":0},\"h\":422,\"region_id\":576,\"roi_type\":\"person\",\"w\":581,\"x\":1339,\"y\":483}],\"resolution\":{\"height\":1080,\"width\":1920},\"timestamp\":133305076}\n</code></pre> <li> <p>Stop the demo using docker compose down <pre><code>make down\n</code></pre></p> </li>"},{"location":"use-cases/automated-self-checkout/getting_started.html#proceed-to-advanced-settings","title":"Proceed to Advanced Settings","text":""},{"location":"use-cases/automated-self-checkout/getting_started.html#pipeline-performance-tools","title":"Pipeline Performance Tools","text":""},{"location":"use-cases/automated-self-checkout/performance.html","title":"Performance Testing","text":"<p>The performance tools repository is included as a github submodule in this project. The performance tools enable you to test the pipeline system performance on various hardware. </p>"},{"location":"use-cases/automated-self-checkout/performance.html#benchmark-specific-number-of-pipelines","title":"Benchmark specific number of pipelines","text":"<p>You can launch a specific number of Automated Self Checkout containers using the PIPELINE_COUNT environment variable. Default is to launch One yolov5s_full.sh pipeline. You can override these values through Environment Variables.</p> <pre><code>make benchmark\n</code></pre> <pre><code>make PIPELINE_COUNT=2 benchmark \n</code></pre> <p>Environment variable overrides can also be added to the command</p> <pre><code>make PIPELINE_SCRIPT=yolov5s_effnetb0.sh PIPELINE_COUNT=2 benchmark\n</code></pre> <p>Alternatively you can directly call the benchmark.py. This enables you to take advantage of all performance tools parameters. More details about the performance tools can be found HERE</p> <pre><code>cd performance-tools/benchmark-scripts &amp;&amp; python benchmark.py --compose_file ../../src/docker-compose.yml --pipeline 2\n</code></pre>"},{"location":"use-cases/automated-self-checkout/performance.html#benchmark-stream-density","title":"Benchmark Stream Density","text":"<p>To test the maximum amount of Automated Self Checkout containers that can run on a system you can use the TARGET_FPS environment variable. Default is to find the container threshold over FPS over 14.95 with the yolov5s_full.sh pipeline. You can override these values through Environment Variables.</p> <pre><code>make benchmark-stream-density\n</code></pre> <pre><code>make TARGET_FPS=13.5 benchmark-stream-density\n</code></pre> <p>Environment variable overrides can also be added to the command</p> <pre><code>make PIPELINE_SCRIPT=yolov5s_effnetb0.sh TARGET_FPS=13.5 benchmark-stream-density\n</code></pre> <p>Alternatively you can directly call the benchmark.py. This enables you to take advantage of all performance tools parameters. More details about the performance tools can be found HERE</p> <pre><code>cd performance-tools/benchmark-scripts &amp;&amp; python benchmark.py --compose_file ../../src/docker-compose.yml --target_fps 14\n</code></pre>"}]}