{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Intel Retail Documentation","text":"<p>Welcome to the Intel Retail organization.</p>"},{"location":"index.html#learn-more-about-our-features","title":"Learn more about our features","text":"<p>Architecture</p> <p>Performance Tools</p> <p>Use Cases</p> <p>Releases</p>"},{"location":"releasenotes.html","title":"Releases","text":"<p>Release v1.0.1</p> <p>Release v1.5.0</p> <p>Release v2.0.0</p> <p>Release v2.1.0</p> <p>Release v3.0.0</p> <p>Release v3.1.0</p> <p>Release v3.2.0</p>"},{"location":"troubleshooting.html","title":"Troubleshooting","text":""},{"location":"troubleshooting.html#q-why-is-the-performance-on-cpu-sometimes-better-than-on-gpu-when-running-pipeline-benchmarking-like-stream-density","title":"Q: Why is the performance on CPU sometimes better than on GPU, when running pipeline benchmarking like stream density?","text":"<p>A: The performance of pipeline benchmarking strongly depends on the models.  Specifically for <code>yolo11n</code> object detection, it is recommended to use the model precision FP32 when it is running on device <code>GPU</code>.  If supported, then you can change the model precision by updating the pipeline script to point to the precision of your choice.  For example, you can change the model of <code>FP16</code> to <code>FP32</code> assuming the precision <code>FP32</code> of the target model is available:  </p> <pre><code>    src/pipelines/yolo11n.sh\n\n    ...\n    model=models/object_detection/yolo11n/FP16/yolo11n.xml\n    ...\n</code></pre>"},{"location":"troubleshooting.html#q-what-happens-if-the-system-keeps-crashing-when-building-the-dlstreamer-realsense-image","title":"Q: What happens if the system keeps crashing when building the <code>dlstreamer-realsense</code> image?","text":"<p>A: Some systems may run into issues with memory when building the <code>dlstreamer-realsense</code> image. In the <code>Dockerfile.dlstreamer</code> file, change the make command to not use the <code>-j</code> threading option.</p> <pre><code>- make -j\"$(cat &lt; /proc/cpuinfo |grep -c proc)\" &amp;&amp;\n+ make &amp;&amp;\n</code></pre>"},{"location":"troubleshooting.html#q-what-happens-if-the-rtsp-source-is-not-found","title":"Q: What happens if the RTSP source is not found?","text":"<p>A: Depending on your systems performance it is possible that the RTSP simulator may take additional time to initialize and start streaming. To avoid issues you can add a waiting period before the pipeline starts. For example you can add a 5 second sleep timer to /src/entrypoint.sh</p> <pre><code>sleep 5 # sleep for 5 seconds before starting the pipeline\neval $gstLaunchCmd\n</code></pre>"},{"location":"troubleshooting.html#q-how-can-i-use-an-intel-realsense-camera-as-the-input","title":"Q: How can I use an Intel RealSense Camera as the input?","text":"<p>A: To use a RealSense camara as an input for any of the retail use cases, you need to obtain the serial number first. Follow the instructions here to run <code>rs-enumerate-devices</code> and obtain the following information:</p> <pre><code>Device Name                   Serial Number       Firmware Version\nIntel RealSense D415          725112060400        05.12.02.100\nDevice info:\n    Name                          :     Intel RealSense D415\n    Serial Number                 :     725112060400\n    Firmware Version              :     05.12.02.100\n    Recommended Firmware Version  :     05.12.02.100\n    Physical Port                 :     \\\\?\\usb#vid_8086&amp;pid_0ad3&amp;mi_00#6&amp;2a371216&amp;0&amp;0000#{e5323777-f976-4f5b-9b55-b94699c46e44}\\global\n    Debug Op Code                 :     15\n    Advanced Mode                 :     YES\n    Product Id                    :     0AD3\n    Camera Locked                 :     NO\n    Usb Type Descriptor           :     3.2\n    Product Line                  :     D400\n    Asic Serial Number            :     012345678901\n    Firmware Update Id            :     012345678901\n</code></pre> <p>Simply add the serial number to the <code>INPUTSRC</code> argument when calling the pipelines:</p> <pre><code>INPUTSRC=725112060400 make run-demo\n</code></pre>"},{"location":"troubleshooting.html#q-how-to-configure-centralized-proxy","title":"Q: How to configure centralized proxy?","text":"<p>A: Please follow the below steps to configure the proxy</p>"},{"location":"troubleshooting.html#1-configure-proxy-for-the-current-shell-session","title":"1. Configure Proxy for the Current Shell Session","text":"<pre><code>export http_proxy=http://&lt;proxy-host&gt;:&lt;port&gt;\nexport https_proxy=http://&lt;proxy-host&gt;:&lt;port&gt;\nexport HTTP_PROXY=http://&lt;proxy-host&gt;:&lt;port&gt;\nexport HTTPS_PROXY=http://&lt;proxy-host&gt;:&lt;port&gt;\nexport NO_PROXY=localhost,127.0.0.1,::1\nexport no_proxy=localhost,127.0.0.1,::1\nexport socks_proxy=http://&lt;proxy-host&gt;:&lt;port&gt;\nexport SOCKS_PROXY=http://&lt;proxy-host&gt;:&lt;port&gt;\n</code></pre>"},{"location":"troubleshooting.html#2-system-wide-proxy-configuration","title":"2. System-Wide Proxy Configuration","text":"<p>System-wide environment (/etc/environment) (Run: sudo nano /etc/environment and add or update)</p> <pre><code>http_proxy=http://&lt;proxy-host&gt;:&lt;port&gt;\nhttps_proxy=http://&lt;proxy-host&gt;:&lt;port&gt;\nftp_proxy=http://&lt;proxy-host&gt;:&lt;port&gt;\nsocks_proxy=http://&lt;proxy-host&gt;:&lt;port&gt;\nno_proxy=localhost,127.0.0.1,::1\n\nHTTP_PROXY=http://&lt;proxy-host&gt;:&lt;port&gt;\nHTTPS_PROXY=http://&lt;proxy-host&gt;:&lt;port&gt;\nFTP_PROXY=http://&lt;proxy-host&gt;:&lt;port&gt;\nSOCKS_PROXY=http://&lt;proxy-host&gt;:&lt;port&gt;\nNO_PROXY=localhost,127.0.0.1,::1\n</code></pre>"},{"location":"troubleshooting.html#3-docker-daemon-client-proxy-configuration","title":"3. Docker Daemon &amp; Client Proxy Configuration","text":"<p>Docker daemon drop-in (/etc/systemd/system/docker.service.d/http-proxy.conf) Create dir if missing: sudo mkdir -p /etc/systemd/system/docker.service.d sudo nano /etc/systemd/system/docker.service.d/http-proxy.conf</p> <pre><code>[Service]\nEnvironment=\"http_proxy=http://&lt;proxy-host&gt;:&lt;port&gt;\"\nEnvironment=\"https_proxy=http://&lt;proxy-host&gt;:&lt;port&gt;\"\nEnvironment=\"no_proxy=localhost,127.0.0.1,::1\"\nEnvironment=\"HTTP_PROXY=http://&lt;proxy-host&gt;:&lt;port&gt;\"\nEnvironment=\"HTTPS_PROXY=http://&lt;proxy-host&gt;:&lt;port&gt;\"\nEnvironment=\"NO_PROXY=localhost,127.0.0.1,::1\"\nEnvironment=\"socks_proxy=http://&lt;proxy-host&gt;:&lt;port&gt;\"\nEnvironment=\"SOCKS_PROXY=http://&lt;proxy-host&gt;:&lt;port&gt;\"\n\n# Reload &amp; restart:\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n\n#  Docker client config (~/.docker/config.json)\n#  mkdir -p ~/.docker\n#  nano ~/.docker/config.json\n{\n  \"proxies\": {\n    \"default\": {\n      \"httpProxy\": \"http://&lt;proxy-host&gt;:&lt;port&gt;\",\n      \"httpsProxy\": \"http://&lt;proxy-host&gt;:&lt;port&gt;\",\n      \"noProxy\": \"localhost,127.0.0.1,::1\"\n    }\n  }\n}\n</code></pre>"},{"location":"workshops.html","title":"Workshop Collaterals","text":"<p>ASU - 25 Jan 2025</p>"},{"location":"Architecture/pipelines.html","title":"Intel Retail","text":""},{"location":"Architecture/pipelines.html#repositories","title":"Repositories","text":"<p>In release v3.0.0 Intel-retail modules have been organized into logical repositories. By taking advantage of Github submodules different modules can be referenced from other repositories. For example, performance tools are being used by retail-use-cases and automated-self-checkout. Rather than duplicating and maintaining performance tools between the two repositories we linked the latest performance tools release as a submodule. </p> <p></p>"},{"location":"Architecture/pipelines.html#frameworks","title":"Frameworks","text":""},{"location":"Architecture/pipelines.html#openvino","title":"OpenVINO","text":"<p>OpenVINO is an open source toolkit provided by Intel to assist with running AI and ML on Intel hardware. The tools include a portable inference engine that is compatible with different Intel hardware platforms. The code can be found on the OpenVINO Github and examples can be ran with OpenVINO Jupyter Notebooks.</p> <p>OpenVINO provides some pre-trained models for quick development and testing through the OpenVINO Model Zoo. OpenVINO also supports converting models through they Model Conversion Process</p> <p>Details about the latest version can be found in the OpenVINO Release Notes.</p>"},{"location":"Architecture/pipelines.html#dlstreamer-pipeline","title":"DLStreamer Pipeline","text":"<p>Rather than working directly with the OpenVINO APIs our solutions offers more practical ways to interface with OpenVINO. One method is using Intel DLStreamer. This solution provides a no code way based on GStreamer and OpenVINO to deploy, process, and output a pipeline. </p> <p>The diagrams show how we take advantage of Docker, Docker Compose, and environment variable files to pre-package DLStreamer based pipelines for our use cases. Leveraging Environment Variables allows users to modify properties on the fly when different configurations are required.</p> <p></p>"},{"location":"Architecture/pipelines.html#openvino-model-serverovms","title":"OpenVINO Model Server(OVMS)","text":"<p>Another solution is the OpenVINO Model Server(OVMS). OVMS is a model hosting server that hosts inference models through a set of APIs. Unlike DLStreamer this solution requires developers to write code for pre and post processing model inference results. The advantage is the additional control developers have over their inference processing. Another benefit is the distribution of inference workloads between multiple servers either locally or remotely.</p>"},{"location":"Architecture/pipelines.html#openvino-model-serverovms-pipeline-with-grpc-remote-procedure-call","title":"OpenVINO Model Server(OVMS) Pipeline with gRPC (Remote Procedure Call)","text":"<p>There are two methods for running your inference through OVMS. The more flexible method has the client use gRPC (Remote Procedure Call) to request inference results from OVMS. By providing the proper input type and format the client can push inference compute to OVMS. OVMS can be local or on a remote system as long as the requested model is supported. This provides great flexibility with only minor latency increase. </p> <p>The gRPC interface supports c/c++, python, and go. A python example is located in our retail-use-cases gRPC python. The diagram show how the Docker Compose will deploy the client and OVMS. </p> <p></p>"},{"location":"Architecture/pipelines.html#openvino-model-serverovms-pipeline-with-c-api","title":"OpenVINO Model Server(OVMS) Pipeline with C API","text":"<p>When performance is more important than flexibility a developer can use the C API to bypass the gRPC interface and reduce latency. currently this method is only supported for c/c++ and required client/OVMS to both be deployed in a single Docker container. and example of a C API pipeline can be found in retail-use-cases gst_capi. Similar to DLStreamer the Docker Compose only launches a single container per pipeline now that the client/OVMS directly connect through the C API.</p> <p></p>"},{"location":"Architecture/pipelines.html#performance","title":"Performance","text":"<p>More details about benchmarking pipelines can be found on the Performance Tools Page.</p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html","title":"Distributed Architecture","text":"<ul> <li>Status</li> <li>Decision</li> <li>Context</li> <li>Proposed Design</li> <li>Consequences</li> <li>References</li> </ul>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#decision","title":"Decision","text":""},{"location":"Architecture/v2.0.0/distributed-architecture.html#context","title":"Context","text":"<p>To a wider variety of computer vision use cases in the real world a distributed inference architecture is required for deployment and scale. To achieve this, OpenVINO Model Server (OVMS) will be used for server side inferencing as part of the architecture design. The new architecture will lose some inference throughput but gain flexibility and scale.</p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#proposed-design","title":"Proposed Design","text":"<p>Using OpenVINO Model Server (OVMS) pipeline, workloads can be distributed between different services. For our solution a single system and remote server setup will be supported.</p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#single-system-setup","title":"Single System Setup","text":"<p>The single system solution will launch both the OVMS client and OVMS server on the same system as Docker containers. The local network can be used for communication between the Docker containers. The profile launcher program will load the profile configs and environment variables form a local data volume. The computer vision models will also be located on a local data volume. The models can be downloaded using the provided scripts or manually by the user.</p> <p></p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#remote-server-setup","title":"Remote Server Setup","text":"<p>The remote serve set will launch the same OVMS client and OVMS server containers but on two different systems. These systems can be on the same network on in remote locations as long as the systems can communicate through the network. This will require additional security or a direct connection from client to server. Similar to the single system the profile launcher will load the profile configs and environment variables from a data volume. In this case the data volume can be a local copy or a remote copy of those files. On the server the computer vision models will be in a data volume. Unlike the profile config and environment files these must be located on the server in a data volume. This is to prevent any unwanted changes to the computer vision model when it is located in a remote location. </p> <p></p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#running-the-pipeline","title":"Running the Pipeline","text":"<p>The profile launching program will start a pre-configured OVMS client and OVMS server. Run Pipeline documentation covers the parameter details and how to configure different input sources.</p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#applicable-repos","title":"Applicable Repos","text":"<p>automated-self-checkout</p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#consequences","title":"Consequences","text":"<p>Unlike DLStreamer there will be some latency to call OVMS through gRPC. This will results in a slightly lower stream density for systems. We will however support a wider range and combination of models since the inferencing will be abstracted into the OpenVINO Model Server.</p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#references","title":"References","text":"<p>https://docs.openvino.ai/2023.0/ovms_what_is_openvino_model_server.html https://github.com/openvinotoolkit/model_server</p>"},{"location":"Architecture/v2.0.0/multiple-ovms-json-config.html","title":"Multiple OpenVINO Model Server Config JSON","text":"<ul> <li>Status</li> <li>Decision</li> <li>Context</li> <li>Proposed Design</li> <li>Consequences</li> <li>References</li> </ul>"},{"location":"Architecture/v2.0.0/multiple-ovms-json-config.html#decision","title":"Decision","text":""},{"location":"Architecture/v2.0.0/multiple-ovms-json-config.html#context","title":"Context","text":"<p>Currently, we use same config.json for all instances of OpenVINO Model Server(OVMS) pipelines, which leads to some issue regarding the device mounting for OVMS server: see issue intel-retail/automated-self-checkout#322. So we need a way to have multiple or unique config json file per OVMS instance.</p>"},{"location":"Architecture/v2.0.0/multiple-ovms-json-config.html#proposed-design","title":"Proposed Design","text":"<p>Move the current device update logic from <code>run.sh</code> with the config.json file into profile-launcher in Golang. When profile-launcher about to launch a new instance of OVMS server, it then produces a unique config json file name for that instance of OVMS server.</p> <p>For example, we can use the Docker container name of that OVMS server like ovms_server0, or ovms_server1,...etc to be appended into the config json as part of the file name (e.g. config_ovms_server0.json).</p> <p>One example golang code for updating json file target_device and producing a new config.json is shown below: <pre><code>// ----------------------------------------------------------------------------------\n// Copyright 2023 Intel Corp.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n//  Unless required by applicable law or agreed to in writing, software\n//  distributed under the License is distributed on an \"AS IS\" BASIS,\n//  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n//  See the License for the specific language governing permissions and\n//  limitations under the License.\n//\n// ----------------------------------------------------------------------------------\n\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"log\"\n    \"os\"\n    \"reflect\"\n)\n\ntype OvmsConfig struct {\n    ModelList []ModelConfig `json:\"model_config_list\"`\n}\n\ntype ModelConfig struct {\n    Config map[string]interface{} `json:\"config\"`\n}\n\nfunc main() {\n    updateConfig(\"CPU\")\n}\n\nfunc updateConfig(device string) {\n    contents, err := os.ReadFile(\"config_template.json\")\n    if err != nil {\n        err = fmt.Errorf(\"Cannot read json config %v\", err)\n    }\n\n    var data OvmsConfig\n    err = json.Unmarshal(contents, &amp;data)\n    if err != nil {\n        log.Fatalf(\"failed to unmarshal configuration file configuration.yaml: %v\", err)\n    }\n\n    fmt.Println(reflect.TypeOf(data.ModelList))\n\n    for _, model := range data.ModelList {\n        // fmt.Println(model)\n        model.Config[\"target_device\"] = device\n        fmt.Println(model.Config[\"target_device\"])\n        fmt.Println(\"!!!!!!!!!!!!\")\n        fmt.Println(model.Config)\n    }\n\n    // convert to struct\n    updateConfig, err := json.Marshal(data)\n    if err != nil {\n        log.Fatalf(\"could not marshal config to JSON: %v\", err)\n    }\n    _ = os.WriteFile(\"config_ovms_server0.json\", updateConfig, 0644)\n}\n</code></pre></p> <p>This step is done before the profile-launcher calling the <code>start_ovms_server.sh</code>script.</p> <p>In the profile-launcher we also set the correct  environment variable values for the <code>start_ovms_server.sh</code>script to use.  For example, <code>OVMS_MODEL_CONFIG_JSON</code> to be the unique config json file name that was produced from the above example.</p> <p>For clean-up, we can do deletion of the config json files when <code>make clean-all</code> is called or <code>make clean-ovms-server</code> is called.</p>"},{"location":"Architecture/v2.0.0/multiple-ovms-json-config.html#references","title":"References","text":"<ul> <li>https://docs.openvino.ai/2023.0/ovms_what_is_openvino_model_server.html</li> <li>https://github.com/openvinotoolkit/model_server</li> <li>see issue Classification profile crashed when run the 2nd instance switch from CPU to GPU.0 automated-self-checkout#322</li> </ul>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html","title":"Performance Benchmarking","text":"<ul> <li>Status</li> <li>Decision</li> <li>Context</li> <li>Proposed Design</li> <li>Consequences</li> <li>References</li> </ul>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#decision","title":"Decision","text":""},{"location":"Architecture/v2.0.0/performance_benchmarking.html#context","title":"Context","text":"<p>To assist customers we will provide a set of performance Docker containers to measure the performance of their pipelines. The performance Docker containers will need to be supported on most modern Intel hardware. The output will also need to be formatted and presented to customers as a hardware recommendation.</p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#proposed-design","title":"Proposed Design","text":""},{"location":"Architecture/v2.0.0/performance_benchmarking.html#benchmark-script","title":"Benchmark Script","text":"<p>The benchmark script is designed to help determine the performance needs for a specific pipeline profile. The script will run a designated pipeline profile and can either replicate that pipeline profile a specific number of times or continue to replicate until a performance target is reached.</p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#performance-tools","title":"Performance tools","text":"<p>sysstat: System CPU utilization free: System memory usage iotop: System Disk read and write data igt-gpu-tools: Integrated GPU utilization Intel XPU Manage: Discrete GPU utilization Intel Performance Counter Monitor: System power usage</p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#input-source-types","title":"Input Source Types","text":"<p>For performance inputs we support RTSP video streams, USB camera, Intel\u00ae RealSense\u2122 Camera, and video files. For longer benchmarking runs its' recommended to use a video loop with an RTSP stream for inference result consistency. As an option an RTSP Camera Simulator is provided with the performance script.</p> <p>Input Source Types</p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#specified-number-of-pipelines","title":"Specified Number of Pipelines","text":"<p>If you are looking to test a specific number of pipelines on different hardware SKUs the <code>--pipelines</code> parameter can be used. This parameter will start the specified number of pipelines </p> <p>Specified Number of Pipelines</p> <p></p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#consolidated-results","title":"Consolidated Results","text":"<p>To make reading results easier, a consolidation script has been provided. This script will work with a single or multiple runs of the specified number of pipelines. Details about this process are found in Benchmark Specified Number of Pipelines</p> <pre><code>make consolidate ROOT_DIRECTORY=&lt;output dir&gt;\n</code></pre>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#stream-density","title":"Stream Density","text":"<p>The stream density parameter can be used to find the maximum number of pipelines at a target frames per second (FPS) on a specific hardware SKU. By setting the <code>--stream_density</code> parameter to the desired FPS the script will continue to create pipelines until the average pipelines FPS falls below the desired FPS. The script will provide a detailed log to show each pipeline FPS during the test run. This option provides a method for testing the top performance when introducing a new pipeline or hardware SKU.</p> <p>Stream Density</p> <p></p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#applicable-repos","title":"Applicable Repos","text":"<p>automated-self-checkout</p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#consequences","title":"Consequences","text":"<p>Having a generic and scalable set of performance Docker containers will allow customers to test a wide range of pipelines and hardware setups without extensive configuration of their systems. The flexibility will bring faster time to market and better hardware decision making by customers.</p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#references","title":"References","text":"<p>Pipeline Benchmarking</p>"},{"location":"Architecture/v2.0.0/profile-launcher.html","title":"Distributed Architecture","text":"<ul> <li>Status</li> <li>Decision</li> <li>Context</li> <li>Proposed Design</li> <li>Consequences</li> <li>References</li> </ul>"},{"location":"Architecture/v2.0.0/profile-launcher.html#decision","title":"Decision","text":""},{"location":"Architecture/v2.0.0/profile-launcher.html#context","title":"Context","text":"<p>Depending on the underlying pipeline architecture you may only require one Docker container or you may require many Docker containers. Specifically OVMS has two methods for running: gRPC which uses remote inference calls from a client to server and Capi which does the inferencing locally. Additionally other methods such as GStreamer are run in a single container. The profiles should be able to accommodate both use cases.</p>"},{"location":"Architecture/v2.0.0/profile-launcher.html#proposed-design","title":"Proposed Design","text":"<p>Update the profile to handle an array of configurations. This will allow the user to mix multiple Docker container configurations into a single profile.</p> <p>Each container configuration will contain the following information: - Docker image: The profile launcher will use as the target image - Environment file: Loaded into the container - Entrypoint script: Launch the desired start process - Input arguments: container or entrypoint script - Docker Volumes: Mounted to the container - Docker Networks: Connected to the container</p> <p></p>"},{"location":"Architecture/v2.0.0/profile-launcher.html#single-container-profile","title":"Single Container Profile","text":"<p>A single container profile will run a single Docker image using a single entrypoint script. This use case will be for pipelines that are self contained in a single container. Although the container can interact with other containers on the system the performance tools will only measure the performance of the single running container.</p> <p></p>"},{"location":"Architecture/v2.0.0/profile-launcher.html#multiple-container-profile","title":"Multiple Container Profile","text":"<p>A multiple container profile will run the array of containers defined in the profile config. Each container can have it's own entrypoint script even if they utilize the same base Docker image. The common profile will be the OpenVINO Model Server and client. In this case a OVMS container will contain the inference models defined in the config.json from the profile. Once the OVMS container is started the client will be launched and connect to the OVMS container. This will result in the inference workload being executed in a difference service which can be on the local system or in a remote location. </p> <p></p>"},{"location":"Architecture/v2.0.0/profile-launcher.html#applicable-repos","title":"Applicable Repos","text":"<p>automated-self-checkout</p>"},{"location":"Architecture/v2.0.0/profile-launcher.html#consequences","title":"Consequences","text":"<p>All profiles will need to be updated to use this new array structure. As a benefit common containers such as the OpenVINO Model Server can be shared between profiles.</p>"},{"location":"Architecture/v2.0.0/profile-launcher.html#references","title":"References","text":"<p>https://docs.openvino.ai/2023.0/ovms_what_is_openvino_model_server.html https://github.com/openvinotoolkit/model_server</p>"},{"location":"Architecture/v2.0.0/target-device.html","title":"Distributed Architecture","text":"<ul> <li>Status</li> <li>Decision</li> <li>Context</li> <li>Proposed Design</li> <li>Consequences</li> <li>References</li> </ul>"},{"location":"Architecture/v2.0.0/target-device.html#decision","title":"Decision","text":""},{"location":"Architecture/v2.0.0/target-device.html#context","title":"Context","text":"<p>The platform parameter is inconsistent with the target device being used in the pipeline. To be consistent with OVMS we want to use the target_device parameter in the script to match the target_device setting in the OVMS config file.</p>"},{"location":"Architecture/v2.0.0/target-device.html#proposed-design","title":"Proposed Design","text":"<p>Update the platform parameter to match the target_device standard used by OpenVINO Model Server. This will provide clarity to the device being used for the inferencing portion of the pipeline. The following are the acceptance criteria for the change.</p> <ul> <li>Replace platform parameter with target_device using CPU as the default device.</li> <li>Update the docker_run script to make it run with minimal changes to the profiles.</li> <li>Confirm that the benchmark script works with the target_device parameter update.</li> <li>Update unit tests</li> <li>Update documentation</li> <li>Convert $DEVICE to $TARGET_DEVICE for internal environment variables.</li> <li>Add option to use existing config file and not override all target_devices to support models with different target_device values.</li> </ul>"},{"location":"Architecture/v2.0.0/target-device.html#target-device-list","title":"Target Device list","text":"Device Parameter Description Links CPU CPU Use CPU only OVMS Parameters GPU GPU Use default GPU OVMS Parameters Specified GPU GPU.x Use a specific GPU. ex. GPU.0 = integrated GPU, GPU.1 = discrete Arc GPU OVMS Parameters Mixed Contifuration MULTI:x,y Use a combination of devices for inferencing ex. MULTI:CPU,GPU.1 will use the CPU and discrete Arc GPU for inferencing OVMS Parameters Automatic Device Selection AUTO Allow OpenVINO to automatically select the optimal device for inferencing Possibly depricated? Automatic Device Selection AUTO Allow OpenVINO to automatically select the optimal device for inferencing Possibly depricated? Heterogeneous Execution HETERO Allows OpenVINO to execute inference on multiple devices Heterogeneous Execution Heterogeneous Execution Priority HETERO:x,y Allows OpenVINO to execute inference on multiple devices and set the priority of device. ex. HETERO:CPU,GPU.1 will prioritize CPU and discrete Arc GPU for inferencing Heterogeneous Execution"},{"location":"Architecture/v2.0.0/target-device.html#applicable-repos","title":"Applicable Repos","text":"<p>automated-self-checkout</p>"},{"location":"Architecture/v2.0.0/target-device.html#consequences","title":"Consequences","text":"<p>Removing the platform parameter will break any existing test and benchmark scripts. The change will clarify which device you are targeting for the inference.</p>"},{"location":"Architecture/v2.0.0/target-device.html#references","title":"References","text":"<p>https://docs.openvino.ai/2023.0/ovms_what_is_openvino_model_server.html https://github.com/openvinotoolkit/model_server</p>"},{"location":"Architecture/v3.0.0/aicsd-integration.html","title":"Integration of AI Connect for Scientific Devices (AiCSD)","text":"<ul> <li>Decision</li> <li>Context</li> <li>Proposed Design</li> <li>References</li> </ul>"},{"location":"Architecture/v3.0.0/aicsd-integration.html#decision","title":"Decision","text":""},{"location":"Architecture/v3.0.0/aicsd-integration.html#context","title":"Context","text":"<p>This feature would:</p> <ul> <li>provide the ability to integrate pipelines using Intel Geti, BentoML or OpenVino</li> <li>provide the ability to run the entire AiCSD framework including sending images from another machine</li> <li>extend the capability of AiCSD to process still images to include video streams</li> <li>integrate EdgeX</li> </ul>"},{"location":"Architecture/v3.0.0/aicsd-integration.html#proposed-design","title":"Proposed Design","text":""},{"location":"Architecture/v3.0.0/aicsd-integration.html#crawl-integration-of-the-pipeline","title":"Crawl: Integration of the Pipeline","text":"<p>In the initial phases, the solution can be pulled in as just the pieces surrounding the pipeline validation. In order to do this, the Pipeline Validator service can be pulled in. This service  provides the appropriate endpoints and components necessary to call any pipeline built for this system. In the simplest case, the Pipeline Validator service could be used with the  Pipeline Simulator service without the need to add additional models. The Pipeline Simulator will be modified to use go gRPC to call an OVMS inferencing pipeline. In order to send information to the services, it would be necessary to add a script that calls the endpoint to launch the pipeline. When integrating with the benchmarking  script, the AiCSD services will be started using the necessary docker compose files.  This will ensure that all the services are started under the same Docker network. </p> <p></p> <p>Necessary Components:</p> <ul> <li>Pipeline Validator Service</li> <li>EdgeX Services</li> <li>Pipeline from the options below:<ul> <li>Pipeline Simulator (standalone)</li> <li>Intel Geti</li> <li>OVMS</li> <li>BentoML</li> </ul> </li> <li>Script to call launch pipeline for each image in a directory  (or a script that sends the same image for a fixed period of time)</li> <li>Integrate the ability to launch the appropriate target from the profile launcher</li> </ul>"},{"location":"Architecture/v3.0.0/aicsd-integration.html#walk-integrate-the-file-dropping-capability","title":"Walk: Integrate the file dropping capability","text":"<p>Integrate the entire AiCSD solution to add the ability to use two machines - one for generating image and the other for performing the processing. In integration of this feature, it would also be possible to run AiCSD all on one system. The integration will allow  for benchmarking to run alongside this solution.</p> <p>Necessary components:</p> <ul> <li>Desired pipeline</li> <li>AiCSD Gateway Services</li> <li>Integration of launching services with the profile launcher</li> </ul>"},{"location":"Architecture/v3.0.0/aicsd-integration.html#run-add-video-streaming-capability","title":"Run: Add Video Streaming Capability","text":"<p>This feature would allow for the use of video streaming with models supported by  Intel Geti or BentoML. In this solution, it could be necessary to update the AiCSD  solution in order to support the use of video streams. </p>"},{"location":"Architecture/v3.0.0/aicsd-integration.html#references","title":"References","text":"<ul> <li>AiCSD Architecture Overview</li> <li>AiCSD Pipeline Creation</li> </ul>"},{"location":"performance-tools/benchmark.html","title":"Computer Vision Pipeline Benchmarking","text":"<p>The provided Python-based script works with Docker Compose to get pipeline performance metrics like video processing in frames-per-second (FPS), memory usage, power consumption, and so on.</p>"},{"location":"performance-tools/benchmark.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>Docker Compose</li> <li>Make</li> <li>Git</li> <li> <p>Code from Retail Use Cases Repo and its submodule Performance Tools Repo</p> <p>Note</p> <p>To install the submodule, run <code>make update-submodules</code> from the root of the retail-use-cases repo.</p> </li> <li> <p>Python environment v3.12.2</p> <p>Note</p> <p>This could be accomplished using Miniconda and creating a Python 3.12.2 env</p> <pre><code>    sudo apt install git gcc python3-venv python3-dev\n</code></pre> </li> </ul>"},{"location":"performance-tools/benchmark.html#benchmark-a-cv-pipeline","title":"Benchmark a CV Pipeline","text":"<ol> <li> <p>Build the benchmark container and change into the benchmark-scripts directory.</p> <pre><code>    cd performance-tools/\n    make build-benchmark-docker\n</code></pre> </li> <li> <p>Python packages listed in performance-tools/benchmark-scripts/requirements.txt</p> <pre><code>    cd performance-tools/benchmark-scripts/\n    python3 -m venv venv\n    source venv/bin/activate\n    pip install -r requirements.txt\n</code></pre> </li> <li> <p>[Optional] If NPU data collection is desired, ensure that the following is correct.</p> <p>a. Run the following command to get the correct path to the NPU under <code>/sys/devices</code> <pre><code>lspci | grep -i npu\n</code></pre> b. Ensure the environment variable NPU_PATH in performance-tools/docker/docker-compose.yaml for the npu-util service or the global variable in performance-tools/docker/npu-util/npu_logger.py is set to the correct location.    <pre><code>NPU_PATH=\"/sys/devices/pci0000:00/0000:&lt;insert_results&gt;/npu_busy_time_us\"\n</code></pre></p> <p>Example</p> <p>If the lspci command is:</p> <pre><code>$ lspci | grep -i npu\n00:0b.0 Processing accelerators: Intel Corporation Lunar Lake NPU (rev 04)\n</code></pre> <p>then the NPU_PATH is:</p> <pre><code>NPU_PATH=\"/sys/devices/pci0000:00/0000:00:0b.0/npu_busy_time_us\"\n</code></pre> </li> <li> <p>Choose a CV pipeline from the Retail Use Cases Repo, Automated Self-Checkout or Loss Prevention and note the file paths to the docker compose files.</p> </li> <li> <p>Run the benchmarking script using the docker compose file(s) as inputs to the script (sample command shown below).</p> <p>Automated Self-Checkout: <pre><code>    python benchmark.py --compose_file ../../src/docker-compose.yml --pipeline 1\n</code></pre></p> <p>Retail Use Cases: <pre><code>    python benchmark.py --compose_file ../../use-cases/gst_capi/add_camera-simulator.yml --compose_file ../../use-cases/gst_capi/add_gst_capi_yolov5_ensemble.yml\n</code></pre></p> </li> </ol> <p>Go to Arguments to understand how to customize the benchmarks</p> <p>Specific number of pipelines with single container</p> <p></p> <p>Specific number of pipelines with OVMS and Client</p> <p></p>"},{"location":"performance-tools/benchmark.html#benchmark-stream-density-for-cv-pipelines","title":"Benchmark Stream Density for CV Pipelines","text":"<p>Benchmarking a pipeline can also discover the maximum number of workloads or streams that can be run in parallel for a given target FPS. This information is useful to determine the hardware required to achieve the desired performance for CV pipelines.</p> <p>To run the stream density functionality use <code>--target_fps</code> and/or <code>--density_increment</code> as inputs to the <code>benchmark.py</code> script:</p> <pre><code> python benchmark.py  --retail_use_case_root ../../retail-use-cases --target_fps 14.95 --density_increment 1 --init_duration 40   --compose_file ../../retail-use-cases/use-cases/grpc_python/docker-compose_grpc_python.yml\n</code></pre> <p>where the parameters:</p> <ul> <li><code>target_fps</code> is the given target frames per second (fps) to achieve for maximum number of pipelines</li> <li><code>density_increment</code> is to configure the benchmark logic to increase the number of pipelines each time while trying to find out the maximum number of pipelines before reaching the given target fps.</li> <li> <p><code>init_duration</code> is the initial duration period in second before pipeline performance metrics are taken</p> <p>Note</p> <p>It is recommended to set --target_fps to a value lesser than your target FPS to account for real world variances in hardware readings.</p> </li> </ul> <p>Stream density with single container </p> <p>Stream density with OVMS and Client </p>"},{"location":"performance-tools/benchmark.html#consolidate-results","title":"Consolidate results","text":"<p>The consolidate_multiple_run_of_metrics.py script processes and consolidates performance metrics from various log files (JSON, CSV, and text-based logs) into a structured report. It extracts key performance indicators (KPIs) such as CPU &amp; GPU utilization, memory bandwidth, disk I/O, power consumption, and FPS from multiple sources, aggregates the data, and outputs a summary file.</p> <p>on peformance-tools/benchmark-scripts:</p> <pre><code>    make consolidate\n</code></pre> <p>The summary.csv content should look like this:</p> <pre><code>    Camera_20250303214521714278352 FPS,14.86265306122449\n    Camera_20250303214521714278352 Last log update,03/03/2025 14:46:263943\n    CPU Utilization %,10.069166666666668\n    Memory Utilization %,19.70717535119376\n    Disk Read MB/s,0.0\n    Disk Write MB/s,0.002814426229508197\n    S0 Memory Bandwidth Usage MB/s,8012.58064516129\n    S0 Power Draw W,19.159666666666666\n</code></pre>"},{"location":"performance-tools/benchmark.html#plot-utilization-graphs","title":"Plot Utilization Graphs","text":"<p>After running a benchmark, you can generate a consolidated CPU, NPU, and GPU usage graph based on the collected logs using:  </p> <p>on peformance-tools/benchmark-scripts: <pre><code>make plot\n</code></pre> This command generates a single PNG image (<code>plot_metrics.png</code>) under the <code>results</code> directory, showing:  </p> <p>\ud83e\udde0 CPU Usage Over Time \u2699\ufe0f NPU Utilization Over Time \ud83c\udfae GPU Usage Over Time for each device found </p>"},{"location":"performance-tools/benchmark.html#modifying-additional-benchmarking-variables","title":"Modifying Additional Benchmarking Variables","text":""},{"location":"performance-tools/benchmark.html#arguments","title":"Arguments","text":"Argument Type Default Value Description <code>--pipelines</code> <code>int</code> <code>1</code> Number of pipelines <code>--target_fps</code> <code>float</code> (list) <code>None</code> Stream density target FPS; can take multiple values for multiple pipelines with 1-to-1 mapping via <code>--container_names</code> <code>--container_names</code> <code>str</code> (list) <code>None</code> Container names for stream density target; used together with <code>--target_fps</code> for 1-to-1 mapping <code>--density_increment</code> <code>int</code> <code>None</code> Pipeline increment number for stream density; dynamically adjusted if not specified <code>--results_dir</code> <code>str</code> <code>'./results'</code> Full path to the directory for logs and results <code>--duration</code> <code>int</code> <code>30</code> Time in seconds, not needed when <code>--target_fps</code> is specified <code>--init_duration</code> <code>int</code> <code>20</code> Initial time in seconds before starting metric data collection <code>--target_device</code> <code>str</code> <code>'CPU'</code> Desired running platform [cpu, core, xeon, dgpu.x] <code>--compose_file</code> <code>str</code> (list) <code>None</code> Path(s) to Docker Compose files; can be used multiple times <code>--retail_use_case_root</code> <code>str</code> <code>'../../'</code> Full path to the retail-use-cases repo root <code>--docker_log</code> <code>str</code> <code>None</code> Docker container name to get logs from and save to a file <code>--parser_script</code> <code>str</code> <code>'./parse_csv_to_json.py'</code> Full path to the parsing script to obtain FPS <code>--parser_args</code> <code>str</code> <code>\"-k device -k igt\"</code> Arguments to pass to the parser script; pass args with spaces in quotes: <code>\"args with spaces\"</code>"},{"location":"performance-tools/benchmark.html#change-power-profile","title":"Change Power Profile","text":"<ul> <li>For Ubuntu, follow this documentation to change the power profile.</li> <li>For Windows, follow this documentation to change the power mode.</li> </ul>"},{"location":"performance-tools/benchmark.html#change-or-customize-metric-parsing","title":"Change or Customize Metric Parsing","text":"<p>Two arguments <code>--parser_script</code> and <code>--parser_args</code> control the script and arguments passed to it respectively from the benchmark script.</p> <ul> <li>The <code>--parser_script</code> can be a python script that takes at least an input argument of <code>-d &lt;results_dir&gt;</code>. This will automatically get passed to the parsing script from the benchmarking script. </li> <li>Any other arguments may be passed using the <code>--parser_args</code>, where arguments with spaces are specified in double quotes.</li> </ul>"},{"location":"performance-tools/benchmark.html#developer-resources","title":"Developer Resources","text":""},{"location":"performance-tools/benchmark.html#python-testing","title":"Python Testing","text":"<p>To run the unit tests for the performance tools:</p> <pre><code>cd benchmark-scripts\nmake python-test\n</code></pre> <p>To run the unit tests and determine the coverage:</p> <pre><code>cd benchmark-scripts\nmake python-coverage\n</code></pre>"},{"location":"release-notes/v1-0-1.html","title":"1.0.1","text":"<p>Intel\u00ae Automated Self-Checkout Reference Package 1.0.0 is the first major release. This release includes all items required to run the vision checkout pipeline and benchmarking. For details on running the solution, refer to the Overview. </p>"},{"location":"release-notes/v1-0-1.html#new-features","title":"New Features","text":"Title Description Pipeline scripts Scripts that run the GStreamer-based vision checkout pipeline Benchmark scripts Scripts that start pipelines and system metrics based on parameters Docker* images Dockerized images for the pipeline and benchmark tools for code portability Set up Documentation Markdown files that include setup steps for initial use Unit tests Basic unit tests scripts for smoke testing Camera simulator Camera simulator script to simulate an RTSP stream using a media file Media downloader script Script to assist with downloading sample media for the camera simulator Model downloader script Script to assist with downloading the model files used for the pipelines"},{"location":"release-notes/v1-0-1.html#issues-fixed","title":"Issues Fixed","text":"Issue Number Description Link None Initial Release"},{"location":"release-notes/v1-0-1.html#known-issues","title":"Known Issues","text":"Issue Number Description Link 15 Pipeline core run on some HW is not producing inference results https://github.com/intel-retail/vision-self-checkout/issues/15 29 Unable to modify batch size from run script https://github.com/intel-retail/vision-self-checkout/issues/29"},{"location":"release-notes/v1-5-0.html","title":"1.5.0","text":"<p>Intel\u00ae Automated Self-Checkout Reference Package 1.5.0 is the second main release. This release includes bug fixes, feature enhancements, dockerization of the benchmarking tools, and OpenVINO Model Server support. For details on running the solution, refer to the Overview. </p>"},{"location":"release-notes/v1-5-0.html#new-features","title":"New Features","text":"Title Description OpenVINO Model Server OpenVINO Model Server support OpenVINO Model Server Pipelines Object detection pipelines using OpenVINO Model Server Benchmark scripts Dockerization Benchmark tools have been moved to Docker containers for more flexible deployment Github Build actions Code linting and security scans for pull requests"},{"location":"release-notes/v1-5-0.html#issues-fixed","title":"Issues Fixed","text":"Issue Number Description 41 Pipeline failure log 42 Create makefile docker commands 51 Optimized density script to reduce run time on high powered systems 55 Make performance / powersave mode configurable 57 Add debug option for docker-run.sh 58 Doc update with makefile 61 rename vision self checkout to automated self checkout 65 Update documentation to include OVMS pipelines 66 Add model download top level script 67 [Tech Debt] Make --workload work in any option/argument position when run benchmark.sh 75 docker-run.sh with wrong message when no --workload option is provided 77 XPU Manager not running on multiple GPUs 85 Fix ShellCheck issues in scripts 88 Incorrect instructions for building IGT in pipelinebenchmarking.md 91 format avc mp4 tag logic is inverted 96 For ovms workload getModels.sh not working when it is called by docker-run.sh from project base directory 99 Clean up some checked in dlstreamer models 100 Add cleaning ovms containers to makefile 105 benchmark pcm directory incorrect 109 igt path pointing to the incorrect directory causing the igt log to not be written 112 make CPU as default device for ovms pipeline 115 add dockerfile.bitModel to download bit models 119 pipelinesetup doc has incorrect link to models.list.yml 124 add ovms sample image download into run script 129 Update License to Apache 2.0 131 update mkdoc to navigate to OVMS doc 142 make build-ovms-server failed for 2nd time or later after removed the Docker image for rebuild"},{"location":"release-notes/v1-5-0.html#known-issues","title":"Known Issues","text":"Issue Number Description Link None"},{"location":"release-notes/v2-0-0.html","title":"2.0.0","text":"<p>Intel\u00ae Automated Self-Checkout Reference Package 2.0.0 is the next major release. This release includes bug fixes, feature enhancements, expansion of the OpenVINO Model Server use cases, implementation of gRPC and C API OVMS pipelines. For details on running the solution, refer to the Overview. </p>"},{"location":"release-notes/v2-0-0.html#new-features","title":"New Features","text":"Title Description OpenVINO Model Server gRPC OpenVINO Model Server support OpenVINO Model Server C API OpenVINO Model Server C API example Github Integration Test Action Nightly integration test action Docker Compose Pipeline Docker Compose version of the pipeline"},{"location":"release-notes/v2-0-0.html#issues-fixed","title":"Issues Fixed","text":"Issue Number Description 2.0 Issues Closed Github issues closed in the 2.0 release"},{"location":"release-notes/v2-0-0.html#known-issues","title":"Known Issues","text":"Issue Number Description Link None"},{"location":"release-notes/v2-1-0.html","title":"2.1.0","text":"<p>Intel\u00ae Automated Self-Checkout Reference Package 2.1.0 is minor maintenance release. This release includes bug fixes, feature enhancements, and a new yolov8 with efficientnet profile using OVMS C API. For details on running the solution, refer to the Overview. </p>"},{"location":"release-notes/v2-1-0.html#new-features","title":"New Features","text":"Title Description Yolov8 + Efficientnet C API Profile OVMS based yolov8 + efficientnet profile Sample video indexing Sample video indexing added for camera simulator container name consistency Batch size param for DLStreamer Add bach size parameter for DLStreamer profiles"},{"location":"release-notes/v2-1-0.html#issues-fixed","title":"Issues Fixed","text":"Issue Number Description 2.1 Issues Closed Github issues closed in the 2.1 release"},{"location":"release-notes/v2-1-0.html#known-issues","title":"Known Issues","text":"Issue Number Description Link None"},{"location":"release-notes/v3-0-0.html","title":"3.0.0","text":"<p>Intel\u00ae Retail organization 3.0.0 is major release. The release splits the Automated Self Checkout reference solution into multiple repositories for improved maintainability and scalability.</p>"},{"location":"release-notes/v3-0-0.html#new-features","title":"New Features","text":"Title Description Documentation (New Repository) Documentation, architecture, and requirements for Intel\u00ae retail repositories Automated Self Checkout Automated self checkout use case Retail Use Cases (New Repository) Retail use cases using DLStreamer and OpenVINO Model Server Performance Tools (New Submodule Repository) Performance tools for pipeline benchmarking"},{"location":"release-notes/v3-0-0.html#issues-fixed","title":"Issues Fixed","text":"<p>Release 3.0.0 Issues Closed</p>"},{"location":"release-notes/v3-0-0.html#known-issues","title":"Known Issues","text":"Issue Number Description Link None"},{"location":"release-notes/v3-1-0.html","title":"3.1.0","text":"<p>Intel\u00ae Retail organization 3.1.0 is major release that adds support for the Edge Video Analytics Microservice (EVAM) pipelines. There is also added support for EVAM visualization tools as well as various bug fixes.</p>"},{"location":"release-notes/v3-1-0.html#new-features","title":"New Features","text":"Title Description Edge Video Analytics Microservice Edge Video Analytics Microservice (EAVM) inference pipelines"},{"location":"release-notes/v3-1-0.html#issues-fixed","title":"Issues Fixed","text":""},{"location":"release-notes/v3-1-0.html#release-310-issues-closed","title":"Release 3.1.0 Issues Closed","text":"Repository Automated Self Checkout Core Services Documentation Performance Tools Retail Use Cases"},{"location":"release-notes/v3-1-0.html#known-issues","title":"Known Issues","text":"Repository Automated Self Checkout Core Services Documentation Performance Tools Retail Use Cases"},{"location":"release-notes/v3-2-0.html","title":"3.2.0","text":"<p>Intel\u00ae Retail organization 3.2.0 is a minor release that add loss prevention use case as well as various bug fixes.</p>"},{"location":"release-notes/v3-2-0.html#new-features","title":"New Features","text":"Title Description Loss Prevention Loss Prevention Use Case"},{"location":"release-notes/v3-2-0.html#issues-fixed","title":"Issues Fixed","text":""},{"location":"release-notes/v3-2-0.html#release-320-issues-closed","title":"Release 3.2.0 Issues Closed","text":"Repository Automated Self Checkout Core Services Documentation Performance Tools Retail Use Cases Loss Prevention"},{"location":"release-notes/v3-2-0.html#known-issues","title":"Known Issues","text":"Repository Automated Self Checkout Core Services Documentation Performance Tools Retail Use Cases Loss Prevention"},{"location":"use-cases/use-cases.html","title":"Intel Retail Use Cases","text":"<ul> <li>Automated Self Checkout</li> <li>Retail Use Cases: C-API for YOLOV8 ensemble</li> <li>Loss Prevention</li> </ul>"},{"location":"use-cases/automated-self-checkout/automated-self-checkout.html","title":"Intel\u00ae Automated Self-Checkout Reference Package","text":"<p>\ud83d\udd04 Package Integration Notice The Automated Self-Checkout functionality has been consolidated into the Intel\u00ae Loss Prevention Reference Package for a unified retail computer vision platform.</p>"},{"location":"use-cases/automated-self-checkout/automated-self-checkout.html#what-this-means-for-you","title":"What This Means for You","text":"<ul> <li>Existing Users: Your automated self-checkout use cases are now supported in the Loss Prevention package</li> <li>New Users: Start directly with the Loss Prevention package for the latest features</li> <li>Migration: No code changes needed - simply use the new package location</li> </ul>"},{"location":"use-cases/automated-self-checkout/automated-self-checkout.html#why-computer-vision-for-retail","title":"Why Computer Vision for Retail?","text":"<p>Automated self-checkout systems process complex visual data through multiple stages to transform raw video into actionable business insights:</p> <ol> <li>Video Ingestion: Capture customer interactions and product movements in real-time</li> <li>Object Detection: Identify products and items using YOLOv5 models</li> <li>Classification: Categorize and verify items with EfficientNet algorithms  </li> <li>Analytics: Generate loss prevention data and checkout validation</li> </ol> <p>The pipeline below demonstrates this workflow, where video data flows through preprocessing, dual AI model inference (YOLOv5 and EfficientNet), and post-processing to generate metadata and visual bounding boxes for each frame.</p> <p></p> <p>This unified platform simplifies deployment complexity with pre-configured, hardware-optimized workflows that scale from pilot programs to enterprise-wide implementations.</p>"},{"location":"use-cases/automated-self-checkout/automated-self-checkout.html#integration-benefits","title":"Integration Benefits","text":"<p>The automated self-checkout functionality has been consolidated into the Intel\u00ae Loss Prevention Reference Package, providing a unified platform for retail computer vision solutions. This integration offers several advantages:</p> <ul> <li>Unified Platform: Single application supporting both loss prevention and automated self-checkout use cases</li> <li>Hardware Optimization: Pre-configured workloads optimized for Intel\u00ae CPU, GPU, and NPU hardware</li> <li>Flexible Deployment: Multiple workload configurations including:</li> <li>Object Detection (CPU/GPU/NPU)</li> <li>Object Detection &amp; Classification (CPU/GPU/NPU)</li> <li>Age Prediction &amp; Face Detection (CPU/GPU/NPU)</li> <li>Heterogeneous configurations</li> <li>Simplified Management: Single codebase, unified configuration, and streamlined deployment process</li> </ul>"},{"location":"use-cases/automated-self-checkout/automated-self-checkout.html#what-you-want-to-do","title":"What You Want to Do","text":""},{"location":"use-cases/automated-self-checkout/automated-self-checkout.html#im-new-to-intel-retail-solutions","title":"\ud83d\ude80 I'm New to Intel Retail Solutions","text":"<p>Quick Start (15 minutes): Loss Prevention Getting Started Guide - Set up your environment - Run your first automated self-checkout demo - Understand the basic workflow</p>"},{"location":"use-cases/automated-self-checkout/automated-self-checkout.html#i-want-to-customize-the-solution","title":"\u2699\ufe0f I Want to Customize the Solution","text":"<p>Advanced Configuration (30-60 minutes): Loss Prevention Advanced Guide - Customize workload configurations - Optimize for your hardware setup - Configure multiple detection models</p>"},{"location":"use-cases/automated-self-checkout/automated-self-checkout.html#i-need-performance-data","title":"\ud83d\udcca I Need Performance Data","text":"<p>Benchmark &amp; Optimize: Loss Prevention Performance Guide - Compare CPU/GPU/NPU performance - Optimize for your specific use case - Understand throughput metrics</p>"},{"location":"use-cases/loss-prevention/advanced.html","title":"Advanced Settings","text":""},{"location":"use-cases/loss-prevention/advanced.html#configuration-options","title":"Configuration Options","text":""},{"location":"use-cases/loss-prevention/advanced.html#local-image-building","title":"Local Image Building","text":"<p>By default, the application uses pre-built Docker images for faster setup. If you need to build images locally (for customization or development):</p> <pre><code># Build and run locally instead of using pre-built images\nmake run-lp RENDER_MODE=1 REGISTRY=false\n\n# Apply to any command\nmake &lt;command&gt; REGISTRY=false\n\n# Examples:\nmake benchmark REGISTRY=false\nmake benchmark-stream-density REGISTRY=false\n</code></pre> <p>When to use local building: - Modifying source code or configurations - Development and testing changes - Air-gapped environments without internet access - Custom hardware optimizations</p> <p>Note: Local building takes significantly longer (15-30 minutes) compared to pre-built images (2-5 minutes).</p>"},{"location":"use-cases/loss-prevention/advanced.html#step-by-step-local-build-process","title":"Step-by-Step Local Build Process","text":"<ul> <li> <p>To build the images locally step by step:</p> <ul> <li> <p>Follow the following steps:   <pre><code>make download-models REGISTRY=false\nmake update-submodules REGISTRY=false\nmake download-sample-videos\nmake run-render-mode REGISTRY=false\n</code></pre></p> </li> <li> <p>The above series of commands can be executed using only one command:</p> </li> </ul> <pre><code>make run-lp REGISTRY=false RENDER_MODE=1\n</code></pre> </li> </ul> <p>The application is highly configurable via JSON files in the <code>configs/</code> directory</p> <p>To try a new camera or workload:</p> <pre><code>1. Create new camera to workload mapping in `configs/camera_to_workload_custom.json` to add your camera and assign workloads.\n- **camera_to_workload_custom.json**: Maps each camera to one or more workloads. \n    - To add or remove a camera, edit the `lane_config.cameras` array in the file.\n    - Each camera entry can specify its video source, region of interest, and assigned workloads.\n    Example:\n    ```json\n        {\n            \"lane_config\": {\n            \"cameras\": [\n                {\n                    \"camera_id\": \"cam1\",\n                    \"streamUri\": \"rtsp://rtsp-streamer:8554/video-stream-name\",\n                    \"workloads\": [\"items_in_basket\", \"multi_product_identification\"],\n                    \"region_of_interest\": {\"x\": 100, \"y\": 100, \"x2\": 800, \"y2\": 600}\n                }\n                ]\n                }\n         }\n    ```\nIf adding new videos, place your video files in the directory **performance-tools/sample-media/** and update the `streamUri` path.\n\n1. Connecting External RTSP Cameras:\nTo use real RTSP cameras instead of the built-in server:\n\n```json\n    {\n      \"camera_id\": \"external_cam1\",\n      \"streamUri\": \"rtsp://192.168.1.100:554/stream1\",\n      \"workloads\": [\"items_in_basket\"]\n   }\n ```\n\n2. Create new `configs/workload_to_pipeline_custom.json` to define pipeline for your workload.\n- **workload_to_pipeline_custom.json**: Maps each workload name to a pipeline definition (sequence of GStreamer elements and models). \n    Example:\n\n  ```json\n  {\n    \"workload_pipeline_map\": {\n      \"custom_workload_1\": [\n        {\"type\": \"gvadetect\", \"model\": \"yolo11n\", \"precision\": \"INT8\", \"device\": \"CPU\"},\n        {\"type\": \"gvaclassify\", \"model\": \"efficientnet-v2-b0\", \"precision\": \"INT8\", \"device\": \"CPU\"}\n      ],\n      \"custom_workload_2\": [\n        {\"type\": \"gvadetect\", \"model\": \"yolo11n\", \"precision\": \"INT16\", \"device\": \"NPU\"},\n        {\"type\": \"gvaclassify\", \"model\": \"efficientnet-v2-b0\", \"precision\": \"INT16\", \"device\": \"NPU\"}\n      ],\n      \"custom_workload_3\": [\n        {\"type\": \"gvadetect\", \"model\": \"yolo11n\", \"precision\": \"INT8\", \"device\": \"GPU\"},\n        {\"type\": \"gvaclassify\", \"model\": \"efficientnet-v2-b0\", \"precision\": \"INT8\", \"device\": \"GPU\"}\n      ]\n    }\n  }\n  ```\n1. Run validate configs command, to verify configuration files\n</code></pre> <p><code>sh    make validate-all-configs</code>     2. Re-run the pipeline as described above.</p> <p>[!NOTE] Since the GStreamer pipeline is generated dynamically based on the provided configuration(camera_to_workload and workload_to_pipeline json), the pipeline.sh file gets updated every time the user runs make run-lp or make benchmark. This ensures that the pipeline reflects the latest changes. <pre><code>src/pipelines/pipeline.sh\n</code></pre></p>"},{"location":"use-cases/loss-prevention/advanced.html#user-defined-workloads","title":"User Defined Workloads","text":""},{"location":"use-cases/loss-prevention/advanced.html#complete-workload-configuration-matrix","title":"Complete Workload Configuration Matrix","text":"<p>The preconfigured workloads support multiple hardware configurations out of the box. Use the <code>CAMERA_STREAM</code> and <code>WORKLOAD_DIST</code> variables to customize which cameras and hardware (CPU, GPU, NPU) are used by your pipeline.</p> <p>Usage Pattern: <pre><code>CAMERA_STREAM=&lt;camera_stream&gt; WORKLOAD_DIST=&lt;workload_dist&gt; make run-lp\n# Or for benchmarking:\nCAMERA_STREAM=&lt;camera_stream&gt; WORKLOAD_DIST=&lt;workload_dist&gt; make benchmark\n</code></pre></p>"},{"location":"use-cases/loss-prevention/advanced.html#loss-prevention-configurations","title":"Loss Prevention Configurations","text":"Description CAMERA_STREAM WORKLOAD_DIST CPU (Default) camera_to_workload.json workload_to_pipeline.json GPU camera_to_workload.json workload_to_pipeline_gpu.json NPU + GPU camera_to_workload.json workload_to_pipeline_gpu-npu.json Heterogeneous camera_to_workload.json workload_to_pipeline_hetero.json VLM camera_to_workload_vlm.json workload_to_pipeline_vlm.json <p>[!NOTE] Included Sub-Workloads: The following detection types are automatically enabled in all Loss Prevention configurations:</p> <ul> <li><code>items_in_basket</code> - Monitors items placed in shopping baskets</li> <li><code>hidden_items</code> - Detects concealed or hidden products  </li> <li><code>fake_scan_detection</code> - Identifies scanning without actual item registration</li> <li><code>multi_product_identification</code> - Tracks multiple products simultaneously</li> <li><code>product_switching</code> - Detects when customers switch high-value items for lower-value ones</li> <li><code>sweet_heartening</code> - Monitors for collusion between customers and cashiers</li> </ul>"},{"location":"use-cases/loss-prevention/advanced.html#automated-self-checkout-configurations","title":"Automated Self-Checkout Configurations","text":"Description CAMERA_STREAM WORKLOAD_DIST Object Detection (CPU) camera_to_workload_asc_object_detection.json workload_to_pipeline_asc_object_detection_cpu.json Object Detection (GPU) camera_to_workload_asc_object_detection.json workload_to_pipeline_asc_object_detection_gpu.json Object Detection (NPU) camera_to_workload_asc_object_detection.json workload_to_pipeline_asc_object_detection_npu.json Object Detection &amp; Classification (CPU) camera_to_workload_asc_object_detection_classification.json workload_to_pipeline_asc_object_detection_classification_cpu.json Object Detection &amp; Classification (GPU) camera_to_workload_asc_object_detection_classification.json workload_to_pipeline_asc_object_detection_classification_gpu.json Object Detection &amp; Classification (NPU) camera_to_workload_asc_object_detection_classification.json workload_to_pipeline_asc_object_detection_classification_npu.json Age Prediction &amp; Face Detection (CPU) camera_to_workload_asc_age_verification.json workload_to_pipeline_asc_age_verification_cpu.json Age Prediction &amp; Face Detection (GPU) camera_to_workload_asc_age_verification.json workload_to_pipeline_asc_age_verification_gpu.json Age Prediction &amp; Face Detection (NPU) camera_to_workload_asc_age_verification.json workload_to_pipeline_asc_age_verification_npu.json Age Verification Heterogeneous camera_to_workload_asc_age_verification.json workload_to_pipeline_asc_age_verification_hetero.json Object Detection Heterogeneous camera_to_workload_asc_object_detection_classification.json workload_to_pipeline_asc_object_detection_classification_hetero.json"},{"location":"use-cases/loss-prevention/architecture.html","title":"Loss Prevention Pipeline System Architecture","text":""},{"location":"use-cases/loss-prevention/architecture.html#table-of-contents","title":"Table of Contents","text":"<ol> <li>System Overview</li> <li>Architecture Diagrams</li> <li>Configuration Management</li> <li>System Benefits</li> </ol>"},{"location":"use-cases/loss-prevention/architecture.html#system-overview","title":"System Overview","text":"<p>The Loss Prevention Pipeline System is a comprehensive video analytics platform designed for retail loss prevention using Intel hardware acceleration. The system processes multiple camera feeds simultaneously, applying AI-powered detection and classification models to identify potential security incidents.</p>"},{"location":"use-cases/loss-prevention/architecture.html#key-features","title":"Key Features","text":"<ul> <li>Multi-camera Support: Simultaneous processing of multiple camera feeds</li> <li>AI-Powered Analytics: Object detection and classification using YOLO and EfficientNet models</li> <li>Intel Hardware Optimization: Leverages CPU, GPU, and NPU acceleration</li> <li>Real-time Processing: GStreamer-based pipeline for low-latency video processing</li> <li>Flexible Configuration: JSON-based configuration for cameras, workloads, and pipelines</li> <li>Performance Monitoring: Comprehensive metrics collection and analysis</li> <li>Containerized Deployment: Docker-based deployment with Intel DL Streamer</li> </ul>"},{"location":"use-cases/loss-prevention/architecture.html#architecture-diagrams","title":"Architecture Diagrams","text":""},{"location":"use-cases/loss-prevention/architecture.html#component-architecture","title":"Component Architecture","text":"<pre><code>graph TB\n    subgraph \"Configuration Layer\"\n        CWC[camera_to_workload.json]\n        WPC[workload_to_pipeline.json]\n    end\n\n    subgraph \"Pipeline Generation Layer\"\n        PGL[Pipeline Generator]\n        CL[Config Loader]\n    end\n\n    subgraph \"Execution Layer\"\n        GST[GStreamer Runtime]\n        ENV[Environment Manager]\n        DEV[Device Manager]\n    end\n\n    subgraph \"Processing Components\"\n        subgraph \"Source\"\n            FS[filesrc]\n            DEC[decodebin]\n        end\n\n        subgraph \"AI Processing\"\n            ATT[gvaattachroi]\n            DET[gvadetect]\n            TRK[gvatrack]\n            CLS[gvaclassify]\n        end\n\n        subgraph \"Output\"\n            CNV[gvametaconvert]\n            PUB[gvametapublish]\n            TEE[tee]\n            SINK[fakesink/fpsdisplaysink]\n        end\n    end\n\n    subgraph \"Performance Tools\"\n        BM[Benchmark Scripts]\n        MC[Metrics Collection]\n        AN[Analysis Tools]\n    end\n\n    CWC --&gt; CL\n    WPC --&gt; CL\n    CL --&gt; PGL\n\n    PGL --&gt; GST\n    GST --&gt; ENV\n    ENV --&gt; DEV\n\n    GST --&gt; FS\n    FS --&gt; DEC\n    DEC --&gt; ATT\n    ATT --&gt; DET\n    DET --&gt; TRK\n    TRK --&gt; CLS\n    CLS --&gt; CNV\n    CNV --&gt; PUB\n    CNV --&gt; TEE\n    TEE --&gt; SINK\n\n    GST --&gt; BM\n    BM --&gt; MC\n    MC --&gt; AN\n</code></pre>"},{"location":"use-cases/loss-prevention/architecture.html#configuration-flow","title":"Configuration Flow","text":"<pre><code>flowchart LR\n    subgraph \"Configuration Files\"\n        CAM[camera_to_workload.json \\n\u2022 Camera definitions \\n\u2022 Workload assignments \\n\u2022 ROI specifications]\n        WORK[workload_to_pipeline.json \\n\u2022 Pipeline steps \\n\u2022 Model configurations \\n\u2022 Device assignments]\n    end\n\n    subgraph \"Processing\"\n        LOAD[Config Loader]\n        NORM[Normalization]\n        MERGE[Pipeline Consolidation]\n    end\n\n    subgraph \"Output\"\n        GST[GStreamer Command]\n        EXEC[Pipeline Execution]\n    end\n\n    CAM --&gt; LOAD\n    WORK --&gt; LOAD\n    LOAD --&gt; NORM\n    NORM --&gt; MERGE\n    MERGE --&gt; GST\n    GST --&gt; EXEC\n</code></pre>"},{"location":"use-cases/loss-prevention/architecture.html#component-architecture_1","title":"Component Architecture","text":""},{"location":"use-cases/loss-prevention/architecture.html#core-components","title":"Core Components","text":""},{"location":"use-cases/loss-prevention/architecture.html#1-pipeline-generator-gst-pipeline-generatorpy","title":"1. Pipeline Generator (<code>gst-pipeline-generator.py</code>)","text":"<p>Purpose: Dynamic GStreamer pipeline generation based on configuration Key Functions: - Configuration loading and validation - Pipeline signature generation for optimization - Dynamic element creation based on workload requirements - Device-specific environment variable management</p>"},{"location":"use-cases/loss-prevention/architecture.html#2-configuration-management","title":"2. Configuration Management","text":"<p>Files: - <code>camera_to_workload.json</code>: Camera definitions and workload assignments - <code>workload_to_pipeline.json</code>: Pipeline step definitions</p>"},{"location":"use-cases/loss-prevention/architecture.html#3-gstreamer-elements","title":"3. GStreamer Elements","text":"<p>Source Elements: - <code>filesrc</code>: Video file input - <code>decodebin</code>: Automatic video decoding</p> <p>AI Processing Elements: - <code>gvaattachroi</code>: Region of Interest attachment - <code>gvadetect</code>: Object detection using YOLO models - <code>gvatrack</code>: Object tracking across frames - <code>gvaclassify</code>: Object classification using EfficientNet</p> <p>Output Elements: - <code>gvametaconvert</code>: Metadata format conversion - <code>gvametapublish</code>: Results publishing to files - <code>tee</code>: Pipeline branching for multiple outputs</p>"},{"location":"use-cases/loss-prevention/architecture.html#environment-configuration","title":"Environment Configuration","text":"<p>Device-specific environment files: - <code>/res/all-cpu.env</code>: CPU optimization settings - <code>/res/all-gpu.env</code>: GPU acceleration settings - <code>/res/all-npu.env</code>: NPU configuration</p>"},{"location":"use-cases/loss-prevention/architecture.html#configuration-management","title":"Configuration Management","text":""},{"location":"use-cases/loss-prevention/architecture.html#camera-configuration","title":"Camera Configuration","text":"<pre><code>{\n  \"lane_config\": {\n    \"cameras\": [\n      {\n        \"camera_id\": \"cam1\",\n        \"fileSrc\": \"video-file.mp4\",\n        \"width\": 1920,\n        \"fps\": 15,\n        \"workloads\": [\"detection\", \"classification\"],\n        \"region_of_interest\": {\n          \"x\": 0.1,\n          \"y\": 0.1,\n          \"x2\": 0.9,\n          \"y2\": 0.9\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"use-cases/loss-prevention/architecture.html#workload-configuration","title":"Workload Configuration","text":"<pre><code>{\n  \"workload_pipeline_map\": {\n    \"detection\": [\n      {\n        \"type\": \"gvadetect\",\n        \"model\": \"yolov5s\",\n        \"device\": \"GPU\",\n        \"precision\": \"FP16\"\n      }\n    ],\n    \"classification\": [\n      {\n        \"type\": \"gvaclassify\",\n        \"model\": \"efficientnet-b0\",\n        \"device\": \"NPU\",\n        \"precision\": \"INT8\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"use-cases/loss-prevention/architecture.html#system-benefits","title":"System Benefits","text":""},{"location":"use-cases/loss-prevention/architecture.html#performance-advantages","title":"Performance Advantages","text":"<ul> <li>Hardware Optimization: Native Intel hardware acceleration</li> <li>Pipeline Efficiency: Optimized GStreamer pipelines with minimal overhead</li> <li>Parallel Processing: Multi-camera, multi-workload concurrent execution</li> <li>Resource Management: Intelligent device allocation and load balancing</li> </ul>"},{"location":"use-cases/loss-prevention/architecture.html#scalability-features","title":"Scalability Features","text":"<ul> <li>Configuration-Driven: Easy addition of new cameras and workloads</li> <li>Modular Design: Independent processing pipelines</li> <li>Container-Based: Horizontal scaling with Docker orchestration</li> <li>Performance Monitoring: Real-time metrics for optimization</li> </ul> <p>This architecture documentation provides a comprehensive overview of the Loss Prevention Pipeline System. For implementation details, refer to the source code and configuration files in the repository.</p>"},{"location":"use-cases/loss-prevention/getting_started.html","title":"Getting Started","text":""},{"location":"use-cases/loss-prevention/getting_started.html#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<ul> <li>Ubuntu 24.04 or newer (Linux recommended), Desktop edition (or Server edition with GUI installed).</li> <li>Docker</li> <li>Make (<code>sudo apt install make</code>)</li> <li>Python 3 (<code>sudo apt install python3</code>) - required for video download and validation scripts</li> <li>Intel hardware (CPU, iGPU, dGPU, NPU)</li> <li>Intel drivers:<ul> <li>Intel GPU drivers</li> <li>NPU</li> </ul> </li> <li>Sufficient disk space for models, videos, and results</li> </ul> <p>Note</p> <p>First-time setup downloads AI models, sample videos, and Docker images - this may take 5-15 minutes depending on your internet connection.</p>"},{"location":"use-cases/loss-prevention/getting_started.html#choose-your-workload-type","title":"Choose Your Workload Type","text":""},{"location":"use-cases/loss-prevention/getting_started.html#loss-prevention","title":"\ud83d\udee1\ufe0f Loss Prevention","text":"<p>Purpose: Detect theft, suspicious behavior, and inventory shrinkage Use When: You want to monitor customer interactions and prevent loss Includes: Hidden items detection, fake scan detection, product switching alerts</p>"},{"location":"use-cases/loss-prevention/getting_started.html#automated-self-checkout","title":"\ud83d\uded2 Automated Self-Checkout","text":"<p>Purpose: Validate customer scanning and detect checkout fraud Use When: You want to ensure proper product scanning at self-checkout stations Options: Object detection, age verification, product classification</p>"},{"location":"use-cases/loss-prevention/getting_started.html#enhanced-loss-prevention-with-lvlm","title":"\ud83e\udde0 Enhanced Loss Prevention with LVLM","text":"<p>Purpose: Advanced item recognition using Large Vision Language Models Use When: You need higher accuracy for complex items or edge cases Includes: Agent-based LVLM invocation, traditional CV fallback, improved item identification</p>"},{"location":"use-cases/loss-prevention/getting_started.html#quick-configuration-reference","title":"Quick Configuration Reference","text":""},{"location":"use-cases/loss-prevention/getting_started.html#loss-prevention-hardware-options","title":"\ud83d\udee1\ufe0f Loss Prevention Hardware Options","text":"Configuration Command Best For CPU (Default) <code>make run-lp RENDER_MODE=1</code> Basic setups, testing GPU <code>make run-lp WORKLOAD_DIST=workload_to_pipeline_gpu.json RENDER_MODE=1</code> Performance, real-time processing NPU + GPU <code>make run-lp WORKLOAD_DIST=workload_to_pipeline_gpu-npu.json RENDER_MODE=1</code> Latest Intel hardware Heterogeneous <code>make run-lp WORKLOAD_DIST=workload_to_pipeline_hetero.json RENDER_MODE=1</code> Mixed workloads across hardware VLM <code>make run-lp STREAM_LOOP=false CAMERA_STREAM=camera_to_workload_vlm.json WORKLOAD_DIST=workload_to_pipeline_vlm.json RENDER_MODE=1</code> Advanced AI-powered detection <p>Included Detection Types: Hidden items, fake scanning, product switching, multi-product ID, sweet-heartening</p>"},{"location":"use-cases/loss-prevention/getting_started.html#automated-self-checkout-quick-commands","title":"\ud83d\uded2 Automated Self-Checkout Quick Commands","text":"<p>Object Detection (Identify scanned products): <pre><code># GPU (recommended)\nmake run-lp CAMERA_STREAM=camera_to_workload_asc_object_detection.json WORKLOAD_DIST=workload_to_pipeline_asc_object_detection_gpu.json RENDER_MODE=1\n\n# CPU alternative\nmake run-lp CAMERA_STREAM=camera_to_workload_asc_object_detection.json WORKLOAD_DIST=workload_to_pipeline_asc_object_detection_cpu.json RENDER_MODE=1\n</code></pre></p> <p>Age Verification (Age-restricted products): <pre><code># GPU (recommended)\nmake run-lp CAMERA_STREAM=camera_to_workload_asc_age_verification.json WORKLOAD_DIST=workload_to_pipeline_asc_age_verification_gpu.json RENDER_MODE=1\n</code></pre></p> <p>Combined Detection &amp; Classification: <pre><code># GPU (recommended)\nmake run-lp CAMERA_STREAM=camera_to_workload_asc_object_detection_classification.json WORKLOAD_DIST=workload_to_pipeline_asc_object_detection_classification_gpu.json RENDER_MODE=1\n</code></pre></p> <p>Tip</p> <p>Hardware Selection: Replace <code>_gpu</code> with <code>_cpu</code> or <code>_npu</code> in any command above based on your available hardware. </p> <p>Complete Reference: For the full configuration matrix with all hardware combinations and detailed specifications, see Complete Workload Configuration Matrix in Advanced Settings.</p>"},{"location":"use-cases/loss-prevention/getting_started.html#step-by-step-instructions","title":"Step by step instructions:","text":"<ol> <li> <p>Clone the repo with the below command     <pre><code>git clone -b &lt;release-or-tag&gt; --single-branch https://github.com/intel-retail/loss-prevention\n</code></pre>     &gt;Replace  with the version you want to clone (for example, v4.0.0).     <pre><code>git clone -b v4.0.0 --single-branch https://github.com/intel-retail/loss-prevention\n</code></pre> <li> <p>Run Loss Prevention (Recommended for First Time)</p> <p>Simple Setup with Visual Output: <pre><code># RENDER_MODE=1 shows live video with detection boxes (recommended for first run)\nmake run-lp RENDER_MODE=1\n</code></pre></p> <p>Alternative Options: <pre><code># Headless mode - no visual output (for servers or automated testing)\nmake run-lp\n</code></pre></p> <p>What This Does: - Downloads AI models (YOLOv5, EfficientNet) - Downloads sample videos for testing - Starts the loss prevention pipeline - Enables 6 detection types: hidden items, fake scanning, product switching, etc. - Visual mode shows real-time detection boxes and alerts</p> </li> <li> <p>Run Automated Self-Checkout Workloads</p> <p>Object Detection (Identifies scanned products): <pre><code># GPU accelerated - recommended for performance\nCAMERA_STREAM=camera_to_workload_asc_object_detection.json WORKLOAD_DIST=workload_to_pipeline_asc_object_detection_gpu.json make run-lp RENDER_MODE=1\n\n# CPU version - for systems without GPU\nCAMERA_STREAM=camera_to_workload_asc_object_detection.json WORKLOAD_DIST=workload_to_pipeline_asc_object_detection_cpu.json make run-lp RENDER_MODE=1\n</code></pre></p> <p>Age Verification (For age-restricted products): <pre><code># Detects customer age for alcohol/tobacco purchases\nCAMERA_STREAM=camera_to_workload_asc_age_verification.json WORKLOAD_DIST=workload_to_pipeline_asc_age_verification_gpu.json make run-lp RENDER_MODE=1\n</code></pre></p> <p>Combined Detection &amp; Classification: <pre><code># Both identifies AND categorizes products\nCAMERA_STREAM=camera_to_workload_asc_object_detection_classification.json WORKLOAD_DIST=workload_to_pipeline_asc_object_detection_classification_gpu.json make run-lp RENDER_MODE=1\n</code></pre></p> </li> <p>Tip</p> <p>Display Options: Add <code>RENDER_MODE=1</code> to any command above to see live video with detection boxes. Remove it for headless operation (servers/automation).</p> <p>Choose Your Hardware: Replace <code>_gpu</code> with <code>_cpu</code> or <code>_npu</code> based on your available hardware. See Automated Self-Checkout Configurations for all options.</p>"},{"location":"use-cases/loss-prevention/getting_started.html#what-youll-see-when-working","title":"What You'll See When Working","text":""},{"location":"use-cases/loss-prevention/getting_started.html#loss-prevention-results","title":"\ud83d\udee1\ufe0f Loss Prevention Results","text":"<ul> <li>Visual: Red bounding boxes around suspicious activities</li> <li>Alerts: Console notifications for hidden items, fake scanning, product switching</li> <li>Logs: Detection confidence scores and behavior analysis</li> </ul>"},{"location":"use-cases/loss-prevention/getting_started.html#automated-self-checkout-results","title":"\ud83d\uded2 Automated Self-Checkout Results","text":"<ul> <li>Object Detection: Green boxes around identified products with labels</li> <li>Age Verification: Face detection boxes with estimated age ranges</li> <li>Classification: Product categories and scanning validation status</li> </ul>"},{"location":"use-cases/loss-prevention/getting_started.html#expected-performance","title":"Expected Performance","text":"<ul> <li>Startup Time: 2-5 minutes (first run includes downloads)</li> <li>Processing: Real-time video analysis at 15-30 FPS</li> <li> <p>Results: JSON files appear in <code>results/</code> folder within seconds</p> </li> <li> <p>Verify Results</p> <p>After starting Loss Prevention you will begin to see result files being written into the results/ directory. Here are example outputs from the 3 log files.</p> <p>gst-launch__gst.log <pre><code>/GstPipeline:pipeline0/GstGvaWatermark:gvawatermark0/GstCapsFilter:capsfilter1: caps = video/x-raw(memory:VASurface), format=(string)RGBA\n/GstPipeline:pipeline0/GstFPSDisplaySink:fpsdisplaysink0/GstXImageSink:ximagesink0: sync = true\nGot context from element 'vaapipostproc1': gst.vaapi.Display=context, gst.vaapi.Display=(GstVaapiDisplay)\"\\(GstVaapiDisplayGLX\\)\\ vaapidisplayglx0\", gst.vaapi.Display.GObject=(GstObject)\"\\(GstVaapiDisplayGLX\\)\\ vaapidisplayglx0\";\nProgress: (open) Opening Stream\nPipeline is PREROLLED ...\nPrerolled, waiting for progress to finish...\nProgress: (connect) Connecting to rtsp://localhost:8554/camera_0\nProgress: (open) Retrieving server options\nProgress: (open) Retrieving media info\nProgress: (request) SETUP stream 0\n</code></pre> <p>pipeline_gst.log <pre><code>14.58\n14.58\n15.47\n15.47\n15.10\n15.10\n14.60\n14.60\n14.88\n14.88\n</code></pre> <p>r_gst.jsonl <pre><code>{\n    \"objects\": [\n        {\n            \"detection\": {\n                \"bounding_box\": {\n                    \"x_max\": 0.7873924346958825,\n                    \"x_min\": 0.6701603093745723,\n                    \"y_max\": 0.7915918938548927,\n                    \"y_min\": 0.14599881349270305\n                },\n                \"confidence\": 0.8677337765693665,\n                \"label\": \"apple\",\n                \"label_id\": 39\n            },\n            \"h\": 697,\n            \"region_id\": 610,\n            \"roi_type\": \"apple\",\n            \"w\": 225,\n            \"x\": 1287,\n            \"y\": 158\n        },\n        {\n            \"detection\": {\n                \"bounding_box\": {\n                    \"x_max\": 0.3221945836811382,\n                    \"x_min\": 0.19950163649114616,\n                    \"y_max\": 0.7924592239981934,\n                    \"y_min\": 0.1336837231479251\n                },\n                \"confidence\": 0.8625879287719727,\n                \"label\": \"apple\",\n                \"label_id\": 39\n            },\n            \"h\": 711,\n            \"region_id\": 611,\n            \"roi_type\": \"apple\",\n            \"w\": 236,\n            \"x\": 383,\n            \"y\": 144\n        },\n        {\n            \"detection\": {\n                \"bounding_box\": {\n                    \"x_max\": 0.5730873789069046,\n                    \"x_min\": 0.42000878963365595,\n                    \"y_max\": 0.9749763191740435,\n                    \"y_min\": 0.12431765065780453\n                },\n                \"confidence\": 0.854443371295929,\n                \"label\": \"apple\",\n                \"label_id\": 39\n            },\n            \"h\": 919,\n            \"region_id\": 612,\n            \"roi_type\": \"apple\",\n            \"w\": 294,\n            \"x\": 806,\n            \"y\": 134\n        }\n    ],\n    \"resolution\": {\n        \"height\": 1080,\n        \"width\": 1920\n    },\n    \"timestamp\": 755106652\n}\n</code></pre> <p>Note</p> <p>If unable to see results folder or files, please refer to the Troubleshooting section for more details.</p> <ol> <li> <p>Stop the containers:</p> <p>When pre-built images are pulled-</p> <pre><code>make down-lp\n</code></pre> <p>When images are built locally-</p> <pre><code>make down-lp REGISTRY=false\n</code></pre> </li> </ol>"},{"location":"use-cases/loss-prevention/getting_started.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p>If results folder is empty, check Docker logs for errors:</p> <ul> <li>List the docker containers   <pre><code>  docker ps -a\n</code></pre></li> <li>Verify Docker containers if it is running or no errors in container logs</li> </ul> <p><pre><code>docker ps --all\n</code></pre> Result: <pre><code>NAMES                    STATUS                     IMAGE\nsrc-pipeline-runner-1    Up 17 seconds (healthy)    pipeline-runner:lp\nmodel-downloader         Exited(0) 17 seconds       model-downloader:lp\n</code></pre></p> <ul> <li>Check each container logs   <pre><code>  docker logs &lt;container_id&gt;\n</code></pre></li> <li>If the file content in  <code>&lt;loss-prevention-workspace&gt;/results/pipeline_stream*.log</code> is empty, check GStreamer output file for errors:</li> <li><code>&lt;loss-prevention-workspace&gt;/results/gst-launch_*.log</code></li> </ul> </li> <li> <p>RTSP :</p> </li> <li>Connection timeout: Check <code>RTSP_STREAM_HOST</code> and <code>RTSP_STREAM_PORT</code> environment variables</li> <li>Stream not found: Verify video file exists in <code>performance-tools/sample-media/</code></li> <li>Black frames: Ensure video codec is H.264 (most compatible)</li> <li>Check RTSP server logs: <code>docker logs rtsp-streamer</code></li> </ul>"},{"location":"use-cases/loss-prevention/getting_started.html#proceed-to-advanced-settings","title":"Proceed to Advanced Settings","text":""},{"location":"use-cases/loss-prevention/loss-prevention.html","title":"Intel\u00ae Loss Prevention Reference Package","text":""},{"location":"use-cases/loss-prevention/loss-prevention.html#overview","title":"Overview","text":"<p>As computer vision technology becomes more mainstream in industrial and retail settings, using it for loss prevention is becoming increasingly complex. These vision workloads are substantial and require multiple stages of processing. For example, a typical loss prevention pipeline might capture video data, define regions of interest, implement tracking to monitor which products customers interact with, analyze the data using models like YOLOv5 and EfficientNet, and then post-process it to generate metadata that highlights which products are being purchased or potentially stolen. This is just one example of how such models and workflows can be utilized.</p> <p>Implementing loss prevention solutions in retail isn't straightforward. Retailers, independent software vendors (ISVs), and system integrators (SIs) need a solid understanding of both hardware and software, as well as the costs involved in setting up and scaling these systems. Given the data-intensive nature of vision workloads, systems must be carefully designed, built, and deployed with numerous considerations in mind. Effectively combating shrinkage requires the right mix of hardware, software, and optimized configurations.</p> <p>The Intel\u00ae Loss Prevention Reference Package is designed to help with this. It provides the essential components needed to develop and deploy a loss prevention solution using Intel\u00ae hardware, software, and open-source tools. This reference implementation includes a pre-configured pipeline that's optimized for Intel\u00ae hardware, simplifying the setup of an effective computer vision-based loss prevention system for retailers.</p>"},{"location":"use-cases/loss-prevention/loss-prevention.html#what-you-want-to-do","title":"What You Want to Do","text":""},{"location":"use-cases/loss-prevention/loss-prevention.html#im-new-to-loss-prevention-solutions","title":"\ud83d\ude80 I'm New to Loss Prevention Solutions","text":"<p>Quick Start (25 minutes): Getting Started Guide - Set up your development environment - Run your first loss prevention demo - Understand the detection pipeline workflow</p>"},{"location":"use-cases/loss-prevention/loss-prevention.html#i-want-to-customize-the-solution","title":"\u2699\ufe0f I Want to Customize the Solution","text":"<p>Advanced Configuration (45-90 minutes): Advanced Guide - Configure custom source and workloads</p>"},{"location":"use-cases/loss-prevention/loss-prevention.html#i-need-performance-analysis","title":"\ud83d\udcca I Need Performance Analysis","text":"<p>Benchmark &amp; Optimize: Performance Guide - Compare CPU/GPU/NPU detection performance</p>"},{"location":"use-cases/loss-prevention/performance.html","title":"Performance Testing &amp; Benchmarking","text":"<p>Test your Loss Prevention and Automated Self-Checkout pipeline performance on various hardware configurations. This guide covers everything from quick performance checks to comprehensive system capacity testing.</p>"},{"location":"use-cases/loss-prevention/performance.html#quick-start-5-minutes","title":"Quick Start (5 minutes)","text":"<p>Goal: Run a basic performance test to verify your system works correctly</p>"},{"location":"use-cases/loss-prevention/performance.html#1-initialize-performance-tools","title":"1. Initialize Performance Tools","text":"<pre><code>make update-submodules\n</code></pre>"},{"location":"use-cases/loss-prevention/performance.html#2-run-quick-benchmark","title":"2. Run Quick Benchmark","text":"<pre><code>make benchmark-quickstart\n</code></pre> <p>What this does: - Tests GPU performance with 6 different loss prevention workloads - Runs headless (no display needed) - Uses pre-built Docker images for faster setup - Automatically generates consolidated metrics</p> <p>Expected results: You'll see FPS metrics and resource utilization for each workload.</p>"},{"location":"use-cases/loss-prevention/performance.html#understanding-performance-testing-types","title":"Understanding Performance Testing Types","text":""},{"location":"use-cases/loss-prevention/performance.html#basic-performance-testing","title":"Basic Performance Testing","text":"<p>Default Benchmark Command: <pre><code>make benchmark\n</code></pre></p> <p>Configuration: - Single pipeline instance (<code>PIPELINE_COUNT=1</code>)  - CPU-only processing (<code>WORKLOAD_DIST=workload_to_pipeline.json</code>) - Standard camera setup (<code>CAMERA_STREAM=camera_to_workload.json</code>) - No visual output (<code>RENDER_MODE=0</code>)</p>"},{"location":"use-cases/loss-prevention/performance.html#environment-variables-reference","title":"Environment Variables Reference","text":"Category Variable Description Common Values Display <code>RENDER_MODE</code> Show/hide visual output <code>0</code> (headless), <code>1</code> (visual) Performance <code>PIPELINE_COUNT</code> Number of parallel pipeline instances <code>1</code>, <code>2</code>, <code>4</code> (higher = more stress) Hardware <code>WORKLOAD_DIST</code> Target processing hardware <code>workload_to_pipeline_cpu.json</code>, <code>workload_to_pipeline_gpu.json</code>, <code>workload_to_pipeline_gpu-npu.json</code> Camera Setup <code>CAMERA_STREAM</code> Camera configuration <code>camera_to_workload.json</code>, <code>camera_to_workload_full.json</code> Build <code>REGISTRY</code> Use pre-built vs local images <code>true</code> (faster), <code>false</code> (custom builds) <p>| Build | <code>REGISTRY</code> | Use pre-built vs local images | <code>true</code> (faster), <code>false</code> (custom builds) |</p>"},{"location":"use-cases/loss-prevention/performance.html#workload-configuration-options","title":"Workload Configuration Options","text":""},{"location":"use-cases/loss-prevention/performance.html#camera-stream-configurations","title":"Camera Stream Configurations","text":"<p>Standard Setup (<code>camera_to_workload.json</code>): | Camera | Workloads | |:-------|:----------| | cam1 | items_in_basket + multi_product_identification | | cam2 | hidden_items + product_switching | | cam3 | fake_scan_detection |</p> <p>Full Workload Testing (<code>camera_to_workload_full.json</code>): | Camera | Workload | |:-------|:---------| | cam1 | items_in_basket | | cam2 | hidden_items | | cam3 | fake_scan_detection | | cam4 | multi_product_identification | | cam5 | product_switching | | cam6 | sweet_heartening |</p>"},{"location":"use-cases/loss-prevention/performance.html#hardware-distribution-options","title":"Hardware Distribution Options","text":"Configuration File Best For CPU Only <code>workload_to_pipeline_cpu.json</code> Testing, development environments GPU Only <code>workload_to_pipeline_gpu.json</code> Production, high performance Mixed GPU/NPU <code>workload_to_pipeline_gpu-npu.json</code> Latest Intel hardware Heterogeneous <code>workload_to_pipeline_hetero.json</code> Maximum performance across all hardware Default Mixed <code>workload_to_pipeline.json</code> Balanced CPU/GPU/NPU distribution"},{"location":"use-cases/loss-prevention/performance.html#advanced-performance-testing-15-30-minutes","title":"Advanced Performance Testing (15-30 minutes)","text":""},{"location":"use-cases/loss-prevention/performance.html#gpu-performance-testing","title":"GPU Performance Testing","text":"<pre><code>make benchmark WORKLOAD_DIST=workload_to_pipeline_gpu.json CAMERA_STREAM=camera_to_workload_full.json\n</code></pre>"},{"location":"use-cases/loss-prevention/performance.html#multi-pipeline-stress-testing","title":"Multi-Pipeline Stress Testing","text":"<pre><code># Test with 2 parallel pipelines\nmake PIPELINE_COUNT=2 benchmark\n\n# High stress test with 4 pipelines\nmake PIPELINE_COUNT=4 benchmark\n</code></pre>"},{"location":"use-cases/loss-prevention/performance.html#custom-hardware-configuration","title":"Custom Hardware Configuration","text":"<pre><code># Test heterogeneous workload distribution\nmake benchmark WORKLOAD_DIST=workload_to_pipeline_hetero.json CAMERA_STREAM=camera_to_workload_full.json REGISTRY=false\n</code></pre>"},{"location":"use-cases/loss-prevention/performance.html#automated-self-checkout-performance","title":"Automated Self-Checkout Performance","text":"<pre><code># Object detection workload\nCAMERA_STREAM=camera_to_workload_asc_object_detection.json WORKLOAD_DIST=workload_to_pipeline_asc_object_detection_gpu.json make benchmark\n\n# Age verification workload  \nCAMERA_STREAM=camera_to_workload_asc_age_verification.json WORKLOAD_DIST=workload_to_pipeline_asc_age_verification_gpu.json make benchmark\n</code></pre>"},{"location":"use-cases/loss-prevention/performance.html#viewing-results","title":"Viewing Results","text":""},{"location":"use-cases/loss-prevention/performance.html#generate-consolidated-metrics","title":"Generate Consolidated Metrics","text":"<pre><code>make consolidate-metrics\n</code></pre> <p>Output: <code>benchmark/metrics.csv</code> containing: - FPS (frames per second) for each pipeline - CPU/GPU/NPU utilization percentages - Memory usage statistics - Power consumption data - Latency measurements</p>"},{"location":"use-cases/loss-prevention/performance.html#view-results","title":"View Results","text":"<pre><code>cat benchmark/metrics.csv\n</code></pre>"},{"location":"use-cases/loss-prevention/performance.html#stream-density-testing-30-minutes","title":"Stream Density Testing (30+ minutes)","text":"<p>Goal: Find the maximum number of parallel pipelines your system can handle while maintaining target performance.</p>"},{"location":"use-cases/loss-prevention/performance.html#basic-stream-density-test","title":"Basic Stream Density Test","text":"<pre><code>make benchmark-stream-density\n</code></pre> <p>Default behavior: - Tests until FPS drops below 14.95 target - Uses OOM protection to prevent system crashes - Reports maximum sustainable pipeline count</p>"},{"location":"use-cases/loss-prevention/performance.html#custom-target-fps","title":"Custom Target FPS","text":"<pre><code># Test for different performance thresholds\nmake TARGET_FPS=13.5 benchmark-stream-density\n\n# With custom pipeline configuration\nmake PIPELINE_SCRIPT=yolo11n_effnetb0.sh TARGET_FPS=13.5 benchmark-stream-density\n</code></pre>"},{"location":"use-cases/loss-prevention/performance.html#stream-density-environment-variables","title":"Stream Density Environment Variables","text":"Variable Description Values <code>TARGET_FPS</code> Minimum FPS threshold <code>14.95</code> (default), <code>13.5</code>, <code>20.0</code> <code>OOM_PROTECTION</code> Prevent out-of-memory crashes <code>1</code> (enabled), <code>0</code> (disabled) <p>\u26a0\ufe0f Warning: Setting <code>OOM_PROTECTION=0</code> may crash your system requiring a hard reboot.</p>"},{"location":"use-cases/loss-prevention/performance.html#expected-output","title":"Expected Output","text":"<pre><code>Total averaged FPS per stream: 15.210442307692306 for 26 pipeline(s)\n</code></pre>"},{"location":"use-cases/loss-prevention/performance.html#visualization-analysis","title":"Visualization &amp; Analysis","text":""},{"location":"use-cases/loss-prevention/performance.html#generate-performance-graphs","title":"Generate Performance Graphs","text":"<pre><code>make plot-metrics\n</code></pre> <p>Output: <code>benchmark/plot_metrics.png</code> showing: - \ud83e\udde0 CPU Usage Over Time - \u2699\ufe0f NPU Utilization Over Time - \ud83c\udfae GPU Usage for each GPU device found</p>"},{"location":"use-cases/loss-prevention/performance.html#useful-maintenance-commands","title":"Useful Maintenance Commands","text":"<p><pre><code>make validate-all-configs    # Validate configuration files\nmake clean-images           # Remove dangling Docker images\nmake clean-containers       # Remove stopped containers  \nmake clean-all             # Remove all unused Docker resources\n</code></pre> make clean-all             # Remove all unused Docker resources <pre><code>## Custom Configuration (Advanced)\n\n### Creating Custom Workloads\n\nThe application is highly configurable via JSON files in the `configs/` directory:\n\n#### Camera Configuration (`camera_to_workload.json`)\n```json\n{\n  \"lane_config\": {\n    \"cameras\": [\n      {\n        \"camera_id\": \"cam1\",\n        \"fileSrc\": \"sample-media/video1.mp4\",              \n        \"workloads\": [\"items_in_basket\", \"multi_product_identification\"],\n        \"region_of_interest\": {\"x\": 100, \"y\": 100, \"x2\": 800, \"y2\": 600}\n      }\n    ]\n  }\n}\n</code></pre></p>"},{"location":"use-cases/loss-prevention/performance.html#pipeline-configuration-workload_to_pipelinejson","title":"Pipeline Configuration (<code>workload_to_pipeline.json</code>)","text":"<pre><code>{\n  \"workload_pipeline_map\": {\n    \"items_in_basket\": [\n      {\"type\": \"gvadetect\", \"model\": \"yolo11n\", \"precision\": \"INT8\", \"device\": \"CPU\"},\n      {\"type\": \"gvaclassify\", \"model\": \"efficientnet-v2-b0\", \"precision\": \"INT8\", \"device\": \"CPU\"}\n    ]\n  }\n}\n</code></pre>"},{"location":"use-cases/loss-prevention/performance.html#to-add-custom-workloads","title":"To Add Custom Workloads:","text":"<ol> <li>Edit <code>configs/camera_to_workload.json</code> to add your camera and assign workloads</li> <li>Edit <code>configs/workload_to_pipeline.json</code> to define the pipeline for your workload</li> <li>Place your video files in <code>performance-tools/sample-media/</code> and update the <code>fileSrc</code> path</li> <li>Run <code>make validate-all-configs</code> to verify your configuration</li> <li>Re-run the pipeline: <code>make benchmark</code></li> </ol>"},{"location":"use-cases/loss-prevention/performance.html#detailed-hardware-distribution-reference","title":"Detailed Hardware Distribution (Reference)","text":""},{"location":"use-cases/loss-prevention/performance.html#heterogeneous-configuration-breakdown","title":"Heterogeneous Configuration Breakdown","text":"<p>The <code>workload_to_pipeline_hetero.json</code> distributes workloads across multiple processing units:</p> Workload Object Detection Classification Inference items_in_basket GPU GPU - hidden_items GPU CPU - fake_scan_detection GPU CPU - multi_product_identification GPU CPU - product_switching GPU GPU - sweet_heartening NPU - NPU"},{"location":"use-cases/loss-prevention/performance.html#mixed-configuration-details","title":"Mixed Configuration Details","text":"<p>The <code>workload_to_pipeline.json</code> balances workloads across available hardware: - CPU: items_in_basket, multi_product_identification, sweet_heartening - GPU: product_switching, hidden_items - NPU: fake_scan_detection</p>"},{"location":"use-cases/loss-prevention/performance.html#project-structure-reference","title":"Project Structure (Reference)","text":"<ul> <li><code>configs/</code> \u2014 Configuration files (camera/workload mapping)</li> <li><code>docker/</code> \u2014 Dockerfiles for containers</li> <li><code>download-scripts/</code> \u2014 Model and video download scripts</li> <li><code>src/</code> \u2014 Pipeline runner scripts</li> <li><code>performance-tools/benchmark-scripts/results/</code> \u2014 Performance test results</li> <li><code>Makefile</code> \u2014 Build automation commands</li> </ul>"},{"location":"use-cases/loss-prevention/performance.html#appendix-system-configuration","title":"Appendix: System Configuration","text":""},{"location":"use-cases/loss-prevention/performance.html#proxy-configuration-if-required","title":"Proxy Configuration (If Required)","text":"<p>If your organization requires proxy settings for internet access:</p>"},{"location":"use-cases/loss-prevention/performance.html#shell-session-proxy","title":"Shell Session Proxy","text":"<pre><code>export http_proxy=http://&lt;proxy-host&gt;:&lt;port&gt;\nexport https_proxy=http://&lt;proxy-host&gt;:&lt;port&gt;\nexport HTTP_PROXY=http://&lt;proxy-host&gt;:&lt;port&gt;\nexport HTTPS_PROXY=http://&lt;proxy-host&gt;:&lt;port&gt;\nexport NO_PROXY=localhost,127.0.0.1,::1\n</code></pre>"},{"location":"use-cases/loss-prevention/performance.html#system-wide-proxy-etcenvironment","title":"System-Wide Proxy (<code>/etc/environment</code>)","text":"<pre><code>sudo nano /etc/environment\n# Add the following lines:\nhttp_proxy=http://&lt;proxy-host&gt;:&lt;port&gt;\nhttps_proxy=http://&lt;proxy-host&gt;:&lt;port&gt;\nHTTP_PROXY=http://&lt;proxy-host&gt;:&lt;port&gt;\nHTTPS_PROXY=http://&lt;proxy-host&gt;:&lt;port&gt;\nNO_PROXY=localhost,127.0.0.1,::1\n</code></pre>"},{"location":"use-cases/loss-prevention/performance.html#docker-proxy-configuration","title":"Docker Proxy Configuration","text":"<p>Create <code>/etc/systemd/system/docker.service.d/http-proxy.conf</code>: <pre><code>sudo mkdir -p /etc/systemd/system/docker.service.d\nsudo nano /etc/systemd/system/docker.service.d/http-proxy.conf\n\n[Service]\nEnvironment=\"http_proxy=http://&lt;proxy-host&gt;:&lt;port&gt;\"\nEnvironment=\"https_proxy=http://&lt;proxy-host&gt;:&lt;port&gt;\"\nEnvironment=\"no_proxy=localhost,127.0.0.1,::1\"\n\n# Reload and restart\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n</code></pre></p>"},{"location":"use-cases/order-accuracy/advanced.html","title":"Advanced Settings","text":""},{"location":"use-cases/order-accuracy/advanced.html#configuration-options","title":"Configuration Options","text":""},{"location":"use-cases/order-accuracy/advanced.html#local-image-building","title":"Local Image Building","text":"<p>By default, the application uses pre-built Docker images for faster setup. If you need to build images locally (for customization or development):</p> <pre><code># Build and run locally instead of using pre-built images\nmake build REGISTRY=false\nmake up\n\n# Examples for both applications:\n# Dine-In\ncd dine-in &amp;&amp; make build REGISTRY=false &amp;&amp; make up\n\n# Take-Away\ncd take-away &amp;&amp; make build REGISTRY=false &amp;&amp; make up\n</code></pre> <p>When to use local building: - Modifying source code or configurations - Development and testing changes - Air-gapped environments without internet access - Custom hardware optimizations</p> <p>Note: Local building takes significantly longer (15-30 minutes) compared to pre-built images (2-5 minutes).</p>"},{"location":"use-cases/order-accuracy/advanced.html#dine-in-configuration","title":"Dine-In Configuration","text":""},{"location":"use-cases/order-accuracy/advanced.html#environment-configuration-env","title":"Environment Configuration (.env)","text":"<pre><code># =============================================================================\n# Logging\n# =============================================================================\nLOG_LEVEL=INFO\n\n# =============================================================================\n# Service Endpoints\n# =============================================================================\nOVMS_ENDPOINT=http://ovms-vlm:8000\nOVMS_MODEL_NAME=Qwen/Qwen2.5-VL-7B-Instruct\nSEMANTIC_SERVICE_ENDPOINT=http://semantic-service:8080\nAPI_TIMEOUT=60\n</code></pre>"},{"location":"use-cases/order-accuracy/advanced.html#test-data-configuration","title":"Test Data Configuration","text":"<ol> <li>Add Images: Place food tray/plate images in <code>images/</code> folder</li> <li>Supported formats: <code>.jpg</code>, <code>.jpeg</code>, <code>.png</code></li> <li> <p>Images should clearly show food items on the plate</p> </li> <li> <p>Update Orders: Edit <code>configs/orders.json</code> with test orders</p> </li> <li>Each order needs <code>image_id</code> and list of <code>items_ordered</code></li> <li> <p>Image IDs should match filenames (without extension)</p> </li> <li> <p>Update Inventory: Edit <code>configs/inventory.json</code> with menu items</p> </li> <li>Define all possible food items</li> <li>Include item names, categories, and metadata</li> </ol>"},{"location":"use-cases/order-accuracy/advanced.html#dine-in-docker-services","title":"Dine-In Docker Services","text":"Container Ports Description <code>dinein_app</code> 7861, 8083 Main application (Gradio + FastAPI) <code>dinein_ovms_vlm</code> 8002 Vision-Language Model server <code>dinein_semantic_service</code> 8081, 9091 Semantic text matching <code>metrics-collector</code> 8084 System metrics aggregation"},{"location":"use-cases/order-accuracy/advanced.html#take-away-configuration","title":"Take-Away Configuration","text":""},{"location":"use-cases/order-accuracy/advanced.html#environment-configuration-env_1","title":"Environment Configuration (.env)","text":"<pre><code># =============================================================================\n# VLM Backend\n# =============================================================================\nVLM_BACKEND=ovms\nOVMS_ENDPOINT=http://ovms-vlm:8000\nOVMS_MODEL_NAME=Qwen/Qwen2.5-VL-7B-Instruct\nOPENVINO_DEVICE=GPU          # 'GPU', 'CPU', or 'AUTO'\n\n# =============================================================================\n# Semantic Service\n# =============================================================================\nSEMANTIC_VLM_BACKEND=ovms\nDEFAULT_MATCHING_STRATEGY=hybrid   # 'exact', 'semantic', or 'hybrid'\nSIMILARITY_THRESHOLD=0.85\nOVMS_TIMEOUT=60\n\n# =============================================================================\n# MinIO Storage\n# =============================================================================\nMINIO_ROOT_USER=minioadmin\nMINIO_ROOT_PASSWORD=minioadmin\nMINIO_ENDPOINT=minio:9000\n</code></pre>"},{"location":"use-cases/order-accuracy/advanced.html#service-modes","title":"Service Modes","text":"Mode Configuration Use Case Single <code>SERVICE_MODE=single</code> Development, testing Parallel <code>SERVICE_MODE=parallel WORKERS=4</code> Production deployment <p>Start in Different Modes: <pre><code># Single mode (default)\nmake up\n\n# Parallel mode with 4 workers\nmake up-parallel WORKERS=4\n\n# Parallel mode with auto-scaling\nmake up-parallel WORKERS=4 SCALING_MODE=auto\n</code></pre></p>"},{"location":"use-cases/order-accuracy/advanced.html#take-away-docker-services","title":"Take-Away Docker Services","text":"Container Ports Description <code>takeaway_app</code> 7860, 8080 Main application (Gradio + FastAPI) <code>ovms-vlm</code> 8001 Vision-Language Model server <code>frame-selector</code> 8085 YOLO-based frame selection <code>semantic-service</code> 8081, 9091 Semantic text matching <code>minio</code> 9000, 9001 S3-compatible storage"},{"location":"use-cases/order-accuracy/advanced.html#benchmarking","title":"Benchmarking","text":""},{"location":"use-cases/order-accuracy/advanced.html#dine-in-benchmarking","title":"Dine-In Benchmarking","text":"<p>Initialize Performance Tools: <pre><code>cd dine-in\nmake update-submodules\n</code></pre></p> <p>Run Benchmark: <pre><code>make benchmark\n</code></pre></p> <p>Stream Density Test: <pre><code>make benchmark-density\n</code></pre></p> <p>View Results: <pre><code>make benchmark-density-results\ncat results/benchmark_results.json\n</code></pre></p>"},{"location":"use-cases/order-accuracy/advanced.html#take-away-benchmarking","title":"Take-Away Benchmarking","text":"<p>Initialize Performance Tools: <pre><code>cd take-away\nmake update-submodules\n</code></pre></p> <p>Single Video Benchmark: <pre><code>make benchmark\n</code></pre></p> <p>Fixed Workers Benchmark: <pre><code>make benchmark-oa BENCHMARK_WORKERS=4 BENCHMARK_DURATION=300\n</code></pre></p> <p>Stream Density Benchmark: <pre><code>make benchmark-stream-density \\\n  BENCHMARK_TARGET_LATENCY_MS=25000 \\\n  BENCHMARK_MIN_TRANSACTIONS=3 \\\n  BENCHMARK_WORKER_INCREMENT=1\n</code></pre></p>"},{"location":"use-cases/order-accuracy/advanced.html#benchmark-configuration-variables","title":"Benchmark Configuration Variables","text":"Variable Default Description <code>TARGET_LATENCY_MS</code> 25000 Target latency threshold (ms) <code>LATENCY_METRIC</code> avg 'avg', 'p95', or 'max' <code>WORKER_INCREMENT</code> 1 Workers added per iteration <code>INIT_DURATION</code> 10 Warmup time (seconds) <code>MIN_TRANSACTIONS</code> 3 Min transactions before measuring <code>MAX_ITERATIONS</code> 50 Max scaling iterations <code>RESULTS_DIR</code> ./results Results output directory"},{"location":"use-cases/order-accuracy/advanced.html#system-requirements","title":"System Requirements","text":""},{"location":"use-cases/order-accuracy/advanced.html#minimum-configuration","title":"Minimum Configuration","text":"Component Specification CPU Intel Xeon 8+ cores RAM 16 GB GPU Intel Arc A770 8GB / NVIDIA RTX 3060 Storage 50 GB SSD Docker 24.0+ with Compose V2"},{"location":"use-cases/order-accuracy/advanced.html#recommended-configuration","title":"Recommended Configuration","text":"Component Specification CPU Intel Xeon 16+ cores RAM 32 GB GPU NVIDIA RTX 3080+ / Intel Data Center GPU Storage 200 GB NVMe SSD Network 10 Gbps (for Take-Away RTSP)"},{"location":"use-cases/order-accuracy/advanced.html#useful-make-commands","title":"Useful Make Commands","text":""},{"location":"use-cases/order-accuracy/advanced.html#dine-in-commands","title":"Dine-In Commands","text":"<pre><code>make build                  # Build Docker images\nmake up                     # Start services\nmake down                   # Stop services\nmake logs                   # View logs\nmake update-submodules      # Initialize performance-tools\nmake benchmark              # Run benchmark\nmake benchmark-density      # Run stream density test\n</code></pre>"},{"location":"use-cases/order-accuracy/advanced.html#take-away-commands","title":"Take-Away Commands","text":"<pre><code>make build                  # Build Docker images\nmake up                     # Start (single mode)\nmake up-parallel WORKERS=4  # Start (parallel mode)\nmake down                   # Stop services\nmake logs                   # View logs\nmake update-submodules      # Initialize performance-tools\nmake benchmark              # Run benchmark\nmake benchmark-stream-density  # Stream density test\n</code></pre>"},{"location":"use-cases/order-accuracy/advanced.html#common-commands","title":"Common Commands","text":"<ul> <li><code>make clean-images</code> \u2014 Remove dangling Docker images</li> <li><code>make clean-all</code> \u2014 Remove all unused Docker resources</li> <li><code>make check-env</code> \u2014 Verify configuration</li> <li><code>make show-config</code> \u2014 Display current configuration</li> </ul>"},{"location":"use-cases/order-accuracy/advanced.html#configure-system-proxy","title":"Configure System Proxy","text":"<p>Please follow these steps to configure proxy settings:</p>"},{"location":"use-cases/order-accuracy/advanced.html#1-configure-proxy-for-current-shell-session","title":"1. Configure Proxy for Current Shell Session","text":"<pre><code>export http_proxy=http://&lt;proxy-host&gt;:&lt;port&gt;\nexport https_proxy=http://&lt;proxy-host&gt;:&lt;port&gt;\nexport HTTP_PROXY=http://&lt;proxy-host&gt;:&lt;port&gt;\nexport HTTPS_PROXY=http://&lt;proxy-host&gt;:&lt;port&gt;\nexport NO_PROXY=localhost,127.0.0.1,::1\nexport no_proxy=localhost,127.0.0.1,::1\n</code></pre>"},{"location":"use-cases/order-accuracy/advanced.html#2-docker-daemon-proxy-configuration","title":"2. Docker Daemon Proxy Configuration","text":"<p>Create directory if missing: <pre><code>sudo mkdir -p /etc/systemd/system/docker.service.d\nsudo nano /etc/systemd/system/docker.service.d/http-proxy.conf\n</code></pre></p> <p>Add configuration: <pre><code>[Service]\nEnvironment=\"http_proxy=http://&lt;proxy-host&gt;:&lt;port&gt;\"\nEnvironment=\"https_proxy=http://&lt;proxy-host&gt;:&lt;port&gt;\"\nEnvironment=\"no_proxy=localhost,127.0.0.1,::1\"\n</code></pre></p> <p>Reload and restart: <pre><code>sudo systemctl daemon-reload\nsudo systemctl restart docker\n</code></pre></p>"},{"location":"use-cases/order-accuracy/advanced.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"use-cases/order-accuracy/advanced.html#common-issues","title":"Common Issues","text":"<p>OVMS Not Loading: - Ensure GPU drivers are installed - Check model files exist in <code>ovms-service/models/</code> - Verify OVMS endpoint in <code>.env</code></p> <p>VLM Timeout Errors: - Increase <code>API_TIMEOUT</code> in <code>.env</code> - Check GPU memory utilization - Consider using a smaller model precision (INT8)</p> <p>Stream Processing Issues (Take-Away): - Verify RTSP stream URLs are accessible - Check network bandwidth - Consider reducing number of parallel workers</p>"},{"location":"use-cases/order-accuracy/advanced.html#debug-commands","title":"Debug Commands","text":"<pre><code># Check container logs\ndocker logs &lt;container_name&gt;\n\n# Check GPU utilization\nnvidia-smi -l 1\n\n# Check network connectivity\ncurl http://localhost:8001/v2/models\n\n# Verify service health\ncurl http://localhost:8083/health\n</code></pre>"},{"location":"use-cases/order-accuracy/architecture.html","title":"Order Accuracy System Architecture","text":""},{"location":"use-cases/order-accuracy/architecture.html#table-of-contents","title":"Table of Contents","text":"<ol> <li>System Overview</li> <li>Architecture Diagrams</li> <li>Component Details</li> <li>Data Flow</li> <li>Production Features</li> </ol>"},{"location":"use-cases/order-accuracy/architecture.html#system-overview","title":"System Overview","text":"<p>The Order Accuracy platform is an enterprise AI vision system designed for real-time order validation in quick-service restaurant (QSR) environments. The system uses Vision Language Models (VLM) to analyze images or video feeds, automatically identifying items and validating them against order data.</p>"},{"location":"use-cases/order-accuracy/architecture.html#key-features","title":"Key Features","text":"<ul> <li>VLM-Powered Detection: Uses Qwen2.5-VL-7B for accurate item identification</li> <li>Intel Hardware Optimization: Optimized for Intel CPUs and GPUs via OpenVINO</li> <li>Dual Application Support: Dine-In (image-based) and Take-Away (video stream-based)</li> <li>Semantic Matching: Fuzzy matching for item name variations</li> <li>Real-time Processing: Sub-15-second validation for operational efficiency</li> <li>Containerized Deployment: Docker-based deployment with microservices architecture</li> </ul>"},{"location":"use-cases/order-accuracy/architecture.html#architecture-diagrams","title":"Architecture Diagrams","text":""},{"location":"use-cases/order-accuracy/architecture.html#platform-architecture","title":"Platform Architecture","text":"<pre><code>graph TB\n    subgraph \"Order Accuracy Platform\"\n        subgraph \"Dine-In Application\"\n            DUI[Gradio UI :7861]\n            DAPI[FastAPI API :8083]\n            DVLM[VLM Client]\n            DSEM[Semantic Client]\n        end\n\n        subgraph \"Take-Away Application\"\n            TUI[Gradio UI :7860]\n            TAPI[FastAPI API :8080]\n            TSW[Station Workers]\n            TVS[VLM Scheduler]\n            TFS[Frame Selector]\n        end\n\n        subgraph \"Shared Services\"\n            OVMS[OVMS VLMQwen2.5-VL-7B]\n            SEM[Semantic Service]\n            MINIO[MinIO Storage]\n        end\n    end\n\n    DUI --&gt; DAPI\n    DAPI --&gt; DVLM\n    DAPI --&gt; DSEM\n    DVLM --&gt; OVMS\n    DSEM --&gt; SEM\n\n    TUI --&gt; TAPI\n    TAPI --&gt; TSW\n    TSW --&gt; TFS\n    TSW --&gt; TVS\n    TVS --&gt; OVMS\n    TFS --&gt; MINIO\n</code></pre>"},{"location":"use-cases/order-accuracy/architecture.html#dine-in-architecture","title":"Dine-In Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           DINE-IN ORDER ACCURACY                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502             \u2502      \u2502                  \u2502      \u2502                     \u2502    \u2502\n\u2502  \u2502  Gradio UI  \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   FastAPI API    \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   Validation        \u2502    \u2502\n\u2502  \u2502  (Port 7861)\u2502      \u2502   (Port 8083)    \u2502      \u2502   Service           \u2502    \u2502\n\u2502  \u2502             \u2502      \u2502                  \u2502      \u2502                     \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                                \u2502                           \u2502               \u2502\n\u2502                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502               \u2502\n\u2502                    \u2502                       \u2502               \u2502               \u2502\n\u2502                    \u25bc                       \u25bc               \u25bc               \u2502\n\u2502           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502           \u2502  VLM Client    \u2502     \u2502 Semantic Client \u2502 \u2502 Metrics       \u2502    \u2502\n\u2502           \u2502  (Circuit      \u2502     \u2502 (Circuit        \u2502 \u2502 Collector     \u2502    \u2502\n\u2502           \u2502   Breaker)     \u2502     \u2502  Breaker)       \u2502 \u2502               \u2502    \u2502\n\u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                   \u2502                       \u2502                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502                       \u2502\n                    \u25bc                       \u25bc\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u2502   OVMS VLM      \u2502     \u2502   Semantic      \u2502\n          \u2502   (Qwen2.5-VL)  \u2502     \u2502   Service       \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"use-cases/order-accuracy/architecture.html#take-away-architecture","title":"Take-Away Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                            TAKE-AWAY ORDER ACCURACY SYSTEM                           \u2502\n\u2502                                                                                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502  \u2502    RTSP Video   \u2502    \u2502  Frame Selector \u2502    \u2502      Order      \u2502                  \u2502\n\u2502  \u2502     Streams     \u2502\u2500\u2500\u2500\u25b6\u2502     Service     \u2502\u2500\u2500\u2500\u25b6\u2502    Accuracy     \u2502                  \u2502\n\u2502  \u2502   (GStreamer)   \u2502    \u2502     (YOLO)      \u2502    \u2502    Service      \u2502                  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502                                                          \u2502                           \u2502\n\u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                           \u2502\n\u2502         \u2502                                                \u2502                           \u2502\n\u2502         \u25bc                                                \u25bc                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n\u2502  \u2502   VLM Scheduler \u2502                          \u2502    Validation   \u2502                   \u2502\n\u2502  \u2502   (Batcher)     \u2502                          \u2502      Agent      \u2502                   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n\u2502           \u2502                                            \u2502                             \u2502\n\u2502           \u25bc                                            \u25bc                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n\u2502  \u2502    OVMS VLM     \u2502                          \u2502    Semantic     \u2502                   \u2502\n\u2502  \u2502  (Qwen2.5-VL)   \u2502                          \u2502    Service      \u2502                   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n\u2502                                                                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"use-cases/order-accuracy/architecture.html#component-details","title":"Component Details","text":""},{"location":"use-cases/order-accuracy/architecture.html#core-components","title":"Core Components","text":""},{"location":"use-cases/order-accuracy/architecture.html#1-vlm-backend-ovms","title":"1. VLM Backend (OVMS)","text":"<p>OpenVINO Model Server hosting Qwen2.5-VL-7B for vision-language inference.</p> <p>Features: - OpenAI-compatible API (<code>/v3/chat/completions</code>) - INT8 quantization for optimized performance - GPU acceleration via Intel/NVIDIA hardware - Shared model instance for both applications</p> <p>API Usage: <pre><code>response = requests.post(\n    f\"{OVMS_ENDPOINT}/v3/chat/completions\",\n    json={\n        \"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": prompt},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_b64}\"}}\n                ]\n            }\n        ]\n    }\n)\n</code></pre></p>"},{"location":"use-cases/order-accuracy/architecture.html#2-semantic-comparison-service","title":"2. Semantic Comparison Service","text":"<p>AI-powered semantic matching microservice for intelligent item comparison.</p> <p>Matching Strategies: - Exact: Direct string comparison - Semantic: Vector similarity using sentence-transformers - Hybrid: Exact first, then semantic fallback</p> <p>Example Matches: - \"Big Mac\" \u2194 \"Maharaja Mac\" (regional name variant) - \"green apple\" \u2194 \"apple\" (partial match) - \"large fries\" \u2194 \"french fries large\" (word reordering)</p>"},{"location":"use-cases/order-accuracy/architecture.html#3-frame-selector-service-take-away","title":"3. Frame Selector Service (Take-Away)","text":"<p>YOLO-based intelligent frame selection for optimal VLM input.</p> <p>Process: 1. Receive raw video frames from GStreamer pipeline 2. Run YOLO object detection on each frame 3. Score frames by item visibility and clarity 4. Select top K frames per order 5. Store selected frames in MinIO</p>"},{"location":"use-cases/order-accuracy/architecture.html#4-vlm-scheduler-take-away","title":"4. VLM Scheduler (Take-Away)","text":"<p>Request batching scheduler optimizing OVMS throughput.</p> <p>Batching Strategy: - Time Window: 50-100ms collection period - Max Batch Size: Configurable (default: 16) - Fair Scheduling: Round-robin across workers - Response Routing: Match responses to original requesters</p>"},{"location":"use-cases/order-accuracy/architecture.html#docker-services","title":"Docker Services","text":""},{"location":"use-cases/order-accuracy/architecture.html#dine-in-services","title":"Dine-In Services","text":"Container Ports Description <code>dinein_app</code> 7861, 8083 Main application (Gradio + FastAPI) <code>dinein_ovms_vlm</code> 8002 Vision-Language Model server <code>dinein_semantic_service</code> 8081 Semantic text matching <code>metrics-collector</code> 8084 System metrics aggregation"},{"location":"use-cases/order-accuracy/architecture.html#take-away-services","title":"Take-Away Services","text":"Container Ports Description <code>takeaway_app</code> 7860, 8080 Main application (Gradio + FastAPI) <code>ovms-vlm</code> 8001 Vision-Language Model server <code>frame-selector</code> 8085 YOLO-based frame selection <code>semantic-service</code> 8081 Semantic text matching <code>minio</code> 9000, 9001 S3-compatible storage <code>rtsp-streamer</code> 8554 RTSP stream simulator (testing)"},{"location":"use-cases/order-accuracy/architecture.html#data-flow","title":"Data Flow","text":""},{"location":"use-cases/order-accuracy/architecture.html#dine-in-validation-pipeline","title":"Dine-In Validation Pipeline","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        VALIDATION PIPELINE                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                     \u2502\n\u2502  1. IMAGE PREPROCESSING                                            \u2502\n\u2502     Raw Image \u2192 Auto-Orient \u2192 Resize (672px) \u2192 Enhance \u2192           \u2502\n\u2502     Sharpen \u2192 JPEG Compress (82%) \u2192 Base64 Encode                  \u2502\n\u2502                              \u2502                                      \u2502\n\u2502                              \u25bc                                      \u2502\n\u2502  2. VLM INFERENCE                                                  \u2502\n\u2502     Prompt: \"Analyze this food plate image...\"                     \u2502\n\u2502     + Inventory list for context                                   \u2502\n\u2502     \u2192 OVMS POST /v3/chat/completions                               \u2502\n\u2502     \u2192 Parse JSON response for detected items                       \u2502\n\u2502                              \u2502                                      \u2502\n\u2502                              \u25bc                                      \u2502\n\u2502  3. SEMANTIC MATCHING                                              \u2502\n\u2502     For each expected item:                                        \u2502\n\u2502       Find best match in detected items (similarity &gt; 0.7)         \u2502\n\u2502       Track: matched, missing, extra, quantity mismatches          \u2502\n\u2502                              \u2502                                      \u2502\n\u2502                              \u25bc                                      \u2502\n\u2502  4. RESULT AGGREGATION                                             \u2502\n\u2502     {                                                              \u2502\n\u2502       \"order_complete\": true/false,                                \u2502\n\u2502       \"accuracy_score\": 0.0-1.0,                                   \u2502\n\u2502       \"missing_items\": [...],                                      \u2502\n\u2502       \"extra_items\": [...]                                         \u2502\n\u2502     }                                                              \u2502\n\u2502                                                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"use-cases/order-accuracy/architecture.html#take-away-processing-pipeline","title":"Take-Away Processing Pipeline","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         DATA FLOW PIPELINE                                  \u2502\n\u2502                                                                             \u2502\n\u2502  1. VIDEO CAPTURE                                                          \u2502\n\u2502     RTSP Camera \u2500\u2500\u25b6 GStreamer Pipeline \u2500\u2500\u25b6 Frame Buffer                   \u2502\n\u2502                                                                             \u2502\n\u2502  2. FRAME SELECTION                                                        \u2502\n\u2502     Frame Selector (YOLO):                                                 \u2502\n\u2502     \u2022 Object detection on raw frames                                       \u2502\n\u2502     \u2022 Score frames by item visibility                                      \u2502\n\u2502     \u2022 Select top K frames per order                                        \u2502\n\u2502     \u2022 Store selected frames in MinIO                                       \u2502\n\u2502                                                                             \u2502\n\u2502  3. VLM PROCESSING                                                         \u2502\n\u2502     VLM Scheduler \u2192 OVMS (Qwen2.5-VL):                                    \u2502\n\u2502     \u2022 Batch frames by time window                                          \u2502\n\u2502     \u2022 Send to OVMS with detection prompt                                   \u2502\n\u2502     \u2022 Parse structured item response                                       \u2502\n\u2502                                                                             \u2502\n\u2502  4. ORDER VALIDATION                                                       \u2502\n\u2502     Validation Agent:                                                      \u2502\n\u2502     \u2022 Compare detected items with expected order                           \u2502\n\u2502     \u2022 Exact match \u2192 Semantic match \u2192 Flag mismatch                        \u2502\n\u2502     \u2022 Generate validation result                                           \u2502\n\u2502                                                                             \u2502\n\u2502  5. RESULT OUTPUT                                                          \u2502\n\u2502     { \"matched\": [...], \"missing\": [...], \"extra\": [...] }                \u2502\n\u2502                                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"use-cases/order-accuracy/architecture.html#production-features","title":"Production Features","text":""},{"location":"use-cases/order-accuracy/architecture.html#circuit-breaker-pattern","title":"Circuit Breaker Pattern","text":"<p>Prevents cascading failures when external services are unhealthy.</p> <pre><code>                    5 consecutive failures\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502        \u2502                               \u2502        \u2502\n       \u2502 CLOSED \u2502                               \u2502  OPEN  \u2502\n       \u2502        \u2502 \u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502        \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    2 successes in half_open  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u25b2                                       \u2502\n            \u2502                                       \u2502\n            \u2502        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502 HALF_OPEN  \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      2 successes    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    30s timeout\n                           \u2502\n                           \u2502 1 failure \u2192 Back to OPEN\n</code></pre> <p>Configuration: - VLM Client: 5 failures \u2192 OPEN, 30s recovery \u2192 HALF_OPEN - Semantic Client: 15s recovery timeout (faster than VLM)</p>"},{"location":"use-cases/order-accuracy/architecture.html#connection-pooling","title":"Connection Pooling","text":"<pre><code># VLM Client Pool Configuration\nlimits = httpx.Limits(\n    max_keepalive_connections=20,\n    max_connections=50,\n    keepalive_expiry=30.0\n)\ntimeout = httpx.Timeout(\n    connect=10.0,\n    read=300.0,   # Extended for VLM inference\n    write=10.0,\n    pool=10.0\n)\n</code></pre>"},{"location":"use-cases/order-accuracy/architecture.html#bounded-cache-lru","title":"Bounded Cache (LRU)","text":"<p>Thread-safe LRU cache with automatic eviction to prevent memory exhaustion: - Maximum 10,000 entries - Automatic eviction of oldest entries when full - Thread-safe operations with locking</p>"},{"location":"use-cases/order-accuracy/architecture.html#station-worker-reliability-take-away","title":"Station Worker Reliability (Take-Away)","text":"Feature Implementation GStreamer Pipeline RTSP \u2192 H.264 decode \u2192 Frame capture Circuit Breaker 5 failures in 5 min \u2192 30s cooldown Exponential Backoff 2s \u2192 4s \u2192 8s \u2192 ... \u2192 60s max Stall Detection No frames for 5 min triggers restart Health Monitoring Frame rate, pipeline state tracking"},{"location":"use-cases/order-accuracy/architecture.html#performance-characteristics","title":"Performance Characteristics","text":"Metric Dine-In Take-Away End-to-End Latency 8-15 seconds Real-time stream VLM Inference 5-10 seconds 5-10 seconds (batched) Semantic Matching 50-200ms 50-200ms Throughput ~4-6 req/min Multiple concurrent streams GPU Utilization 60-80% 70-90% (parallel mode)"},{"location":"use-cases/order-accuracy/getting_started.html","title":"Getting Started","text":""},{"location":"use-cases/order-accuracy/getting_started.html#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<ul> <li>Ubuntu 24.04 or newer (Linux recommended), Desktop edition (or Server edition with GUI installed)</li> <li>Docker 24.0+</li> <li>Docker Compose V2+</li> <li>Make (<code>sudo apt install make</code>)</li> <li>Intel hardware (CPU, iGPU, dGPU)</li> <li>Intel drivers:<ul> <li>Intel GPU drivers</li> </ul> </li> <li>Sufficient disk space for models, videos, and results (50GB minimum)</li> </ul> <p>Note</p> <p>First-time setup downloads AI models (~7GB) and Docker images - this may take 30-60 minutes depending on your internet connection.</p>"},{"location":"use-cases/order-accuracy/getting_started.html#choose-your-application","title":"Choose Your Application","text":""},{"location":"use-cases/order-accuracy/getting_started.html#dine-in-order-accuracy","title":"\ud83c\udf7d\ufe0f Dine-In Order Accuracy","text":"<p>Purpose: Validate food plates at serving stations before delivery to tables Use When: You need image-based validation for restaurant table service Input: Static images of food trays/plates Features: Gradio web interface, REST API for POS integration</p>"},{"location":"use-cases/order-accuracy/getting_started.html#take-away-order-accuracy","title":"\ud83e\udd61 Take-Away Order Accuracy","text":"<p>Purpose: Real-time order validation for drive-through and counter service Use When: You need continuous video stream validation at multiple stations Input: RTSP video streams Features: Multi-station parallel processing, VLM request batching</p>"},{"location":"use-cases/order-accuracy/getting_started.html#quick-start-reference","title":"Quick Start Reference","text":""},{"location":"use-cases/order-accuracy/getting_started.html#dine-in-quick-commands","title":"\ud83c\udf7d\ufe0f Dine-In Quick Commands","text":"Configuration Command Description Start Services <code>make up</code> Start all dine-in services Build Locally <code>make build REGISTRY=false</code> Build images from source View Logs <code>make logs</code> View service logs Stop Services <code>make down</code> Stop all containers"},{"location":"use-cases/order-accuracy/getting_started.html#take-away-quick-commands","title":"\ud83e\udd61 Take-Away Quick Commands","text":"Configuration Command Description Single Mode <code>make up</code> Start in single worker mode (development) Parallel Mode <code>make up-parallel WORKERS=4</code> Start with 4 parallel workers (production) Build Locally <code>make build REGISTRY=false</code> Build images from source View Logs <code>make logs</code> View service logs <p>Tip</p> <p>Single Mode is best for development and testing. Parallel Mode is recommended for production with multiple camera stations.</p>"},{"location":"use-cases/order-accuracy/getting_started.html#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"use-cases/order-accuracy/getting_started.html#option-1-dine-in-order-accuracy","title":"Option 1: Dine-In Order Accuracy","text":"<ol> <li> <p>Clone the Repository <pre><code>git clone -b &lt;release-or-tag&gt; --single-branch https://github.com/intel-retail/order-accuracy\n</code></pre>     &gt;Replace <code>&lt;release-or-tag&gt;</code> with the version you want to clone (for example, v2026.0).     <pre><code>git clone -b v2026.0 --single-branch https://github.com/intel-retail/order-accuracy\ncd order-accuracy/dine-in\n</code></pre></p> </li> <li> <p>Setup OVMS Models (First Time Only) <pre><code>cd ../ovms-service\n./setup_models.sh\ncd ../dine-in\n</code></pre>     This downloads and converts the Qwen2.5-VL-7B model (~7GB). This only needs to be done once.</p> </li> <li> <p>Prepare Test Data</p> <ul> <li>Add your food tray/plate images to the <code>images/</code> folder</li> <li>Update <code>configs/orders.json</code> with test orders</li> <li>Update <code>configs/inventory.json</code> with your menu items</li> </ul> </li> <li> <p>Build and Start Services <pre><code># Using pre-built images (recommended for first run)\nmake build\nmake up\n\n# OR build locally from source\nmake build REGISTRY=false\nmake up\n</code></pre></p> </li> <li> <p>Access the Application</p> <ul> <li>Gradio UI: http://localhost:7861</li> <li>REST API Docs: http://localhost:8083/docs</li> </ul> </li> </ol>"},{"location":"use-cases/order-accuracy/getting_started.html#option-2-take-away-order-accuracy","title":"Option 2: Take-Away Order Accuracy","text":"<ol> <li> <p>Clone the Repository <pre><code>git clone -b &lt;release-or-tag&gt; --single-branch https://github.com/intel-retail/order-accuracy\n</code></pre>     &gt;Replace <code>&lt;release-or-tag&gt;</code> with the version you want to clone (for example, v2026.0).     <pre><code>git clone -b v2026.0 --single-branch https://github.com/intel-retail/order-accuracy\ncd order-accuracy/take-away\n</code></pre></p> </li> <li> <p>Setup OVMS Models (First Time Only) <pre><code>cd ../ovms-service\n./setup_models.sh\ncd ../take-away\n</code></pre>     This downloads the VLM and EasyOCR models. This only needs to be done once.</p> </li> <li> <p>Initialize Environment <pre><code>make init-env\n</code></pre></p> </li> <li> <p>Build and Start Services <pre><code># Single worker mode (development/testing)\nmake build\nmake up\n\n# OR Parallel mode (production)\nmake build\nmake up-parallel WORKERS=4\n</code></pre></p> </li> <li> <p>Access the Application</p> <ul> <li>Gradio UI: http://localhost:7860</li> <li>MinIO Console: http://localhost:9001 (minioadmin/minioadmin)</li> </ul> </li> </ol>"},{"location":"use-cases/order-accuracy/getting_started.html#what-youll-see-when-working","title":"What You'll See When Working","text":""},{"location":"use-cases/order-accuracy/getting_started.html#dine-in-results","title":"\ud83c\udf7d\ufe0f Dine-In Results","text":"<ul> <li>Visual: Gradio UI displays detected items with confidence scores</li> <li>Validation: Order match/mismatch status with detailed comparison</li> <li>Logs: Detection results and semantic matching scores</li> </ul>"},{"location":"use-cases/order-accuracy/getting_started.html#take-away-results","title":"\ud83e\udd61 Take-Away Results","text":"<ul> <li>Visual: Real-time frame processing with item detection</li> <li>Validation: Continuous order validation against POS data</li> <li>Storage: Frames and results stored in MinIO buckets</li> </ul>"},{"location":"use-cases/order-accuracy/getting_started.html#expected-performance","title":"Expected Performance","text":"<ul> <li>Startup Time: 2-5 minutes (first run includes model loading)</li> <li>Processing: Sub-15-second validation latency (Dine-In), real-time stream processing (Take-Away)</li> <li>Results: JSON files appear in <code>results/</code> directory</li> </ul>"},{"location":"use-cases/order-accuracy/getting_started.html#verify-results","title":"Verify Results","text":"<p>After starting the application, you can verify it's working:</p> <p>Dine-In: <pre><code># Check running containers\ndocker ps\n\n# View application logs\nmake logs\n\n# Test the API\ncurl http://localhost:8083/health\n</code></pre></p> <p>Take-Away: <pre><code># Check running containers\ndocker ps\n\n# View application logs\nmake logs\n\n# Check MinIO storage\n# Visit http://localhost:9001\n</code></pre></p>"},{"location":"use-cases/order-accuracy/getting_started.html#stop-the-services","title":"Stop the Services","text":"<pre><code># Stop all services\nmake down\n\n# Stop and remove volumes (clean restart)\nmake down-volumes\n</code></pre>"},{"location":"use-cases/order-accuracy/getting_started.html#proceed-to-advanced-settings","title":"Proceed to Advanced Settings","text":""},{"location":"use-cases/order-accuracy/order-accuracy.html","title":"Intel\u00ae Order Accuracy Reference Package","text":""},{"location":"use-cases/order-accuracy/order-accuracy.html#overview","title":"Overview","text":"<p>Order Accuracy is an enterprise AI vision platform that validates food orders in real-time using Vision Language Models (VLM). The platform automatically detects items in food trays, bags, or containers, compares them against expected order data, and identifies discrepancies before orders reach customers.</p> <p>As computer vision technology becomes more mainstream in industrial and retail settings, using it for order accuracy in quick service restaurants is becoming increasingly complex. These vision workloads are substantial and require multiple stages of processing. The Intel\u00ae Order Accuracy Reference Package provides the essential components needed to develop and deploy order accuracy solutions using Intel\u00ae hardware, software, and open-source tools.</p>"},{"location":"use-cases/order-accuracy/order-accuracy.html#platform-applications","title":"Platform Applications","text":"<p>The platform provides two specialized applications optimized for different restaurant scenarios:</p> Application Use Case Input Type Dine-In Restaurant table service validation Static images Take-Away Drive-through and counter service Video streams (RTSP)"},{"location":"use-cases/order-accuracy/order-accuracy.html#dine-in-order-accuracy","title":"Dine-In Order Accuracy","text":"<p>Image-based order validation for restaurant dining applications</p> <p>Optimized for validating food trays at serving stations before delivery to tables. Uses single image capture and VLM analysis for fast, accurate item detection.</p>"},{"location":"use-cases/order-accuracy/order-accuracy.html#key-features","title":"Key Features","text":"<ul> <li>Single image capture and analysis</li> <li>Food tray/plate item detection</li> <li>REST API for POS integration</li> <li>Gradio web interface for manual validation</li> <li>Hybrid semantic matching</li> <li>Zero-training deployment with pre-trained Qwen2.5-VL-7B model</li> </ul>"},{"location":"use-cases/order-accuracy/order-accuracy.html#use-case","title":"Use Case","text":"<p>In a full-service restaurant: 1. Kitchen prepares a dish for Table 12 2. Expo staff places the plate in the validation station 3. Staff triggers validation via Gradio UI or API 4. System analyzes plate contents using VLM 5. System compares detected items against the order manifest 6. Staff receives immediate feedback on order accuracy</p>"},{"location":"use-cases/order-accuracy/order-accuracy.html#take-away-order-accuracy","title":"Take-Away Order Accuracy","text":"<p>Real-time video stream validation for drive-through and counter service</p> <p>Optimized for high-throughput drive-through environments with multiple camera stations. Processes RTSP video streams in parallel using intelligent frame selection and VLM batching.</p>"},{"location":"use-cases/order-accuracy/order-accuracy.html#key-features_1","title":"Key Features","text":"<ul> <li>Real-time RTSP video stream processing</li> <li>Multi-station parallel processing (up to 8 workers)</li> <li>GStreamer-based video pipeline</li> <li>YOLO-powered frame selection</li> <li>VLM request batching for throughput</li> <li>Circuit breaker and auto-recovery</li> </ul>"},{"location":"use-cases/order-accuracy/order-accuracy.html#service-modes","title":"Service Modes","text":"Mode Description Use Case Single Single worker, Gradio UI Development, testing Parallel Multi-worker, VLM scheduler Production deployment"},{"location":"use-cases/order-accuracy/order-accuracy.html#choosing-the-right-application","title":"Choosing the Right Application","text":"Criteria Dine-In Take-Away Input Type Static images Video streams (RTSP) Throughput Low-medium High (parallel) Latency Priority Accuracy over speed Speed and accuracy Camera Setup Fixed position Multi-station Typical Use Table service Drive-through, counter Processing Single request Batch processing"},{"location":"use-cases/order-accuracy/order-accuracy.html#recommendation","title":"Recommendation","text":"<ul> <li>Choose Dine-In if you need to validate orders from captured images at serving stations</li> <li>Choose Take-Away if you need real-time validation from continuous video streams</li> </ul>"},{"location":"use-cases/order-accuracy/order-accuracy.html#shared-platform-components","title":"Shared Platform Components","text":""},{"location":"use-cases/order-accuracy/order-accuracy.html#vlm-backend-ovms","title":"VLM Backend (OVMS)","text":"<p>Both applications use OpenVINO Model Server with Qwen2.5-VL for vision-language inference.</p>"},{"location":"use-cases/order-accuracy/order-accuracy.html#semantic-comparison-service","title":"Semantic Comparison Service","text":"<p>AI-powered semantic matching microservice for intelligent item comparison:</p> <ul> <li>Matching Strategies: Exact \u2192 Semantic \u2192 Hybrid</li> <li>Example: Matches \"green apple\" \u2194 \"apple\" using semantic reasoning</li> <li>Fallback: Automatic fallback to local matching if service unavailable</li> </ul>"},{"location":"use-cases/order-accuracy/order-accuracy.html#what-you-want-to-do","title":"What You Want to Do","text":""},{"location":"use-cases/order-accuracy/order-accuracy.html#im-new-to-order-accuracy-solutions","title":"\ud83d\ude80 I'm New to Order Accuracy Solutions","text":"<p>Quick Start (25 minutes): Getting Started Guide - Set up your development environment - Run your first order validation demo - Understand the platform workflow</p>"},{"location":"use-cases/order-accuracy/order-accuracy.html#i-want-to-customize-the-solution","title":"\u2699\ufe0f I Want to Customize the Solution","text":"<p>Advanced Configuration (45-90 minutes): Advanced Guide - Configure custom settings and workloads - Tune performance parameters - Set up benchmarking</p>"},{"location":"use-cases/order-accuracy/order-accuracy.html#i-want-to-understand-the-architecture","title":"\ud83d\udcd0 I Want to Understand the Architecture","text":"<p>System Design: Architecture Guide - Understand system components - Review data flow diagrams - Learn about service interactions</p>"},{"location":"use-cases/order-accuracy/order-accuracy.html#i-need-performance-analysis","title":"\ud83d\udcca I Need Performance Analysis","text":"<p>Benchmark &amp; Optimize: Performance Guide - Compare CPU/GPU performance - Run stream density tests - Optimize for your hardware</p>"},{"location":"use-cases/order-accuracy/performance.html","title":"Performance Testing &amp; Benchmarking","text":"<p>Test your Order Accuracy pipeline performance on various hardware configurations. This guide covers everything from quick performance checks to comprehensive system capacity testing.</p>"},{"location":"use-cases/order-accuracy/performance.html#quick-start-5-minutes","title":"Quick Start (5 minutes)","text":"<p>Goal: Run a basic performance test to verify your system works correctly</p>"},{"location":"use-cases/order-accuracy/performance.html#1-initialize-performance-tools","title":"1. Initialize Performance Tools","text":"<pre><code>make update-submodules\n</code></pre>"},{"location":"use-cases/order-accuracy/performance.html#2-run-quick-benchmark","title":"2. Run Quick Benchmark","text":"<p>Dine-In: <pre><code>cd dine-in\nmake benchmark\n</code></pre></p> <p>Take-Away: <pre><code>cd take-away\nmake benchmark\n</code></pre></p> <p>What this does: - Tests GPU/CPU performance for order validation - Measures end-to-end latency - Generates performance metrics - Outputs results to <code>results/</code> directory</p>"},{"location":"use-cases/order-accuracy/performance.html#understanding-benchmark-types","title":"Understanding Benchmark Types","text":""},{"location":"use-cases/order-accuracy/performance.html#dine-in-benchmarks","title":"Dine-In Benchmarks","text":""},{"location":"use-cases/order-accuracy/performance.html#single-request-benchmark","title":"Single Request Benchmark","text":"<pre><code>make benchmark\n</code></pre> <p>Tests single image validation latency: - Image preprocessing time - VLM inference time - Semantic matching time - Total end-to-end latency</p>"},{"location":"use-cases/order-accuracy/performance.html#stream-density-benchmark","title":"Stream Density Benchmark","text":"<pre><code>make benchmark-density\n</code></pre> <p>Finds maximum concurrent requests the system can handle under latency constraints: - Target latency threshold (configurable) - Progressive load increase - Identifies performance ceiling</p>"},{"location":"use-cases/order-accuracy/performance.html#take-away-benchmarks","title":"Take-Away Benchmarks","text":""},{"location":"use-cases/order-accuracy/performance.html#single-video-benchmark","title":"Single Video Benchmark","text":"<pre><code>make benchmark\n</code></pre> <p>Tests end-to-end latency for single order validation: - Video upload time - Frame extraction time - VLM inference latency - Validation time - Total processing time</p>"},{"location":"use-cases/order-accuracy/performance.html#fixed-workers-benchmark","title":"Fixed Workers Benchmark","text":"<pre><code>make benchmark-oa BENCHMARK_WORKERS=4 BENCHMARK_DURATION=300\n</code></pre> <p>Tests system with fixed number of concurrent workers: - Throughput (orders/minute) - Latency percentiles (P50, P95, P99) - GPU utilization - Memory usage</p>"},{"location":"use-cases/order-accuracy/performance.html#stream-density-benchmark_1","title":"Stream Density Benchmark","text":"<pre><code>make benchmark-stream-density\n</code></pre> <p>Finds maximum sustainable worker count under latency constraints: - Maximum concurrent workers - Latency at each worker count - Point of degradation - Resource utilization at capacity</p>"},{"location":"use-cases/order-accuracy/performance.html#environment-variables-reference","title":"Environment Variables Reference","text":""},{"location":"use-cases/order-accuracy/performance.html#dine-in-configuration","title":"Dine-In Configuration","text":"Variable Default Description <code>TARGET_LATENCY_MS</code> 15000 Target latency threshold (ms) <code>LATENCY_METRIC</code> avg 'avg', 'p95', or 'max' <code>DENSITY_INCREMENT</code> 1 Concurrent images per iteration <code>INIT_DURATION</code> 60 Warmup time (seconds) <code>MIN_REQUESTS</code> 3 Min requests before measuring <code>REQUEST_TIMEOUT</code> 300 Individual request timeout (seconds) <code>API_ENDPOINT</code> http://localhost:8083 API endpoint URL <code>RESULTS_DIR</code> ./results Results output directory"},{"location":"use-cases/order-accuracy/performance.html#take-away-configuration","title":"Take-Away Configuration","text":"Variable Default Description <code>TARGET_LATENCY_MS</code> 25000 Target latency threshold (ms) <code>LATENCY_METRIC</code> avg 'avg', 'p95', or 'max' <code>WORKER_INCREMENT</code> 1 Workers added per iteration <code>INIT_DURATION</code> 10 Warmup time (seconds) <code>MIN_TRANSACTIONS</code> 3 Min transactions before measuring <code>MAX_ITERATIONS</code> 50 Max scaling iterations <code>MAX_WAIT_SEC</code> 600 Max wait per iteration (seconds) <code>BENCHMARK_WORKERS</code> 1 Number of workers (fixed mode) <code>BENCHMARK_DURATION</code> 60 Test duration (seconds)"},{"location":"use-cases/order-accuracy/performance.html#hardware-testing-commands","title":"Hardware Testing Commands","text":""},{"location":"use-cases/order-accuracy/performance.html#gpu-performance-testing","title":"GPU Performance Testing","text":"<p>Dine-In: <pre><code># Ensure GPU device is configured in .env\n# OPENVINO_DEVICE=GPU\nmake benchmark\n</code></pre></p> <p>Take-Away: <pre><code># Configure GPU in .env\n# OPENVINO_DEVICE=GPU\nmake benchmark-oa BENCHMARK_WORKERS=4\n</code></pre></p>"},{"location":"use-cases/order-accuracy/performance.html#multi-worker-stress-testing-take-away","title":"Multi-Worker Stress Testing (Take-Away)","text":"<pre><code># Test with 2 parallel workers\nmake up-parallel WORKERS=2\nmake benchmark-oa BENCHMARK_WORKERS=2\n\n# High stress test with 8 workers\nmake up-parallel WORKERS=8\nmake benchmark-oa BENCHMARK_WORKERS=8\n</code></pre>"},{"location":"use-cases/order-accuracy/performance.html#progressive-load-testing","title":"Progressive Load Testing","text":"<pre><code># Automatically find maximum sustainable workers\nmake benchmark-stream-density \\\n  BENCHMARK_TARGET_LATENCY_MS=25000 \\\n  BENCHMARK_WORKER_INCREMENT=1 \\\n  BENCHMARK_MAX_ITERATIONS=20\n</code></pre>"},{"location":"use-cases/order-accuracy/performance.html#viewing-results","title":"Viewing Results","text":""},{"location":"use-cases/order-accuracy/performance.html#dine-in-results","title":"Dine-In Results","text":"<pre><code># View density benchmark results\nmake benchmark-density-results\n\n# View raw results\ncat results/benchmark_results.json\nls -la results/\n</code></pre>"},{"location":"use-cases/order-accuracy/performance.html#take-away-results","title":"Take-Away Results","text":"<pre><code># View benchmark results\nmake benchmark-oa-results\n\n# View density results\ncat results/stream_density_results.json\nls -la results/\n</code></pre>"},{"location":"use-cases/order-accuracy/performance.html#consolidate-metrics","title":"Consolidate Metrics","text":"<pre><code>make consolidate-metrics\ncat results/metrics_summary.csv\n</code></pre>"},{"location":"use-cases/order-accuracy/performance.html#expected-performance","title":"Expected Performance","text":""},{"location":"use-cases/order-accuracy/performance.html#typical-latency-ranges","title":"Typical Latency Ranges","text":"Operation Dine-In Take-Away Image Preprocessing 100-500ms N/A Frame Selection N/A 200-500ms VLM Inference 5-10s 5-10s Semantic Matching 50-200ms 50-200ms Total End-to-End 8-15s 8-15s per order"},{"location":"use-cases/order-accuracy/performance.html#hardware-impact","title":"Hardware Impact","text":"Configuration Typical Performance CPU Only 15-25s per validation Intel iGPU 8-15s per validation Intel Arc dGPU 5-10s per validation NVIDIA RTX 4-8s per validation"},{"location":"use-cases/order-accuracy/performance.html#throughput-expectations","title":"Throughput Expectations","text":"Mode Expected Throughput Dine-In Single 4-6 orders/minute Take-Away Single 4-6 orders/minute Take-Away Parallel (4 workers) 16-24 orders/minute Take-Away Parallel (8 workers) 30-40 orders/minute"},{"location":"use-cases/order-accuracy/performance.html#optimization-tips","title":"Optimization Tips","text":""},{"location":"use-cases/order-accuracy/performance.html#gpu-utilization","title":"GPU Utilization","text":"<ul> <li>Monitor GPU usage with <code>nvidia-smi -l 1</code> or <code>intel_gpu_top</code></li> <li>Target 70-90% GPU utilization for optimal throughput</li> <li>If GPU is underutilized, increase worker count</li> </ul>"},{"location":"use-cases/order-accuracy/performance.html#memory-management","title":"Memory Management","text":"<ul> <li>Monitor container memory with <code>docker stats</code></li> <li>VLM models require 8-16GB GPU memory</li> <li>Reduce batch size if out-of-memory errors occur</li> </ul>"},{"location":"use-cases/order-accuracy/performance.html#network-optimization-take-away","title":"Network Optimization (Take-Away)","text":"<ul> <li>Use wired connections for RTSP streams</li> <li>Ensure 1Gbps+ network bandwidth per camera</li> <li>Consider local video storage for testing</li> </ul>"},{"location":"use-cases/order-accuracy/performance.html#latency-reduction","title":"Latency Reduction","text":"<ul> <li>Use INT8 model quantization</li> <li>Enable HTTP/2 for API connections</li> <li>Pre-warm VLM model before benchmarking</li> </ul>"},{"location":"use-cases/order-accuracy/performance.html#troubleshooting-performance-issues","title":"Troubleshooting Performance Issues","text":""},{"location":"use-cases/order-accuracy/performance.html#low-fps-high-latency","title":"Low FPS / High Latency","text":"<ul> <li>Check GPU driver installation</li> <li>Verify OPENVINO_DEVICE setting in .env</li> <li>Reduce image resolution or batch size</li> <li>Check for thermal throttling</li> </ul>"},{"location":"use-cases/order-accuracy/performance.html#vlm-timeout-errors","title":"VLM Timeout Errors","text":"<ul> <li>Increase API_TIMEOUT in .env</li> <li>Check GPU memory availability</li> <li>Consider using smaller model precision</li> </ul>"},{"location":"use-cases/order-accuracy/performance.html#memory-exhaustion","title":"Memory Exhaustion","text":"<ul> <li>Reduce number of parallel workers</li> <li>Lower batch size settings</li> <li>Monitor with <code>docker stats</code></li> </ul>"},{"location":"use-cases/order-accuracy/performance.html#inconsistent-results","title":"Inconsistent Results","text":"<ul> <li>Increase warmup duration (INIT_DURATION)</li> <li>Increase minimum transactions (MIN_TRANSACTIONS)</li> <li>Run multiple benchmark iterations</li> </ul>"}]}