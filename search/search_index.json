{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Intel Retail Documentation","text":"<p>Welcome to the Intel Retail organization.</p>"},{"location":"index.html#learn-more-about-our-features","title":"Learn more about our features","text":"<p>Architecture</p> <p>Performance Tools</p> <p>Use Cases</p> <p>Releases</p>"},{"location":"releasenotes.html","title":"Releases","text":"<p>Release v1.0.1</p> <p>Release v1.5.0</p> <p>Release v2.0.0</p> <p>Release v2.1.0</p> <p>Release v3.0.0</p> <p>Release v3.1.0</p> <p>Release v3.2.0</p>"},{"location":"troubleshooting.html","title":"Troubleshooting","text":""},{"location":"troubleshooting.html#q-why-is-the-performance-on-cpu-sometimes-better-than-on-gpu-when-running-pipeline-benchmarking-like-stream-density","title":"Q: Why is the performance on CPU sometimes better than on GPU, when running pipeline benchmarking like stream density?","text":"<p>A: The performance of pipeline benchmarking strongly depends on the models.  Specifically for <code>yolo11n</code> object detection, it is recommended to use the model precision FP32 when it is running on device <code>GPU</code>.  If supported, then you can change the model precision by updating the pipeline script to point to the precision of your choice.  For example, you can change the model of <code>FP16</code> to <code>FP32</code> assuming the precision <code>FP32</code> of the target model is available:  </p> <pre><code>    src/pipelines/yolo11n.sh\n\n    ...\n    model=models/object_detection/yolo11n/FP16/yolo11n.xml\n    ...\n</code></pre>"},{"location":"troubleshooting.html#q-what-happens-if-the-system-keeps-crashing-when-building-the-dlstreamer-realsense-image","title":"Q: What happens if the system keeps crashing when building the <code>dlstreamer-realsense</code> image?","text":"<p>A: Some systems may run into issues with memory when building the <code>dlstreamer-realsense</code> image. In the <code>Dockerfile.dlstreamer</code> file, change the make command to not use the <code>-j</code> threading option.</p> <pre><code>- make -j\"$(cat &lt; /proc/cpuinfo |grep -c proc)\" &amp;&amp;\n+ make &amp;&amp;\n</code></pre>"},{"location":"troubleshooting.html#q-what-happens-if-the-rtsp-source-is-not-found","title":"Q: What happens if the RTSP source is not found?","text":"<p>A: Depending on your systems performance it is possible that the RTSP simulator may take additional time to initialize and start streaming. To avoid issues you can add a waiting period before the pipeline starts. For example you can add a 5 second sleep timer to /src/entrypoint.sh</p> <pre><code>sleep 5 # sleep for 5 seconds before starting the pipeline\neval $gstLaunchCmd\n</code></pre>"},{"location":"troubleshooting.html#q-how-can-i-use-an-intel-realsense-camera-as-the-input","title":"Q: How can I use an Intel RealSense Camera as the input?","text":"<p>A: To use a RealSense camara as an input for any of the retail use cases, you need to obtain the serial number first. Follow the instructions here to run <code>rs-enumerate-devices</code> and obtain the following information:</p> <pre><code>Device Name                   Serial Number       Firmware Version\nIntel RealSense D415          725112060400        05.12.02.100\nDevice info:\n    Name                          :     Intel RealSense D415\n    Serial Number                 :     725112060400\n    Firmware Version              :     05.12.02.100\n    Recommended Firmware Version  :     05.12.02.100\n    Physical Port                 :     \\\\?\\usb#vid_8086&amp;pid_0ad3&amp;mi_00#6&amp;2a371216&amp;0&amp;0000#{e5323777-f976-4f5b-9b55-b94699c46e44}\\global\n    Debug Op Code                 :     15\n    Advanced Mode                 :     YES\n    Product Id                    :     0AD3\n    Camera Locked                 :     NO\n    Usb Type Descriptor           :     3.2\n    Product Line                  :     D400\n    Asic Serial Number            :     012345678901\n    Firmware Update Id            :     012345678901\n</code></pre> <p>Simply add the serial number to the <code>INPUTSRC</code> argument when calling the pipelines:</p> <pre><code>INPUTSRC=725112060400 make run-demo\n</code></pre>"},{"location":"workshops.html","title":"Workshop Collaterals","text":"<p>ASU - 25 Jan 2025</p>"},{"location":"Architecture/pipelines.html","title":"Intel Retail","text":""},{"location":"Architecture/pipelines.html#repositories","title":"Repositories","text":"<p>In release v3.0.0 Intel-retail modules have been organized into logical repositories. By taking advantage of Github submodules different modules can be referenced from other repositories. For example, performance tools are being used by retail-use-cases and automated-self-checkout. Rather than duplicating and maintaining performance tools between the two repositories we linked the latest performance tools release as a submodule. </p> <p></p>"},{"location":"Architecture/pipelines.html#frameworks","title":"Frameworks","text":""},{"location":"Architecture/pipelines.html#openvino","title":"OpenVINO","text":"<p>OpenVINO is an open source toolkit provided by Intel to assist with running AI and ML on Intel hardware. The tools include a portable inference engine that is compatible with different Intel hardware platforms. The code can be found on the OpenVINO Github and examples can be ran with OpenVINO Jupyter Notebooks.</p> <p>OpenVINO provides some pre-trained models for quick development and testing through the OpenVINO Model Zoo. OpenVINO also supports converting models through they Model Conversion Process</p> <p>Details about the latest version can be found in the OpenVINO Release Notes.</p>"},{"location":"Architecture/pipelines.html#dlstreamer-pipeline","title":"DLStreamer Pipeline","text":"<p>Rather than working directly with the OpenVINO APIs our solutions offers more practical ways to interface with OpenVINO. One method is using Intel DLStreamer. This solution provides a no code way based on GStreamer and OpenVINO to deploy, process, and output a pipeline. </p> <p>The diagrams show how we take advantage of Docker, Docker Compose, and environment variable files to pre-package DLStreamer based pipelines for our use cases. Leveraging Environment Variables allows users to modify properties on the fly when different configurations are required.</p> <p></p>"},{"location":"Architecture/pipelines.html#openvino-model-serverovms","title":"OpenVINO Model Server(OVMS)","text":"<p>Another solution is the OpenVINO Model Server(OVMS). OVMS is a model hosting server that hosts inference models through a set of APIs. Unlike DLStreamer this solution requires developers to write code for pre and post processing model inference results. The advantage is the additional control developers have over their inference processing. Another benefit is the distribution of inference workloads between multiple servers either locally or remotely.</p>"},{"location":"Architecture/pipelines.html#openvino-model-serverovms-pipeline-with-grpc-remote-procedure-call","title":"OpenVINO Model Server(OVMS) Pipeline with gRPC (Remote Procedure Call)","text":"<p>There are two methods for running your inference through OVMS. The more flexible method has the client use gRPC (Remote Procedure Call) to request inference results from OVMS. By providing the proper input type and format the client can push inference compute to OVMS. OVMS can be local or on a remote system as long as the requested model is supported. This provides great flexibility with only minor latency increase. </p> <p>The gRPC interface supports c/c++, python, and go. A python example is located in our retail-use-cases gRPC python. The diagram show how the Docker Compose will deploy the client and OVMS. </p> <p></p>"},{"location":"Architecture/pipelines.html#openvino-model-serverovms-pipeline-with-c-api","title":"OpenVINO Model Server(OVMS) Pipeline with C API","text":"<p>When performance is more important than flexibility a developer can use the C API to bypass the gRPC interface and reduce latency. currently this method is only supported for c/c++ and required client/OVMS to both be deployed in a single Docker container. and example of a C API pipeline can be found in retail-use-cases gst_capi. Similar to DLStreamer the Docker Compose only launches a single container per pipeline now that the client/OVMS directly connect through the C API.</p> <p></p>"},{"location":"Architecture/pipelines.html#performance","title":"Performance","text":"<p>More details about benchmarking pipelines can be found on the Performance Tools Page.</p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html","title":"Distributed Architecture","text":"<ul> <li>Status</li> <li>Decision</li> <li>Context</li> <li>Proposed Design</li> <li>Consequences</li> <li>References</li> </ul>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#decision","title":"Decision","text":""},{"location":"Architecture/v2.0.0/distributed-architecture.html#context","title":"Context","text":"<p>To a wider variety of computer vision use cases in the real world a distributed inference architecture is required for deployment and scale. To achieve this, OpenVINO Model Server (OVMS) will be used for server side inferencing as part of the architecture design. The new architecture will lose some inference throughput but gain flexibility and scale.</p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#proposed-design","title":"Proposed Design","text":"<p>Using OpenVINO Model Server (OVMS) pipeline, workloads can be distributed between different services. For our solution a single system and remote server setup will be supported.</p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#single-system-setup","title":"Single System Setup","text":"<p>The single system solution will launch both the OVMS client and OVMS server on the same system as Docker containers. The local network can be used for communication between the Docker containers. The profile launcher program will load the profile configs and environment variables form a local data volume. The computer vision models will also be located on a local data volume. The models can be downloaded using the provided scripts or manually by the user.</p> <p></p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#remote-server-setup","title":"Remote Server Setup","text":"<p>The remote serve set will launch the same OVMS client and OVMS server containers but on two different systems. These systems can be on the same network on in remote locations as long as the systems can communicate through the network. This will require additional security or a direct connection from client to server. Similar to the single system the profile launcher will load the profile configs and environment variables from a data volume. In this case the data volume can be a local copy or a remote copy of those files. On the server the computer vision models will be in a data volume. Unlike the profile config and environment files these must be located on the server in a data volume. This is to prevent any unwanted changes to the computer vision model when it is located in a remote location. </p> <p></p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#running-the-pipeline","title":"Running the Pipeline","text":"<p>The profile launching program will start a pre-configured OVMS client and OVMS server. Run Pipeline documentation covers the parameter details and how to configure different input sources.</p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#applicable-repos","title":"Applicable Repos","text":"<p>automated-self-checkout</p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#consequences","title":"Consequences","text":"<p>Unlike DLStreamer there will be some latency to call OVMS through gRPC. This will results in a slightly lower stream density for systems. We will however support a wider range and combination of models since the inferencing will be abstracted into the OpenVINO Model Server.</p>"},{"location":"Architecture/v2.0.0/distributed-architecture.html#references","title":"References","text":"<p>https://docs.openvino.ai/2023.0/ovms_what_is_openvino_model_server.html https://github.com/openvinotoolkit/model_server</p>"},{"location":"Architecture/v2.0.0/multiple-ovms-json-config.html","title":"Multiple OpenVINO Model Server Config JSON","text":"<ul> <li>Status</li> <li>Decision</li> <li>Context</li> <li>Proposed Design</li> <li>Consequences</li> <li>References</li> </ul>"},{"location":"Architecture/v2.0.0/multiple-ovms-json-config.html#decision","title":"Decision","text":""},{"location":"Architecture/v2.0.0/multiple-ovms-json-config.html#context","title":"Context","text":"<p>Currently, we use same config.json for all instances of OpenVINO Model Server(OVMS) pipelines, which leads to some issue regarding the device mounting for OVMS server: see issue intel-retail/automated-self-checkout#322. So we need a way to have multiple or unique config json file per OVMS instance.</p>"},{"location":"Architecture/v2.0.0/multiple-ovms-json-config.html#proposed-design","title":"Proposed Design","text":"<p>Move the current device update logic from <code>run.sh</code> with the config.json file into profile-launcher in Golang. When profile-launcher about to launch a new instance of OVMS server, it then produces a unique config json file name for that instance of OVMS server.</p> <p>For example, we can use the Docker container name of that OVMS server like ovms_server0, or ovms_server1,...etc to be appended into the config json as part of the file name (e.g. config_ovms_server0.json).</p> <p>One example golang code for updating json file target_device and producing a new config.json is shown below: <pre><code>// ----------------------------------------------------------------------------------\n// Copyright 2023 Intel Corp.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n//  Unless required by applicable law or agreed to in writing, software\n//  distributed under the License is distributed on an \"AS IS\" BASIS,\n//  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n//  See the License for the specific language governing permissions and\n//  limitations under the License.\n//\n// ----------------------------------------------------------------------------------\n\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"log\"\n    \"os\"\n    \"reflect\"\n)\n\ntype OvmsConfig struct {\n    ModelList []ModelConfig `json:\"model_config_list\"`\n}\n\ntype ModelConfig struct {\n    Config map[string]interface{} `json:\"config\"`\n}\n\nfunc main() {\n    updateConfig(\"CPU\")\n}\n\nfunc updateConfig(device string) {\n    contents, err := os.ReadFile(\"config_template.json\")\n    if err != nil {\n        err = fmt.Errorf(\"Cannot read json config %v\", err)\n    }\n\n    var data OvmsConfig\n    err = json.Unmarshal(contents, &amp;data)\n    if err != nil {\n        log.Fatalf(\"failed to unmarshal configuration file configuration.yaml: %v\", err)\n    }\n\n    fmt.Println(reflect.TypeOf(data.ModelList))\n\n    for _, model := range data.ModelList {\n        // fmt.Println(model)\n        model.Config[\"target_device\"] = device\n        fmt.Println(model.Config[\"target_device\"])\n        fmt.Println(\"!!!!!!!!!!!!\")\n        fmt.Println(model.Config)\n    }\n\n    // convert to struct\n    updateConfig, err := json.Marshal(data)\n    if err != nil {\n        log.Fatalf(\"could not marshal config to JSON: %v\", err)\n    }\n    _ = os.WriteFile(\"config_ovms_server0.json\", updateConfig, 0644)\n}\n</code></pre></p> <p>This step is done before the profile-launcher calling the <code>start_ovms_server.sh</code>script.</p> <p>In the profile-launcher we also set the correct  environment variable values for the <code>start_ovms_server.sh</code>script to use.  For example, <code>OVMS_MODEL_CONFIG_JSON</code> to be the unique config json file name that was produced from the above example.</p> <p>For clean-up, we can do deletion of the config json files when <code>make clean-all</code> is called or <code>make clean-ovms-server</code> is called.</p>"},{"location":"Architecture/v2.0.0/multiple-ovms-json-config.html#references","title":"References","text":"<ul> <li>https://docs.openvino.ai/2023.0/ovms_what_is_openvino_model_server.html</li> <li>https://github.com/openvinotoolkit/model_server</li> <li>see issue Classification profile crashed when run the 2nd instance switch from CPU to GPU.0 automated-self-checkout#322</li> </ul>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html","title":"Performance Benchmarking","text":"<ul> <li>Status</li> <li>Decision</li> <li>Context</li> <li>Proposed Design</li> <li>Consequences</li> <li>References</li> </ul>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#decision","title":"Decision","text":""},{"location":"Architecture/v2.0.0/performance_benchmarking.html#context","title":"Context","text":"<p>To assist customers we will provide a set of performance Docker containers to measure the performance of their pipelines. The performance Docker containers will need to be supported on most modern Intel hardware. The output will also need to be formatted and presented to customers as a hardware recommendation.</p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#proposed-design","title":"Proposed Design","text":""},{"location":"Architecture/v2.0.0/performance_benchmarking.html#benchmark-script","title":"Benchmark Script","text":"<p>The benchmark script is designed to help determine the performance needs for a specific pipeline profile. The script will run a designated pipeline profile and can either replicate that pipeline profile a specific number of times or continue to replicate until a performance target is reached.</p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#performance-tools","title":"Performance tools","text":"<p>sysstat: System CPU utilization free: System memory usage iotop: System Disk read and write data igt-gpu-tools: Integrated GPU utilization Intel XPU Manage: Discrete GPU utilization Intel Performance Counter Monitor: System power usage</p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#input-source-types","title":"Input Source Types","text":"<p>For performance inputs we support RTSP video streams, USB camera, Intel\u00ae RealSense\u2122 Camera, and video files. For longer benchmarking runs its' recommended to use a video loop with an RTSP stream for inference result consistency. As an option an RTSP Camera Simulator is provided with the performance script.</p> <p>Input Source Types</p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#specified-number-of-pipelines","title":"Specified Number of Pipelines","text":"<p>If you are looking to test a specific number of pipelines on different hardware SKUs the <code>--pipelines</code> parameter can be used. This parameter will start the specified number of pipelines </p> <p>Specified Number of Pipelines</p> <p></p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#consolidated-results","title":"Consolidated Results","text":"<p>To make reading results easier, a consolidation script has been provided. This script will work with a single or multiple runs of the specified number of pipelines. Details about this process are found in Benchmark Specified Number of Pipelines</p> <pre><code>make consolidate ROOT_DIRECTORY=&lt;output dir&gt;\n</code></pre>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#stream-density","title":"Stream Density","text":"<p>The stream density parameter can be used to find the maximum number of pipelines at a target frames per second (FPS) on a specific hardware SKU. By setting the <code>--stream_density</code> parameter to the desired FPS the script will continue to create pipelines until the average pipelines FPS falls below the desired FPS. The script will provide a detailed log to show each pipeline FPS during the test run. This option provides a method for testing the top performance when introducing a new pipeline or hardware SKU.</p> <p>Stream Density</p> <p></p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#applicable-repos","title":"Applicable Repos","text":"<p>automated-self-checkout</p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#consequences","title":"Consequences","text":"<p>Having a generic and scalable set of performance Docker containers will allow customers to test a wide range of pipelines and hardware setups without extensive configuration of their systems. The flexibility will bring faster time to market and better hardware decision making by customers.</p>"},{"location":"Architecture/v2.0.0/performance_benchmarking.html#references","title":"References","text":"<p>Pipeline Benchmarking</p>"},{"location":"Architecture/v2.0.0/profile-launcher.html","title":"Distributed Architecture","text":"<ul> <li>Status</li> <li>Decision</li> <li>Context</li> <li>Proposed Design</li> <li>Consequences</li> <li>References</li> </ul>"},{"location":"Architecture/v2.0.0/profile-launcher.html#decision","title":"Decision","text":""},{"location":"Architecture/v2.0.0/profile-launcher.html#context","title":"Context","text":"<p>Depending on the underlying pipeline architecture you may only require one Docker container or you may require many Docker containers. Specifically OVMS has two methods for running: gRPC which uses remote inference calls from a client to server and Capi which does the inferencing locally. Additionally other methods such as GStreamer are run in a single container. The profiles should be able to accommodate both use cases.</p>"},{"location":"Architecture/v2.0.0/profile-launcher.html#proposed-design","title":"Proposed Design","text":"<p>Update the profile to handle an array of configurations. This will allow the user to mix multiple Docker container configurations into a single profile.</p> <p>Each container configuration will contain the following information: - Docker image: The profile launcher will use as the target image - Environment file: Loaded into the container - Entrypoint script: Launch the desired start process - Input arguments: container or entrypoint script - Docker Volumes: Mounted to the container - Docker Networks: Connected to the container</p> <p></p>"},{"location":"Architecture/v2.0.0/profile-launcher.html#single-container-profile","title":"Single Container Profile","text":"<p>A single container profile will run a single Docker image using a single entrypoint script. This use case will be for pipelines that are self contained in a single container. Although the container can interact with other containers on the system the performance tools will only measure the performance of the single running container.</p> <p></p>"},{"location":"Architecture/v2.0.0/profile-launcher.html#multiple-container-profile","title":"Multiple Container Profile","text":"<p>A multiple container profile will run the array of containers defined in the profile config. Each container can have it's own entrypoint script even if they utilize the same base Docker image. The common profile will be the OpenVINO Model Server and client. In this case a OVMS container will contain the inference models defined in the config.json from the profile. Once the OVMS container is started the client will be launched and connect to the OVMS container. This will result in the inference workload being executed in a difference service which can be on the local system or in a remote location. </p> <p></p>"},{"location":"Architecture/v2.0.0/profile-launcher.html#applicable-repos","title":"Applicable Repos","text":"<p>automated-self-checkout</p>"},{"location":"Architecture/v2.0.0/profile-launcher.html#consequences","title":"Consequences","text":"<p>All profiles will need to be updated to use this new array structure. As a benefit common containers such as the OpenVINO Model Server can be shared between profiles.</p>"},{"location":"Architecture/v2.0.0/profile-launcher.html#references","title":"References","text":"<p>https://docs.openvino.ai/2023.0/ovms_what_is_openvino_model_server.html https://github.com/openvinotoolkit/model_server</p>"},{"location":"Architecture/v2.0.0/target-device.html","title":"Distributed Architecture","text":"<ul> <li>Status</li> <li>Decision</li> <li>Context</li> <li>Proposed Design</li> <li>Consequences</li> <li>References</li> </ul>"},{"location":"Architecture/v2.0.0/target-device.html#decision","title":"Decision","text":""},{"location":"Architecture/v2.0.0/target-device.html#context","title":"Context","text":"<p>The platform parameter is inconsistent with the target device being used in the pipeline. To be consistent with OVMS we want to use the target_device parameter in the script to match the target_device setting in the OVMS config file.</p>"},{"location":"Architecture/v2.0.0/target-device.html#proposed-design","title":"Proposed Design","text":"<p>Update the platform parameter to match the target_device standard used by OpenVINO Model Server. This will provide clarity to the device being used for the inferencing portion of the pipeline. The following are the acceptance criteria for the change.</p> <ul> <li>Replace platform parameter with target_device using CPU as the default device.</li> <li>Update the docker_run script to make it run with minimal changes to the profiles.</li> <li>Confirm that the benchmark script works with the target_device parameter update.</li> <li>Update unit tests</li> <li>Update documentation</li> <li>Convert $DEVICE to $TARGET_DEVICE for internal environment variables.</li> <li>Add option to use existing config file and not override all target_devices to support models with different target_device values.</li> </ul>"},{"location":"Architecture/v2.0.0/target-device.html#target-device-list","title":"Target Device list","text":"Device Parameter Description Links CPU CPU Use CPU only OVMS Parameters GPU GPU Use default GPU OVMS Parameters Specified GPU GPU.x Use a specific GPU. ex. GPU.0 = integrated GPU, GPU.1 = discrete Arc GPU OVMS Parameters Mixed Contifuration MULTI:x,y Use a combination of devices for inferencing ex. MULTI:CPU,GPU.1 will use the CPU and discrete Arc GPU for inferencing OVMS Parameters Automatic Device Selection AUTO Allow OpenVINO to automatically select the optimal device for inferencing Possibly depricated? Automatic Device Selection AUTO Allow OpenVINO to automatically select the optimal device for inferencing Possibly depricated? Heterogeneous Execution HETERO Allows OpenVINO to execute inference on multiple devices Heterogeneous Execution Heterogeneous Execution Priority HETERO:x,y Allows OpenVINO to execute inference on multiple devices and set the priority of device. ex. HETERO:CPU,GPU.1 will prioritize CPU and discrete Arc GPU for inferencing Heterogeneous Execution"},{"location":"Architecture/v2.0.0/target-device.html#applicable-repos","title":"Applicable Repos","text":"<p>automated-self-checkout</p>"},{"location":"Architecture/v2.0.0/target-device.html#consequences","title":"Consequences","text":"<p>Removing the platform parameter will break any existing test and benchmark scripts. The change will clarify which device you are targeting for the inference.</p>"},{"location":"Architecture/v2.0.0/target-device.html#references","title":"References","text":"<p>https://docs.openvino.ai/2023.0/ovms_what_is_openvino_model_server.html https://github.com/openvinotoolkit/model_server</p>"},{"location":"Architecture/v3.0.0/aicsd-integration.html","title":"Integration of AI Connect for Scientific Devices (AiCSD)","text":"<ul> <li>Decision</li> <li>Context</li> <li>Proposed Design</li> <li>References</li> </ul>"},{"location":"Architecture/v3.0.0/aicsd-integration.html#decision","title":"Decision","text":""},{"location":"Architecture/v3.0.0/aicsd-integration.html#context","title":"Context","text":"<p>This feature would:</p> <ul> <li>provide the ability to integrate pipelines using Intel Geti, BentoML or OpenVino</li> <li>provide the ability to run the entire AiCSD framework including sending images from another machine</li> <li>extend the capability of AiCSD to process still images to include video streams</li> <li>integrate EdgeX</li> </ul>"},{"location":"Architecture/v3.0.0/aicsd-integration.html#proposed-design","title":"Proposed Design","text":""},{"location":"Architecture/v3.0.0/aicsd-integration.html#crawl-integration-of-the-pipeline","title":"Crawl: Integration of the Pipeline","text":"<p>In the initial phases, the solution can be pulled in as just the pieces surrounding the pipeline validation. In order to do this, the Pipeline Validator service can be pulled in. This service  provides the appropriate endpoints and components necessary to call any pipeline built for this system. In the simplest case, the Pipeline Validator service could be used with the  Pipeline Simulator service without the need to add additional models. The Pipeline Simulator will be modified to use go gRPC to call an OVMS inferencing pipeline. In order to send information to the services, it would be necessary to add a script that calls the endpoint to launch the pipeline. When integrating with the benchmarking  script, the AiCSD services will be started using the necessary docker compose files.  This will ensure that all the services are started under the same Docker network. </p> <p></p> <p>Necessary Components:</p> <ul> <li>Pipeline Validator Service</li> <li>EdgeX Services</li> <li>Pipeline from the options below:<ul> <li>Pipeline Simulator (standalone)</li> <li>Intel Geti</li> <li>OVMS</li> <li>BentoML</li> </ul> </li> <li>Script to call launch pipeline for each image in a directory  (or a script that sends the same image for a fixed period of time)</li> <li>Integrate the ability to launch the appropriate target from the profile launcher</li> </ul>"},{"location":"Architecture/v3.0.0/aicsd-integration.html#walk-integrate-the-file-dropping-capability","title":"Walk: Integrate the file dropping capability","text":"<p>Integrate the entire AiCSD solution to add the ability to use two machines - one for generating image and the other for performing the processing. In integration of this feature, it would also be possible to run AiCSD all on one system. The integration will allow  for benchmarking to run alongside this solution.</p> <p>Necessary components:</p> <ul> <li>Desired pipeline</li> <li>AiCSD Gateway Services</li> <li>Integration of launching services with the profile launcher</li> </ul>"},{"location":"Architecture/v3.0.0/aicsd-integration.html#run-add-video-streaming-capability","title":"Run: Add Video Streaming Capability","text":"<p>This feature would allow for the use of video streaming with models supported by  Intel Geti or BentoML. In this solution, it could be necessary to update the AiCSD  solution in order to support the use of video streams. </p>"},{"location":"Architecture/v3.0.0/aicsd-integration.html#references","title":"References","text":"<ul> <li>AiCSD Architecture Overview</li> <li>AiCSD Pipeline Creation</li> </ul>"},{"location":"performance-tools/benchmark.html","title":"Computer Vision Pipeline Benchmarking","text":"<p>The provided Python-based script works with Docker Compose to get pipeline performance metrics like video processing in frames-per-second (FPS), memory usage, power consumption, and so on.</p>"},{"location":"performance-tools/benchmark.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>Docker Compose</li> <li>Make</li> <li>Git</li> <li> <p>Code from Retail Use Cases Repo and its submodule Performance Tools Repo</p> <p>Note</p> <p>To install the submodule, run <code>make update-submodules</code> from the root of the retail-use-cases repo.</p> </li> <li> <p>Python environment v3.12.2</p> <p>Note</p> <p>This could be accomplished using Miniconda and creating a Python 3.12.2 env</p> <pre><code>    sudo apt install git gcc python3-venv python3-dev\n</code></pre> </li> </ul>"},{"location":"performance-tools/benchmark.html#benchmark-a-cv-pipeline","title":"Benchmark a CV Pipeline","text":"<ol> <li> <p>Build the benchmark container and change into the benchmark-scripts directory.</p> <pre><code>    cd performance-tools/\n    make build-benchmark-docker\n</code></pre> </li> <li> <p>Python packages listed in performance-tools/benchmark-scripts/requirements.txt</p> <pre><code>    cd performance-tools/benchmark-scripts/\n    python3 -m venv venv\n    source venv/bin/activate\n    pip install -r requirements.txt\n</code></pre> </li> <li> <p>[Optional] If NPU data collection is desired, ensure that the following is correct.</p> <p>a. Run the following command to get the correct path to the NPU under <code>/sys/devices</code> <pre><code>lspci | grep -i npu\n</code></pre> b. Ensure the environment variable NPU_PATH in performance-tools/docker/docker-compose.yaml for the npu-util service or the global variable in performance-tools/docker/npu-util/npu_logger.py is set to the correct location.    <pre><code>NPU_PATH=\"/sys/devices/pci0000:00/0000:&lt;insert_results&gt;/npu_busy_time_us\"\n</code></pre></p> <p>Example</p> <p>If the lspci command is:</p> <pre><code>$ lspci | grep -i npu\n00:0b.0 Processing accelerators: Intel Corporation Lunar Lake NPU (rev 04)\n</code></pre> <p>then the NPU_PATH is:</p> <pre><code>NPU_PATH=\"/sys/devices/pci0000:00/0000:00:0b.0/npu_busy_time_us\"\n</code></pre> </li> <li> <p>Choose a CV pipeline from the Retail Use Cases Repo, Automated Self-Checkout or Loss Prevention and note the file paths to the docker compose files.</p> </li> <li> <p>Run the benchmarking script using the docker compose file(s) as inputs to the script (sample command shown below).</p> <p>Automated Self-Checkout: <pre><code>    python benchmark.py --compose_file ../../src/docker-compose.yml --pipeline 1\n</code></pre></p> <p>Retail Use Cases: <pre><code>    python benchmark.py --compose_file ../../use-cases/gst_capi/add_camera-simulator.yml --compose_file ../../use-cases/gst_capi/add_gst_capi_yolov5_ensemble.yml\n</code></pre></p> </li> </ol> <p>Go to Arguments to understand how to customize the benchmarks</p> <p>Specific number of pipelines with single container</p> <p></p> <p>Specific number of pipelines with OVMS and Client</p> <p></p>"},{"location":"performance-tools/benchmark.html#benchmark-stream-density-for-cv-pipelines","title":"Benchmark Stream Density for CV Pipelines","text":"<p>Benchmarking a pipeline can also discover the maximum number of workloads or streams that can be run in parallel for a given target FPS. This information is useful to determine the hardware required to achieve the desired performance for CV pipelines.</p> <p>To run the stream density functionality use <code>--target_fps</code> and/or <code>--density_increment</code> as inputs to the <code>benchmark.py</code> script:</p> <pre><code> python benchmark.py  --retail_use_case_root ../../retail-use-cases --target_fps 14.95 --density_increment 1 --init_duration 40   --compose_file ../../retail-use-cases/use-cases/grpc_python/docker-compose_grpc_python.yml\n</code></pre> <p>where the parameters:</p> <ul> <li><code>target_fps</code> is the given target frames per second (fps) to achieve for maximum number of pipelines</li> <li><code>density_increment</code> is to configure the benchmark logic to increase the number of pipelines each time while trying to find out the maximum number of pipelines before reaching the given target fps.</li> <li> <p><code>init_duration</code> is the initial duration period in second before pipeline performance metrics are taken</p> <p>Note</p> <p>It is recommended to set --target_fps to a value lesser than your target FPS to account for real world variances in hardware readings.</p> </li> </ul> <p>Stream density with single container </p> <p>Stream density with OVMS and Client </p>"},{"location":"performance-tools/benchmark.html#consolidate-results","title":"Consolidate results","text":"<p>The consolidate_multiple_run_of_metrics.py script processes and consolidates performance metrics from various log files (JSON, CSV, and text-based logs) into a structured report. It extracts key performance indicators (KPIs) such as CPU &amp; GPU utilization, memory bandwidth, disk I/O, power consumption, and FPS from multiple sources, aggregates the data, and outputs a summary file.</p> <p>on peformance-tools/benchmark-scripts:</p> <pre><code>    make consolidate\n</code></pre> <p>The summary.csv content should look like this:</p> <pre><code>    Camera_20250303214521714278352 FPS,14.86265306122449\n    Camera_20250303214521714278352 Last log update,03/03/2025 14:46:263943\n    CPU Utilization %,10.069166666666668\n    Memory Utilization %,19.70717535119376\n    Disk Read MB/s,0.0\n    Disk Write MB/s,0.002814426229508197\n    S0 Memory Bandwidth Usage MB/s,8012.58064516129\n    S0 Power Draw W,19.159666666666666\n</code></pre>"},{"location":"performance-tools/benchmark.html#plot-utilization-graphs","title":"Plot Utilization Graphs","text":"<p>After running a benchmark, you can generate a consolidated CPU, NPU, and GPU usage graph based on the collected logs using:  </p> <p>on peformance-tools/benchmark-scripts: <pre><code>make plot\n</code></pre> This command generates a single PNG image (<code>plot_metrics.png</code>) under the <code>results</code> directory, showing:  </p> <p>\ud83e\udde0 CPU Usage Over Time \u2699\ufe0f NPU Utilization Over Time \ud83c\udfae GPU Usage Over Time for each device found </p>"},{"location":"performance-tools/benchmark.html#modifying-additional-benchmarking-variables","title":"Modifying Additional Benchmarking Variables","text":""},{"location":"performance-tools/benchmark.html#arguments","title":"Arguments","text":"Argument Type Default Value Description <code>--pipelines</code> <code>int</code> <code>1</code> Number of pipelines <code>--target_fps</code> <code>float</code> (list) <code>None</code> Stream density target FPS; can take multiple values for multiple pipelines with 1-to-1 mapping via <code>--container_names</code> <code>--container_names</code> <code>str</code> (list) <code>None</code> Container names for stream density target; used together with <code>--target_fps</code> for 1-to-1 mapping <code>--density_increment</code> <code>int</code> <code>None</code> Pipeline increment number for stream density; dynamically adjusted if not specified <code>--results_dir</code> <code>str</code> <code>'./results'</code> Full path to the directory for logs and results <code>--duration</code> <code>int</code> <code>30</code> Time in seconds, not needed when <code>--target_fps</code> is specified <code>--init_duration</code> <code>int</code> <code>20</code> Initial time in seconds before starting metric data collection <code>--target_device</code> <code>str</code> <code>'CPU'</code> Desired running platform [cpu, core, xeon, dgpu.x] <code>--compose_file</code> <code>str</code> (list) <code>None</code> Path(s) to Docker Compose files; can be used multiple times <code>--retail_use_case_root</code> <code>str</code> <code>'../../'</code> Full path to the retail-use-cases repo root <code>--docker_log</code> <code>str</code> <code>None</code> Docker container name to get logs from and save to a file <code>--parser_script</code> <code>str</code> <code>'./parse_csv_to_json.py'</code> Full path to the parsing script to obtain FPS <code>--parser_args</code> <code>str</code> <code>\"-k device -k igt\"</code> Arguments to pass to the parser script; pass args with spaces in quotes: <code>\"args with spaces\"</code>"},{"location":"performance-tools/benchmark.html#change-power-profile","title":"Change Power Profile","text":"<ul> <li>For Ubuntu, follow this documentation to change the power profile.</li> <li>For Windows, follow this documentation to change the power mode.</li> </ul>"},{"location":"performance-tools/benchmark.html#change-or-customize-metric-parsing","title":"Change or Customize Metric Parsing","text":"<p>Two arguments <code>--parser_script</code> and <code>--parser_args</code> control the script and arguments passed to it respectively from the benchmark script.</p> <ul> <li>The <code>--parser_script</code> can be a python script that takes at least an input argument of <code>-d &lt;results_dir&gt;</code>. This will automatically get passed to the parsing script from the benchmarking script. </li> <li>Any other arguments may be passed using the <code>--parser_args</code>, where arguments with spaces are specified in double quotes.</li> </ul>"},{"location":"performance-tools/benchmark.html#developer-resources","title":"Developer Resources","text":""},{"location":"performance-tools/benchmark.html#python-testing","title":"Python Testing","text":"<p>To run the unit tests for the performance tools:</p> <pre><code>cd benchmark-scripts\nmake python-test\n</code></pre> <p>To run the unit tests and determine the coverage:</p> <pre><code>cd benchmark-scripts\nmake python-coverage\n</code></pre>"},{"location":"release-notes/v1-0-1.html","title":"1.0.1","text":"<p>Intel\u00ae Automated Self-Checkout Reference Package 1.0.0 is the first major release. This release includes all items required to run the vision checkout pipeline and benchmarking. For details on running the solution, refer to the Overview. </p>"},{"location":"release-notes/v1-0-1.html#new-features","title":"New Features","text":"Title Description Pipeline scripts Scripts that run the GStreamer-based vision checkout pipeline Benchmark scripts Scripts that start pipelines and system metrics based on parameters Docker* images Dockerized images for the pipeline and benchmark tools for code portability Set up Documentation Markdown files that include setup steps for initial use Unit tests Basic unit tests scripts for smoke testing Camera simulator Camera simulator script to simulate an RTSP stream using a media file Media downloader script Script to assist with downloading sample media for the camera simulator Model downloader script Script to assist with downloading the model files used for the pipelines"},{"location":"release-notes/v1-0-1.html#issues-fixed","title":"Issues Fixed","text":"Issue Number Description Link None Initial Release"},{"location":"release-notes/v1-0-1.html#known-issues","title":"Known Issues","text":"Issue Number Description Link 15 Pipeline core run on some HW is not producing inference results https://github.com/intel-retail/vision-self-checkout/issues/15 29 Unable to modify batch size from run script https://github.com/intel-retail/vision-self-checkout/issues/29"},{"location":"release-notes/v1-5-0.html","title":"1.5.0","text":"<p>Intel\u00ae Automated Self-Checkout Reference Package 1.5.0 is the second main release. This release includes bug fixes, feature enhancements, dockerization of the benchmarking tools, and OpenVINO Model Server support. For details on running the solution, refer to the Overview. </p>"},{"location":"release-notes/v1-5-0.html#new-features","title":"New Features","text":"Title Description OpenVINO Model Server OpenVINO Model Server support OpenVINO Model Server Pipelines Object detection pipelines using OpenVINO Model Server Benchmark scripts Dockerization Benchmark tools have been moved to Docker containers for more flexible deployment Github Build actions Code linting and security scans for pull requests"},{"location":"release-notes/v1-5-0.html#issues-fixed","title":"Issues Fixed","text":"Issue Number Description 41 Pipeline failure log 42 Create makefile docker commands 51 Optimized density script to reduce run time on high powered systems 55 Make performance / powersave mode configurable 57 Add debug option for docker-run.sh 58 Doc update with makefile 61 rename vision self checkout to automated self checkout 65 Update documentation to include OVMS pipelines 66 Add model download top level script 67 [Tech Debt] Make --workload work in any option/argument position when run benchmark.sh 75 docker-run.sh with wrong message when no --workload option is provided 77 XPU Manager not running on multiple GPUs 85 Fix ShellCheck issues in scripts 88 Incorrect instructions for building IGT in pipelinebenchmarking.md 91 format avc mp4 tag logic is inverted 96 For ovms workload getModels.sh not working when it is called by docker-run.sh from project base directory 99 Clean up some checked in dlstreamer models 100 Add cleaning ovms containers to makefile 105 benchmark pcm directory incorrect 109 igt path pointing to the incorrect directory causing the igt log to not be written 112 make CPU as default device for ovms pipeline 115 add dockerfile.bitModel to download bit models 119 pipelinesetup doc has incorrect link to models.list.yml 124 add ovms sample image download into run script 129 Update License to Apache 2.0 131 update mkdoc to navigate to OVMS doc 142 make build-ovms-server failed for 2nd time or later after removed the Docker image for rebuild"},{"location":"release-notes/v1-5-0.html#known-issues","title":"Known Issues","text":"Issue Number Description Link None"},{"location":"release-notes/v2-0-0.html","title":"2.0.0","text":"<p>Intel\u00ae Automated Self-Checkout Reference Package 2.0.0 is the next major release. This release includes bug fixes, feature enhancements, expansion of the OpenVINO Model Server use cases, implementation of gRPC and C API OVMS pipelines. For details on running the solution, refer to the Overview. </p>"},{"location":"release-notes/v2-0-0.html#new-features","title":"New Features","text":"Title Description OpenVINO Model Server gRPC OpenVINO Model Server support OpenVINO Model Server C API OpenVINO Model Server C API example Github Integration Test Action Nightly integration test action Docker Compose Pipeline Docker Compose version of the pipeline"},{"location":"release-notes/v2-0-0.html#issues-fixed","title":"Issues Fixed","text":"Issue Number Description 2.0 Issues Closed Github issues closed in the 2.0 release"},{"location":"release-notes/v2-0-0.html#known-issues","title":"Known Issues","text":"Issue Number Description Link None"},{"location":"release-notes/v2-1-0.html","title":"2.1.0","text":"<p>Intel\u00ae Automated Self-Checkout Reference Package 2.1.0 is minor maintenance release. This release includes bug fixes, feature enhancements, and a new yolov8 with efficientnet profile using OVMS C API. For details on running the solution, refer to the Overview. </p>"},{"location":"release-notes/v2-1-0.html#new-features","title":"New Features","text":"Title Description Yolov8 + Efficientnet C API Profile OVMS based yolov8 + efficientnet profile Sample video indexing Sample video indexing added for camera simulator container name consistency Batch size param for DLStreamer Add bach size parameter for DLStreamer profiles"},{"location":"release-notes/v2-1-0.html#issues-fixed","title":"Issues Fixed","text":"Issue Number Description 2.1 Issues Closed Github issues closed in the 2.1 release"},{"location":"release-notes/v2-1-0.html#known-issues","title":"Known Issues","text":"Issue Number Description Link None"},{"location":"release-notes/v3-0-0.html","title":"3.0.0","text":"<p>Intel\u00ae Retail organization 3.0.0 is major release. The release splits the Automated Self Checkout reference solution into multiple repositories for improved maintainability and scalability.</p>"},{"location":"release-notes/v3-0-0.html#new-features","title":"New Features","text":"Title Description Documentation (New Repository) Documentation, architecture, and requirements for Intel\u00ae retail repositories Automated Self Checkout Automated self checkout use case Retail Use Cases (New Repository) Retail use cases using DLStreamer and OpenVINO Model Server Performance Tools (New Submodule Repository) Performance tools for pipeline benchmarking"},{"location":"release-notes/v3-0-0.html#issues-fixed","title":"Issues Fixed","text":"<p>Release 3.0.0 Issues Closed</p>"},{"location":"release-notes/v3-0-0.html#known-issues","title":"Known Issues","text":"Issue Number Description Link None"},{"location":"release-notes/v3-1-0.html","title":"3.1.0","text":"<p>Intel\u00ae Retail organization 3.1.0 is major release that adds support for the Edge Video Analytics Microservice (EVAM) pipelines. There is also added support for EVAM visualization tools as well as various bug fixes.</p>"},{"location":"release-notes/v3-1-0.html#new-features","title":"New Features","text":"Title Description Edge Video Analytics Microservice Edge Video Analytics Microservice (EAVM) inference pipelines"},{"location":"release-notes/v3-1-0.html#issues-fixed","title":"Issues Fixed","text":""},{"location":"release-notes/v3-1-0.html#release-310-issues-closed","title":"Release 3.1.0 Issues Closed","text":"Repository Automated Self Checkout Core Services Documentation Performance Tools Retail Use Cases"},{"location":"release-notes/v3-1-0.html#known-issues","title":"Known Issues","text":"Repository Automated Self Checkout Core Services Documentation Performance Tools Retail Use Cases"},{"location":"release-notes/v3-2-0.html","title":"3.2.0","text":"<p>Intel\u00ae Retail organization 3.2.0 is a minor release that add loss prevention use case as well as various bug fixes.</p>"},{"location":"release-notes/v3-2-0.html#new-features","title":"New Features","text":"Title Description Loss Prevention Loss Prevention Use Case"},{"location":"release-notes/v3-2-0.html#issues-fixed","title":"Issues Fixed","text":""},{"location":"release-notes/v3-2-0.html#release-320-issues-closed","title":"Release 3.2.0 Issues Closed","text":"Repository Automated Self Checkout Core Services Documentation Performance Tools Retail Use Cases Loss Prevention"},{"location":"release-notes/v3-2-0.html#known-issues","title":"Known Issues","text":"Repository Automated Self Checkout Core Services Documentation Performance Tools Retail Use Cases Loss Prevention"},{"location":"use-cases/use-cases.html","title":"Intel Retail Use Cases","text":"<ul> <li>Automated Self Checkout</li> <li>AI Connect for Scientific Data (AiCSD)</li> <li>Retail Use Cases: C-API for YOLOV8 ensemble</li> <li>Loss Prevention</li> </ul>"},{"location":"use-cases/AiCSD/aicsd.html","title":"AI Connect for Scientific Data (AiCSD)","text":"<p>This reference implementation is pulled in as a solution for pipeline management and distributed image processing. This solution offers an EdgeX based secure, two-system setup with microservices for managing data transfer and pipeline processing. Visit the solution's GitHub Pages for more information.</p> <p>As an extension to this project, a new service, as-pipeline-grpc-go, allows for  the processing of simulated camera data using a gRPC call to an OVMS server running  yolo11n. For information on how the service works and how to get started, visit the GRPC Yolov5s Pipeline page.</p>"},{"location":"use-cases/AiCSD/pipeline-grpc-go.html","title":"GRPC Yolov5s Pipeline Service","text":""},{"location":"use-cases/AiCSD/pipeline-grpc-go.html#overview","title":"Overview","text":"<p>This EdgeX-based application service provides an interface to use gRPC to call  OVMS to run the input data through a specified model. The service may be set up  to use different topics to trigger different pipelines and call different models. The current implementation makes a call to OVMS running the Yolov5s model.</p>"},{"location":"use-cases/AiCSD/pipeline-grpc-go.html#getting-started","title":"Getting Started","text":"<ol> <li> <p>Clone the Aicsd code as a submodule in the directory retail-use-cases/use-cases     <pre><code>git submodule add https://github.com/intel/AiCSD\n</code></pre></p> </li> <li> <p>Change directories to retail-use-cases     <pre><code>cd ..\n</code></pre></p> </li> <li>Download models and sample media      <pre><code>make download-models download-sample-media\n</code></pre></li> <li> <p>Build the OVMS Docker container     <pre><code>make build-ovms-server\n</code></pre></p> </li> <li> <p>change directories to use-cases/aicsd     <pre><code>cd use-cases/aicsd\n</code></pre></p> </li> <li>Build the Pipeline Validator and Pipeline grpc-go services      <pre><code>make docker-pipeline-val docker-pipeline-grpc-go\n</code></pre></li> <li>Modify docker-compose-pipeline-val.yml evnironment variables to have the following settings     <pre><code>APPLICATIONSETTINGS_PIPELINEHOST: pipeline-grpc-go\nAPPLICATIONSETTINGS_PIPELINEPORT: 59790\n</code></pre></li> <li>Run the pipeline validator and pipeline grpc-go services along with their dependencies     <pre><code>make run-pipeline-grpc-go\n</code></pre></li> <li>Verify that the pipeline-grpc-go container has started by checking the log for the message     <pre><code>level=INFO ts=2024-05-30T20:06:58.037173489Z app=app-pipeline-grpc-go source=messaging.go:125 msg=\"Waiting for messages from the MessageBus on the 'ovms-grpc/yolov5' topic\"\n</code></pre></li> <li>Send a POST request to http://localhost:59788/api/v1/launchPipeline with the following body     <pre><code>{\n    \"InputFileLocation\":\"rtsp://camera-simulator:8554/camera_0\",\n    \"PipelineTopic\":\"ovms-grpc/yolov5\",\n    \"OutputFileFolder\":\"0.0.0.0:8555\"\n}\n</code></pre></li> <li>Verify that the pipeline-grpc-go container has output in its log. This shows that the pipeline is running.     <pre><code>level=INFO ts=2024-05-30T20:06:58.037173489Z app=app-pipeline-grpc-go source=messaging.go:125 msg=\"Waiting for messages from the MessageBus on the 'ovms-grpc/yolov5' topic\"\nlevel=INFO ts=2024-05-30T20:08:37.753137304Z app=app-pipeline-grpc-go source=functions.go:265 msg=\"RunOvmsModel: Processing time: 45 ms; fps: 16.70864819479429\\n\"\nlevel=INFO ts=2024-05-30T20:08:37.685039109Z app=app-pipeline-grpc-go source=functions.go:265 msg=\"RunOvmsModel: Processing time: 37 ms; fps: 16.72014862354332\\n\"\nlevel=INFO ts=2024-05-30T20:08:40.689475521Z app=app-pipeline-grpc-go source=functions.go:265 msg=\"RunOvmsModel: Processing time: 49 ms; fps: 16.368045264717768\\n\"\nlevel=INFO ts=2024-05-30T20:08:40.801842876Z app=app-pipeline-grpc-go source=functions.go:265 msg=\"RunOvmsModel: Processing time: 29 ms; fps: 16.37919507955609\\n\"\n</code></pre></li> <li>To stop the data stream from coming in, stop the camera-simulator and camera-simulator0 containers.</li> <li>Verify that the pipeline-grpc-go container has successfully finished processing by verifying the log says     <pre><code>level=DEBUG ts=2024-05-30T20:10:56.351778319Z app=app-pipeline-grpc-go source=triggermessageprocessor.go:196 msg=\"trigger successfully processed message 'OVMS Pipeline' in pipeline 64a47e72-3e89-452e-9010-2d64c059c108\"\n</code></pre></li> <li>Verify that the job status from the pipeline validator service making a GET request to http://localhost:59788/api/v1/job which should return an entry as follows     <pre><code>[\n    {\n        \"Id\": \"0\",\n        \"Owner\": \"none\",\n        \"InputFile\": {\n            \"Hostname\": \"gateway\",\n            \"DirName\": \"rtsp://camera-simulator:8554/\",\n            \"Name\": \"camera_0\",\n            \"Extension\": \"\",\n            \"ArchiveName\": \"\",\n            \"Viewable\": \"\",\n            \"Attributes\": {}\n        },\n        \"PipelineDetails\": {\n            \"TaskId\": \"de9ee7bd-aacb-4e6a-ac91-a3c3be3b09c3\",\n            \"Status\": \"PipelineComplete\",\n            \"QCFlags\": \"passed\",\n            \"OutputFileHost\": \"\",\n            \"OutputFiles\": null,\n            \"Results\": \"ovms-server0:9001\"\n        },\n        \"LastUpdated\": 1717025357138890553,\n        \"Status\": \"Complete\",\n        \"ErrorDetails\": null,\n        \"Verification\": 0\n    }\n]\n</code></pre></li> <li>Repeat by restarting the camera-simulator services and sending the POST request to <code>launchPipeline</code>.</li> </ol>"},{"location":"use-cases/AiCSD/pipeline-grpc-go.html#tearing-down","title":"Tearing Down","text":"<p>To tear down the services and clean up any data created, run  <pre><code>make down clean-files clean-volumes\n</code></pre></p>"},{"location":"use-cases/automated-self-checkout/advanced.html","title":"Advanced Settings","text":""},{"location":"use-cases/automated-self-checkout/advanced.html#applying-environment-variablesev-to-run-pipeline","title":"Applying Environment Variables(EV) to Run Pipeline","text":"<p>EV can be applied in two ways:</p> <pre><code>1. As a Docker Compose environment parameter input \n2. In the env files\n</code></pre> <p>The input parameter will override the one in the env files if both are used.</p>"},{"location":"use-cases/automated-self-checkout/advanced.html#run-with-custom-environment-variables","title":"Run with Custom Environment Variables","text":"<p>Environment variables with make commands</p> <p>Example</p> <pre><code>make PIPELINE_SCRIPT=yolo11n_effnetb0.sh RESULTS_DIR=\"../render_results\"  run-render-mode\n</code></pre> <p>Environment variable with docker compose up</p> <p>Example</p> <pre><code>PIPELINE_SCRIPT=yolo11n_effnetb0.sh RESULTS_DIR=\"../render_results\" docker compose -f src/docker-compose.yml --env-file src/res/yolov5-cpu.env up -d\n</code></pre> <p>Note</p> <pre><code>The environment variables set like this are known as command line environment overrides and are applied to this run only.\nThey will override the default values in env files and docker-compose.yml.\n</code></pre>"},{"location":"use-cases/automated-self-checkout/advanced.html#editing-the-environment-files","title":"Editing the Environment Files","text":"<p>Environment variable files can be used to persist environment variables between deployments. You can find these files in <code>src/res/</code> folder with our default environment variables for Automated Self Checkout.</p> Environment File Description <code>src/res/all-cpu.env</code> Runs pipeline on CPU for decoding, pre-processing, and inferencing <code>src/res/all-gpu.env</code> Runs pipeline on GPU for decoding, pre-processing, and inferencing <code>src/res/all-dgpu.env</code> Runs pipeline on discrete GPU for decoding, pre-processing, and inferencing <code>src/res/all-npu.env</code> Runs pipeline on NPU for inferencing only <code>src/res/yolov5-cpu-class-gpu.env</code> Uses CPU for detection and GPU for classification <code>src/res/yolov5-gpu-class-cpu.env</code> Uses GPU for detection and CPU for classification <p>After modifying or creating a new .env file you can load the .env file through the make command or docker compose up</p> <p>Make</p> <pre><code>make PIPELINE_SCRIPT=yolo11n_effnetb0.sh DEVICE_ENV=res/all-gpu.env run-render-mode    \n</code></pre> <p>Docker compose</p> <pre><code>docker compose -f src/docker-compose.yml --env-file src/res/yolov5-cpu-class-gpu.env up -d\n</code></pre>"},{"location":"use-cases/automated-self-checkout/advanced.html#environment-variables-evs","title":"Environment Variables (EVs)","text":"<p>The table below lists the environment variables (EVs) that can be used as inputs for the container running the inferencing pipeline.</p> Docker Compose EVsDocker Compose ParametersCommon EVs <p>This list of EVs is for running through the make file or docker compose up</p> Variable Description Values <code>DEVICE_ENV</code> Path to device specific environment file that will be loaded into the pipeline container res/all-gpu.env <code>DOCKER_COMPOSE</code> The docker-compose.yml file to run src/docker-compose.yml <code>RETAIL_USE_CASE_ROOT</code> The root directory for Automated Self Checkout in relation to the docker-compose.yml .. <code>RESULTS_DIR</code> Directory to output results ../results <p>This list of parameters that can be set when running docker compose up</p> Variable Description Values <code>-v</code> Volume binding for containers in the Docker Compose -v results/:/tmp/results <code>-e</code> Override environment variables inside of the Docker Container -e LOG_LEVEL debug <p>This list of EVs is common for all profiles.</p> Variable Description Values <code>BARCODE_RECLASSIFY_INTERVAL</code> time interval in seconds for barcode classification Ex: 5 <code>BATCH_SIZE</code> number of frames batched together for a single inference to be used in gvadetect batch-size element 0-N <code>CLASSIFICATION_OPTIONS</code> extra classification pipeline instruction parameters \"\", \"reclassify-interval=1 batch-size=1 nireq=4 gpu-throughput-streams=4\" <code>DETECTION_OPTIONS</code> extra object detection pipeline instruction parameters \"\", \"ie-config=NUM_STREAMS=2 nireq=2\" <code>GST_DEBUG</code> for running pipeline in gst debugging mode 0, 1 <code>LOG_LEVEL</code> log level to be set when running gst pipeline ERROR, INFO, WARNING, and more <code>OCR_RECLASSIFY_INTERVAL</code> time interval in seconds for OCR classification Ex: 5 <code>RENDER_MODE</code> for displaying pipeline and overlay CV metadata 1, 0 <code>PIPELINE_COUNT</code> Number of Automated Self Checkout Docker container instances to launch Ex: 1 <code>PIPELINE_SCRIPT</code> Pipeline script to run. yolo11n.sh, yolo11n_effnetb0.sh, yolo11n_full.sh"},{"location":"use-cases/automated-self-checkout/advanced.html#available-pipelines","title":"Available Pipelines","text":"<ul> <li><code>yolo11n.sh</code> - Runs object detection only.</li> <li><code>yolo11n_full.sh</code> - Runs object detection, object classification, text detection, text recognition, and barcode detection.</li> <li><code>yolo11n_effnetb0.sh</code> - Runs object detection, and object classification.</li> <li><code>obj_detection_age_prediction.sh</code> - Runs two parallel streams:         \u2003Stream 1: Object detection and classification on retail video.          \u2003Stream 2: Face detection and age/gender recognition on age prediction video.</li> </ul>"},{"location":"use-cases/automated-self-checkout/advanced.html#models-used","title":"Models used","text":"<ul> <li>Age/Gender Recognition - <code>age-gender-recognition-retail-0013</code></li> <li>Face Detection - <code>face-detection-retail-0004</code></li> <li>Object Classification - <code>efficientNet-B0</code></li> <li>Object Detection - <code>YOLOv11n</code></li> <li>Text Detectoin - <code>horizontal-text-detection-0002</code></li> <li>Text Recognition - <code>text-recognition-0012</code></li> </ul>"},{"location":"use-cases/automated-self-checkout/advanced.html#using-a-custom-model","title":"Using a Custom Model","text":"<p>You can replace the default detection model with your own trained model by following these steps:</p> <ol> <li> <p>Clone the <code>automated-self-checkout</code> repository. This will create a folder named <code>automated-self-checkout</code>.</p> </li> <li> <p>Inside the <code>automated-self-checkout</code> folder, ensure there is a <code>models</code> directory. If it doesn\u2019t exist, create one.</p> </li> <li> <p>Copy your custom model files into the <code>models</code> directory. For example, use this structure:</p> <pre><code>./automated-self-checkout/models/object_detection/&lt;my_custom_model&gt;/INT8/&lt;my_custom_model.xml&gt;\n</code></pre> </li> <li> <p>Open the <code>yolo11n.sh</code> script. Locate the <code>gstLaunchCmd</code> line and update the <code>model</code> path to point to your custom model:</p> <p>Example</p> <pre><code>model=/home/pipeline-server/models/object_detection/&lt;my_custom_model&gt;/INT8/&lt;my_custom_model.xml&gt;\n</code></pre> </li> <li> <p>Run the pipeline as usual to start using your custom model.</p> </li> </ol> <p>When you add a custom model, it replaces the default detection model used by the pipeline.</p> <p>Note</p> <p>If your custom model includes a <code>labels</code> (<code>.txt</code>) file or a <code>model-proc</code> (<code>.json</code>) file, place them in the same folder as your <code>.xml</code> file. Then set the variables in <code>gstLaunchCmd</code> as shown below before running the pipeline.</p> <pre><code>model-proc=/home/pipeline-server/models/object_detection/&lt;my_custom_model&gt;/INT8/&lt;my_custom_model_proc.json&gt;\n</code></pre> <pre><code>labels=/home/pipeline-server/models/object_detection/&lt;my_custom_model&gt;/INT8/&lt;my_custom_model_labels.txt&gt;\n</code></pre>"},{"location":"use-cases/automated-self-checkout/automated-self-checkout.html","title":"Intel\u00ae Automated Self-Checkout Reference Package","text":""},{"location":"use-cases/automated-self-checkout/automated-self-checkout.html#overview","title":"Overview","text":"<p>As Computer Vision becomes more and more mainstream, especially for industrial &amp; retail use cases, development and deployment of these solutions becomes more challenging. Vision workloads are large and complex and need to go through many stages. For instance, in the pipeline below, the video data is ingested, pre-processed before each inferencing step, inferenced using two models - YOLOv5 and EfficientNet, and post processed to generate metadata and show the bounding boxes for each frame. This pipeline is just an example of the supported models and pipelines found within this reference.</p> <p></p> <p>Automated self-checkout solutions are complex, and retailers, independent software vendors (ISVs), and system integrators (SIs) require a good understanding of hardware and software, the costs involved in setting up and scaling the system, and the configuration that best suits their needs. Vision workloads are significantly larger and require systems to be architected, built, and deployed with several considerations. Hence, a set of ingredients needed to create an automated self-checkout solution is necessary. More details are available on the Intel Developer Focused Webpage and on this LinkedIn Blog</p> <p>The Intel\u00ae Automated Self-Checkout Reference Package provides critical components required to build and deploy a self-checkout use case using Intel\u00ae hardware, software, and other open-source software. This reference implementation provides a pre-configured automated self-checkout pipeline that is optimized for Intel\u00ae hardware. </p>"},{"location":"use-cases/automated-self-checkout/automated-self-checkout.html#next-steps","title":"Next Steps","text":"<p>Note</p> <p>If coming from the catalog please follow the Catalog Getting Started Guide.</p> <p>To begin using the automated self-checkout solution you can follow the Getting Started Guide. </p>"},{"location":"use-cases/automated-self-checkout/automated-self-checkout.html#releases","title":"Releases","text":"<p>For the project release notes, refer to the GitHub* Repository.</p>"},{"location":"use-cases/automated-self-checkout/getting_started.html","title":"Getting Started","text":""},{"location":"use-cases/automated-self-checkout/getting_started.html#step-by-step-instructions","title":"Step by step instructions:","text":"<ol> <li> <p>Download the models using download_models/downloadModels.sh</p> <pre><code>make download-models\n</code></pre> </li> <li> <p>Update github submodules</p> <pre><code>make update-submodules\n</code></pre> </li> <li> <p>Download sample videos used by the performance tools</p> <pre><code>make download-sample-videos\n</code></pre> </li> <li> <p>Build the demo Docker image</p> <pre><code>make build\n</code></pre> </li> <li> <p>Start Automated Self Checkout using the Docker Compose file. The Docker Compose also includes an RTSP camera simulator that will infinitely loop through the sample videos downloaded in step 3.</p> <pre><code>make run-render-mode\n</code></pre> </li> <li> <p>Verify Docker containers</p> <p>Verify Docker images <pre><code>docker ps --format 'table{{.Names}}\\t{{.Status}}\\t{{.Image}}'\n</code></pre> Result: <pre><code>NAMES                 STATUS          IMAGE\ncamera-simulator0     Up 12 seconds   jrottenberg/ffmpeg:4.1-alpine\nsrc-ClientGst-1       Up 14 seconds   dlstreamer:dev\ncamera-simulator      Up 13 seconds   aler9/rtsp-simple-server\n</code></pre></p> </li> <li> <p>Verify Results</p> <p>After starting Automated Self Checkout you will begin to see result files being written into the results/ directory. Here are example outputs from the 3 log files.</p> <p>gst-launch__gst.log <pre><code>libva info: VA-API version 1.22.0\nlibva info: User environment variable requested driver 'iHD'\nlibva info: Trying to open /usr/lib/x86_64-linux-gnu/dri/iHD_drv_video.so\nlibva info: Found init function __vaDriverInit_1_22\nlibva info: va_openDriver() returns 0\nSetting pipeline to PAUSED ...\nPipeline is live and does not need PREROLL ...\nRedistribute latency...\n/GstPipeline:pipeline0/GstFPSDisplaySink:fpsdisplaysink0/GstAutoVideoSink:autovideosink0/GstXvImageSink:autovideosink0-actual-sink-xvimage: sync = true\nProgress: (open) Opening Stream\nPipeline is PREROLLED ...\nPrerolled, waiting for progress to finish...\nProgress: (connect) Connecting to rtsp://localhost:8554/camera_0\n</code></pre> <p>pipeline_gst.log <pre><code>14.58\n14.58\n15.47\n15.47\n15.10\n15.10\n14.60\n14.60\n14.88\n14.88\n</code></pre> <p>r_gst.jsonl <pre><code>{\"resolution\":{\"height\":1080,\"width\":1920},\"timestamp\":1}\n{\"objects\":[{\"detection\":{\"bounding_box\":{\"x_max\":1.0,\"x_min\":0.7868695002029238,\"y_max\":0.8493015899134377,\"y_min\":0.4422388975124676},\"confidence\":0.7139435410499573,\"label\":\"person\",\"label_id\":0},\"h\":440,\"region_id\":486,\"roi_type\":\"person\",\"w\":409,\"x\":1511,\"y\":478}],\"resolution\":{\"height\":1080,\"width\":1920},\"timestamp\":66661013}\n{\"objects\":[{\"detection\":{\"bounding_box\":{\"x_max\":1.0,\"x_min\":0.6974737628926411,\"y_max\":0.8381138710318847,\"y_min\":0.44749696271196093},\"confidence\":0.7188630104064941,\"label\":\"person\",\"label_id\":0},\"h\":422,\"region_id\":576,\"roi_type\":\"person\",\"w\":581,\"x\":1339,\"y\":483}],\"resolution\":{\"height\":1080,\"width\":1920},\"timestamp\":133305076}\n</code></pre> <li> <p>Stop the demo using docker compose down <pre><code>make down\n</code></pre></p> </li>"},{"location":"use-cases/automated-self-checkout/getting_started.html#proceed-to-advanced-settings","title":"Proceed to Advanced Settings","text":""},{"location":"use-cases/automated-self-checkout/getting_started.html#pipeline-performance-tools","title":"Pipeline Performance Tools","text":""},{"location":"use-cases/automated-self-checkout/performance.html","title":"Performance Testing","text":"<p>The performance tools repository is included as a github submodule in this project. The performance tools enable you to test the pipeline system performance on various hardware. </p>"},{"location":"use-cases/automated-self-checkout/performance.html#benchmark-quick-start-command","title":"Benchmark Quick Start command","text":"<p><pre><code>make update-submodules\n</code></pre> <code>update-submodules</code> ensures all submodules are initialized, updated to their latest remote versions, and ready for use.</p> <p><pre><code>make benchmark-quickstart\n</code></pre> The above command would: - Run headless (no display needed: <code>RENDER_MODE=0</code>) - Use full pipeline (<code>PIPELINE_SCRIPT=obj_detection_age_prediction.sh</code>) - Target GPU by default (<code>DEVICE_ENV=res/all-gpu.env</code>) - Generate benchmark metrics - Run <code>make consolidate-metrics</code> automatically</p>"},{"location":"use-cases/automated-self-checkout/performance.html#understanding-benchmarking-types","title":"Understanding Benchmarking Types","text":"<p>Before running benchmark commands, make sure you already configured python and its dependencies. Visit the Performance tools installation guide HERE</p>"},{"location":"use-cases/automated-self-checkout/performance.html#default-benchmark-command","title":"Default benchmark command","text":"<p><pre><code>make update-submodules\n</code></pre> <code>update-submodules</code> ensures all submodules are initialized, updated to their latest remote versions, and ready for use.</p> <p><pre><code>make benchmark\n</code></pre> Runs with: - <code>RENDER_MODE=0</code> - <code>PIPELINE_SCRIPT=yolo11n.sh</code> - <code>DEVICE_ENV=res/all-cpu.env</code> - <code>PIPELINE_COUNT=1</code></p> <p>You can override these values through Environment Variables.</p> <p>List of EVs:</p> Variable Description Values <code>BATCH_SIZE_DETECT</code> number of frames batched together for a single inference to be used in gvadetect  batch-size element 0-N <code>BATCH_SIZE_CLASSIFY</code> number of frames batched together for a single inference to be used in gvaclassify batch-size element 0-N <code>RENDER_MODE</code> for displaying pipeline and overlay CV metadata 1, 0 <code>PIPELINE_COUNT</code> number of Automated Self Checkout Docker container instances to launch Ex: 1 <code>PIPELINE_SCRIPT</code> pipeline script to run. yolo11n_effnetb0.sh, obj_detection_age_prediction.sh, etc. <code>DEVICE_ENV</code> device to use for classification and detection res/all-cpu.env, res/all-gpu.env, res/det-gpu_class-npu.env, etc. <p>Note: Higher the <code>PIPELINE_COUNT</code>, higher the stress on the system. Increasing this value will run more parallel pipelines, increasing resource usage and testing system</p> <p>Note</p> <p>The first time running this command may take few minutes. It will build all performance tools containers</p> <p>After running the following commands, you will find the results in <code>performance-tools/benchmark-scripts/results/</code> folder.</p>"},{"location":"use-cases/automated-self-checkout/performance.html#benchmark-2-pipelines-in-parallel","title":"Benchmark <code>2</code> pipelines in parallel:","text":"<pre><code>make PIPELINE_COUNT=2 benchmark \n</code></pre>"},{"location":"use-cases/automated-self-checkout/performance.html#benchmark-command-with-environment-variable-overrides","title":"Benchmark command with environment variable overrides","text":"<pre><code>make PIPELINE_SCRIPT=yolo11n_effnetb0.sh DEVICE_ENV=res/all-gpu.env PIPELINE_COUNT=1 benchmark\n</code></pre>"},{"location":"use-cases/automated-self-checkout/performance.html#benchmark-command-for-full-pipeline-age-predictionobject-classification-using-gpu","title":"Benchmark command for full pipeline (age prediction+object classification) using GPU","text":"<p><pre><code>make PIPELINE_SCRIPT=obj_detection_age_prediction.sh DEVICE_ENV=res/all-gpu.env PIPELINE_COUNT=1 benchmark\n</code></pre> <code>obj_detection_age_prediction.sh</code> runs TWO video streams in parallel even with PIPELINE_COUNT=1:</p> <p>Stream 1: Object detection + classification on retail video  Stream 2: Face detection + age/gender prediction on age prediction video</p>"},{"location":"use-cases/automated-self-checkout/performance.html#create-a-consolidated-metrics-file","title":"Create a consolidated metrics file","text":"<p>After running the benchmark command run this command to see the benchmarking results:</p> <pre><code>make consolidate-metrics\n</code></pre> <p><code>metrics.csv</code> provides a summary of system and pipeline performance, including FPS, latency, CPU/GPU utilization, memory usage, and power consumption for each benchmark run. It helps evaluate hardware efficiency and resource usage during automated self-checkout pipeline tests.</p>"},{"location":"use-cases/automated-self-checkout/performance.html#benchmark-stream-density","title":"Benchmark Stream Density","text":"<p>To test the maximum amount of Automated Self Checkout containers/pipelines that can run on a given system you can use the TARGET_FPS environment variable. Default is to find the container threshold over 14.95 FPS with the yolo11n.sh pipeline. You can override these values through Environment Variables.</p> <p>List of EVs:</p> Variable Description Values <code>TARGET_FPS</code> threshold value for FPS to consider a valid stream Ex. 14.95 <code>OOM_PROTECTION</code> flag to enable/disable OOM checks before scaling the pipeline (enabled by default) 1, 0 <p>Note:</p> <p>An OOM crash occurs when a system or application tries to use more memory (RAM) than is available, causing the operating system to forcibly terminate processes to free up memory. If <code>OOM_PROTECTION</code> is set to 0, the system may crash or become unresponsive, requiring a hard reboot. </p> <pre><code>make benchmark-stream-density\n</code></pre> <p>You can check the output results for performance metrics in the <code>results</code> folder at the root level. Also, the stream density script will output the results in the console:</p> <pre><code>Total averaged FPS per stream: 15.210442307692306 for 26 pipeline(s)\n</code></pre>"},{"location":"use-cases/automated-self-checkout/performance.html#change-the-target-fps-value","title":"Change the Target FPS value:","text":"<pre><code>make TARGET_FPS=13.5 benchmark-stream-density\n</code></pre>"},{"location":"use-cases/automated-self-checkout/performance.html#environment-variable-overrides-can-also-be-added-to-the-command","title":"Environment variable overrides can also be added to the command","text":"<pre><code>make PIPELINE_SCRIPT=yolo11n_effnetb0.sh TARGET_FPS=13.5 benchmark-stream-density\n</code></pre> <p>Alternatively you can directly call the benchmark.py. This enables you to take advantage of all performance tools parameters. More details about the performance tools can be found HERE</p> <pre><code>cd performance-tools/benchmark-scripts &amp;&amp; python benchmark.py --compose_file ../../src/docker-compose.yml --target_fps 14\n</code></pre>"},{"location":"use-cases/automated-self-checkout/performance.html#plot-utilization-graphs","title":"Plot Utilization Graphs","text":"<p>After running a benchmark, you can generate a consolidated CPU, NPU, and GPU usage graph based on the collected logs using: <pre><code>make plot-metrics\n</code></pre> This command generates a single PNG image (<code>plot_metrics.png</code>) under the <code>benchmark</code> directory, showing:  </p> <p>\ud83e\udde0 CPU Usage Over Time \u2699\ufe0f NPU Utilization Over Time \ud83c\udfae GPU Usage Over Time for each device found  </p>"},{"location":"use-cases/automated-self-checkout/catalog/Get-Started-Guide.html","title":"Getting Started Guide","text":"<ul> <li>Time to Complete: 30 minutes</li> <li>Programming Language: Python3, Bash</li> </ul>"},{"location":"use-cases/automated-self-checkout/catalog/Get-Started-Guide.html#prerequisites-for-target-system","title":"Prerequisites for Target System","text":"<ul> <li>Intel\u00ae Core\u2122 processor</li> <li>At least 16 GB RAM</li> <li>At least 64 GB hard drive</li> <li>An Internet connection</li> <li>Docker*</li> <li>Docker Compose* v2 (Optional)</li> <li>Git*</li> <li>Ubuntu* LTS Boot Device</li> </ul> <p>If Ubuntu is not installed on the target system, follow the instructions and install Ubuntu.</p>"},{"location":"use-cases/automated-self-checkout/catalog/Get-Started-Guide.html#install-automated-self-checkout-package-software","title":"Install Automated Self-Checkout Package Software","text":"<p>Do the following to install the software package:</p> <ol> <li> <p>Download the reference implementation package:       Automated Self-Checkout Retail Reference Implementation.</p> </li> <li> <p>Open a new terminal and navigate to the download folder to unzip the <code>automated-self-checkout</code> package:</p> <pre><code>   unzip automated-self-checkout.zip\n</code></pre> </li> <li> <p>Navigate to the <code>automated-self-checkout/</code> directory:</p> <pre><code> cd automated-self-checkout\n</code></pre> </li> <li> <p>Change permission of the executable edgesoftware file:</p> <pre><code>   chmod 755 edgesoftware\n</code></pre> </li> <li> <p>Install the package:</p> <pre><code>   ./edgesoftware install\n</code></pre> </li> <li> <p>You will be prompted for the Product Key during the installation. The Product Key is in the email you received from Intel confirming your download.</p> <p>When the installation is complete, you will see the message\u202f\u201cInstallation of package complete\u201d and the installation status for each module.</p> <p></p> </li> </ol> <p>If the installation fails because of proxy-related issues, follow the troubleshooting steps.</p>"},{"location":"use-cases/automated-self-checkout/catalog/Get-Started-Guide.html#run-and-evaluate-pre-configured-pipelines","title":"Run and Evaluate Pre-Configured Pipelines","text":"<p>In a retail environment, self-checkout solutions analyze video streams from multiple cameras to streamline the checkout process. The system detects and classifies products as items are scanned. Barcode and text recognition ensure accuracy. This data is processed to verify purchases and update inventory in real time. Factors such as latency and frames per second (FPS) help assess the automated self-checkout solution's real-time responsiveness and efficiency.</p> <p>This demonstration shows how to run the pre-configured pipeline, view a simulation that detects and tracks objects, and check the pipeline's status.</p>"},{"location":"use-cases/automated-self-checkout/catalog/Get-Started-Guide.html#step-1-run-pipeline","title":"Step 1: Run Pipeline","text":"<p>Do the following to run the pre-configured pipeline:</p> <ol> <li> <p>Navigate to the <code>automated-self-checkout</code> directory:</p> <pre><code>   cd automated-self-checkout\n</code></pre> </li> <li> <p>Modify the following host IP addresses to match the IP address of the system running the reference implementation: </p> <ul> <li><code>HOST_IP</code> and <code>RSTP_CAMERA_IP</code> in the <code>src/pipeline-server/.env</code> file. </li> <li><code>host_ip</code> in the <code>src/pipeline-server/postman/env.json</code> file.</li> </ul> </li> <li> <p>Run the pipeline server:</p> <pre><code>   make run-pipeline-server\n</code></pre> <p>The containers will start to run.</p> <p></p> </li> </ol>"},{"location":"use-cases/automated-self-checkout/catalog/Get-Started-Guide.html#step-2-launch-grafana-dashboard","title":"Step 2: Launch Grafana Dashboard","text":"<p>Do the following to launch the Grafana* dashboard to view the objects being detected and tracked:</p> <ol> <li> <p>Open a web browser and enter the following URL to access the Grafana dashboard:       <code>http://&lt;target_system_IP&gt;:3000</code>.</p> <p>To get <code>&lt;target_system_IP&gt;</code>, run the <code>hostname -I</code> command.</p> </li> <li> <p>When prompted, provide the following credentials:</p> <ul> <li>Username: <code>root</code></li> <li>Password: <code>evam123</code></li> </ul> </li> <li> <p>On the dashboard, go to Menu &gt; Home, and select Video Analytics Dashboard.</p> <p>The dashboard visualizes the object detection and tracking pipelines. The bounding boxes around the products indicate their detection and tracking. The dashboard also shows the active streams and their corresponding average FPS.</p> <p></p> </li> </ol>"},{"location":"use-cases/automated-self-checkout/catalog/Get-Started-Guide.html#step-3-check-pipeline-status","title":"Step 3: Check Pipeline Status","text":"<p>Do the following to check the metrics: </p> <ol> <li> <p>Check whether the docker containers are running:</p> <p><pre><code>   docker ps --format 'table{{.Names}}\\t{{.Image}}\\t{{.Status}}'\n</code></pre> </p> </li> <li> <p>Check the MQTT inference output:</p> <pre><code>   mosquitto_sub -v -h localhost -p 1883 -t 'AnalyticsData0'\n   mosquitto_sub -v -h localhost -p 1883 -t 'AnalyticsData1'\n   mosquitto_sub -v -h localhost -p 1883 -t 'AnalyticsData2'\n</code></pre> <p>Here is the result for <code>AnalyticsData0</code>:</p> <pre><code>   AnalyticsData0 {\"objects\":[{\"detection\":{\"bounding_box\":{\"x_max\":0.3163176067521043,\"x_min\":0.20249048400491532,\"y_max\":0.7995593662281202,\"y_min\":0.12237883070032396},\"confidence\":0.868196964263916,\"label\":\"bottle\",\"label_id\":39},\"h\":731,\"region_id\":6199,\"roi_type\":\"bottle\",\"w\":219,\"x\":389,\"y\":132},{\"detection\":{\"bounding_box\":{\"x_max\":0.7833052431819754,\"x_min\":0.6710088227893136,\"y_max\":0.810283140877349,\"y_min\":0.1329853767638305},\"confidence\":0.8499506711959839,\"label\":\"bottle\",\"label_id\":39},\"h\":731,\"region_id\":6200,\"roi_type\":\"bottle\",\"w\":216,\"x\":1288,\"y\":144}],\"resolution\":{\"height\":1080,\"width\":1920},\"tags\":{},\"timestamp\":67297301635}\n\n   AnalyticsData0 {\"objects\":[{\"detection\":{\"bounding_box\":{\"x_max\":0.3163306922646063,\"x_min\":0.20249845268772138,\"y_max\":0.7984013488063937,\"y_min\":0.12254781445953},\"confidence\":0.8666459321975708,\"label\":\"bottle\",\"label_id\":39},\"h\":730,\"region_id\":6201,\"roi_type\":\"bottle\",\"w\":219,\"x\":389,\"y\":132},{\"detection\":{\"bounding_box\":{\"x_max\":0.7850104587729607,\"x_min\":0.6687324296210857,\"y_max\":0.7971464600783804,\"y_min\":0.13681757042794374},\"confidence\":0.8462932109832764,\"label\":\"bottle\",\"label_id\":39},\"h\":713,\"region_id\":6202,\"roi_type\":\"bottle\",\"w\":223,\"x\":1284,\"y\":148}],\"resolution\":{\"height\":1080,\"width\":1920},\"tags\":{},\"timestamp\":67330637174}\n</code></pre> </li> <li> <p>Check the pipeline status:</p> <p><pre><code>   ./src/pipeline-server/status.sh \n</code></pre>   The pipeline status should be like:</p> <pre><code>   --------------------- Pipeline Status ---------------------\n   ----------------8080----------------\n   [\n   {\n         \"avg_fps\": 11.862402507697258,\n         \"avg_pipeline_latency\": 0.5888091060475129,\n         \"elapsed_time\": 268.07383918762207,\n         \"id\": \"95204aba458211efa9080242ac180006\",\n         \"message\": \"\",\n         \"start_time\": 1721361269.6349292,\n         \"state\": \"RUNNING\"\n   }\n   ]\n</code></pre> <p>The pipeline status displays the average FPS and average pipeline latency, among other metrics. </p> </li> <li> <p>Stop the services:</p> <pre><code>   make down-pipeline-server\n</code></pre> </li> </ol>"},{"location":"use-cases/automated-self-checkout/catalog/Get-Started-Guide.html#summary","title":"Summary","text":"<p>In this get started guide, you learned how to:</p> <ul> <li>Install the automated self-checkout package software.</li> <li>Verify the installation.</li> <li>Run pre-configured pipelines, visualize object detection and tracking, and extract data from them.</li> </ul>"},{"location":"use-cases/automated-self-checkout/catalog/Get-Started-Guide.html#learn-more","title":"Learn More","text":"<ul> <li>To apply custom environment variables, see Advanced Settings.</li> <li>To evaluate the pipeline system performance across different hardware, see Test Performance.</li> </ul>"},{"location":"use-cases/automated-self-checkout/catalog/Get-Started-Guide.html#troubleshooting","title":"Troubleshooting","text":"<p>Issues with Docker Installation</p> <p>If you are behind a proxy and if you experience connectivity issues, the Docker installation might fail. Do the following to install Docker manually:</p> <ol> <li>Install Docker from a package.</li> <li>Complete the post-installation steps to manage Docker as a non-root user.</li> <li>Configure the Docker CLI to use proxies.</li> </ol>"},{"location":"use-cases/automated-self-checkout/catalog/Get-Started-Guide.html#error-logs","title":"Error Logs","text":"<p>To access the Docker Logs for EVAM server 0, run the following command: </p> <p><pre><code>   docker logs evam_0\n</code></pre>    Here is an example of the error log when the RSTP stream is unreachable for a pipeline:</p> <pre><code>   {\"levelname\": \"INFO\", \"asctime\": \"2024-07-31 23:26:47,257\", \"message\": \"===========================\", \"module\": \"pipeline_manager\"}\n   {\"levelname\": \"INFO\", \"asctime\": \"2024-07-31 23:26:47,257\", \"message\": \"Completed Loading Pipelines\", \"module\": \"pipeline_manager\"}\n   {\"levelname\": \"INFO\", \"asctime\": \"2024-07-31 23:26:47,257\", \"message\": \"===========================\", \"module\": \"pipeline_manager\"}\n   {\"levelname\": \"INFO\", \"asctime\": \"2024-07-31 23:26:47,330\", \"message\": \"Starting Tornado Server on port: 8080\", \"module\": \"__main__\"}\n   {\"levelname\": \"INFO\", \"asctime\": \"2024-07-31 23:26:51,177\", \"message\": \"Creating Instance of Pipeline detection/yolov5\", \"module\": \"pipeline_manager\"}\n   {\"levelname\": \"INFO\", \"asctime\": \"2024-07-31 23:26:51,180\", \"message\": \"Gstreamer RTSP Server Started on port: 8555\", \"module\": \"gstreamer_rtsp_server\"}\n   {\"levelname\": \"ERROR\", \"asctime\": \"2024-07-31 23:26:51,200\", \"message\": \"Error on Pipeline 5d5b3b0a4f9411efb60d0242ac120007: gst-resource-error-quark: Could not open resource for reading. (5): ../gst/rtsp/gstrtspsrc.c(6427): gst_rtspsrc_setup_auth (): /GstPipeline:pipeline3/GstURISourceBin:source/GstRTSPSrc:rtspsrc0:\\nNo supported authentication protocol was found\", \"module\": \"gstreamer_pipeline\"}\n</code></pre>"},{"location":"use-cases/automated-self-checkout/catalog/Get-Started-Guide.html#known-issues","title":"Known Issues","text":"<p>For the list of known issues, see known issues.</p>"},{"location":"use-cases/automated-self-checkout/catalog/Overview.html","title":"Automated Self-Checkout Retail Reference Implementation","text":"<p>Use pre-configured optimized computer vision pipelines to build and deploy a self-checkout use case using Intel\u00ae hardware, software, and other open source software.</p>"},{"location":"use-cases/automated-self-checkout/catalog/Overview.html#summary","title":"Summary","text":"<p>The Automated Self-Checkout Reference Implementation provides essential components to build and deploy a self-checkout solution using Intel\u00ae hardware, software, and open source software. It includes the basic services to get you started running optimized Intel\u00ae Deep Learning Streamer (Intel\u00ae DLStreamer)-based computer vision pipelines. These services are modular, allowing for customization or replacement with your solutions to address specific needs.</p>"},{"location":"use-cases/automated-self-checkout/catalog/Overview.html#features-and-benefits","title":"Features and Benefits","text":"<p>With this reference implementation, the self-checkout stations can:</p> <ul> <li>Recognize the non-barcoded items more quickly.</li> <li>Recognize the product SKU and items placed in transparent bags without requiring manual input.</li> <li>Reduce the steps in identifying products when there is no match by suggesting the top five closest choices. </li> </ul> <p>The pre-configured, optimized computer vision pipelines also accelerate the time to market. Inference results are published to Message Queuing Telemetry Transport (MQTT), allowing easy integration with other applications. The implementation includes examples of using different devices such as CPUs, integrated GPUs, and discrete GPUs.</p>"},{"location":"use-cases/automated-self-checkout/catalog/Overview.html#how-it-works","title":"How It Works","text":"<p>In this reference implementation, the video streams from various cameras are cropped and resized to enable the inference engine to run the associated models. The object detection and product classification features identify the SKUs during checkout. The barcode detection, text detection, and recognition features further verify and increase the accuracy of the detected SKUs. The inference details are then aggregated and pushed to  MQTT to process the combined results further.</p> <p>As Figure 1 shows, Docker Compose is used to deploy the reference implementation on different system setups easily. At the same time, MQTT Broker publishes the inference data that external applications or systems can use. Unique MQTT topics are created for each pipeline for a more refined approach to organizing inference outputs.</p> <p></p> <p>Figure 1: Automated Self-Checkout Architectural Diagram</p> <p>Each automated self-checkout pipeline has a pre-configured setup optimized for running on Intel hardware. The following are the available pipelines: </p> <ul> <li><code>yolov5</code>: yolov5 object detection only.</li> <li><code>yolov5_effnet</code>: yolov5 object detection and <code>efficientnet_b0</code> classification.</li> <li><code>yolov5_full</code>: yolov5 object detection, <code>efficientnet_b0</code> classification, text detection, text recognition, and barcode detection.</li> </ul> <p>Figure 2 shows a pipeline in which the video data is ingested and pre-processed before each inferencing step. The data is then analyzed using two models, <code>YOLOv5</code> and <code>EfficientNet</code>, and post-processed to generate metadata and display bounding boxes for each frame. This pipeline is an example of the models and processing workflows supported in this reference implementation.</p> <p></p> <p>Figure 2: Example of a Pipeline Flow</p> <p>The number of streams and pipelines that can be used are system-dependent. For more details, see the latest performance data. </p> <p>The following are the components in the reference implementation.</p> <ul> <li>Edge Video Analytics Microservice (EVAM) is a Python-based, interoperable containerized microservice for the easy development and deployment of video analytics pipelines. It is built on GStreamer and Intel\u00ae DL Streamer, which provide video ingestion and deep learning inferencing functionalities, respectively.</li> <li>Multimodal Data Visualization Microservice enables the visualization of video streams and time-series data.</li> </ul>"},{"location":"use-cases/automated-self-checkout/catalog/Overview.html#learn-more","title":"Learn More","text":"<ul> <li>Get started with the Automated Self-Checkout Retail Reference Implementation using the Get Started Guide.</li> <li>Know more about GStreamer and Intel\u00ae Deep Learning Streamer (DL Streamer).</li> </ul>"},{"location":"use-cases/capi-yolov8-ensemble/capi-yolov8-ensemble.html","title":"Intel\u00ae C-API YOLOV8 Ensemble Object Detection Reference Package","text":""},{"location":"use-cases/capi-yolov8-ensemble/capi-yolov8-ensemble.html#overview","title":"Overview","text":"<p>YOLOv8 is one of the popular YOLO object detection models. To show case the object detection of using YOLOv8, the efficientnet classification model will also be used to help validate the performance and accuracy of the C-API architecture.</p> <p>The YOLOv8 + efficientnet design will follow a similar custom pipeline as the previously implemented YOLOv5 + efficientnet profile. The same gstreamer decoding will be used for the input stream. C-API will send the frames to the OpenVINO Model Server(OVMS) processing through the custom pipeline defined in the config.json. OVMS will output the object bounding boxes along with the object classification. The profile will also output the processing latency in Frames Per Second (FPS).</p> <p></p>"},{"location":"use-cases/capi-yolov8-ensemble/capi-yolov8-ensemble.html#applicable-repos","title":"Applicable Repos","text":"<p>retail-use-cases-gst-capi-yolov8</p>"},{"location":"use-cases/capi-yolov8-ensemble/capi-yolov8-ensemble.html#next-steps","title":"Next Steps","text":"<p>To begin using the C-API YOLOv8 ensemble Helm/Kubernetes solution you can follow the Getting Started Guide. </p>"},{"location":"use-cases/capi-yolov8-ensemble/getting_started.html","title":"Getting Started for K8s C-API YOLOV8","text":""},{"location":"use-cases/capi-yolov8-ensemble/getting_started.html#instructions-for-running-k8s-c-api-yolov8-using-minikube","title":"Instructions for Running K8s C-API YOLOv8 using minikube:","text":""},{"location":"use-cases/capi-yolov8-ensemble/getting_started.html#presiquistes","title":"Presiquistes:","text":"<ul> <li>Ubuntu 22.04 +</li> <li>minikube</li> <li>kubectl</li> <li>kompose</li> </ul>"},{"location":"use-cases/capi-yolov8-ensemble/getting_started.html#build-c-api-yolov8-container-image","title":"Build C-API YOLOv8 Container Image","text":"<p>change directory to <code>use-cases/gst_capi/helm</code> while you are at the retail-use-cases project base directory:</p> <pre><code>```bash\ncd ./use-cases/gst_capi/helm\n```\n</code></pre> <p>and then build the container image for minikube to run:</p> <pre><code>```bash\nmake minikube_build_capi_yolov8_ensemble\n```\n</code></pre> <p>and you should see all of the dependencies built successfully.</p>"},{"location":"use-cases/capi-yolov8-ensemble/getting_started.html#run-c-api-yolov8-in-minikube","title":"Run C-API YOLOv8 in minikube","text":"<p>Run the following command while you are still on the directory <code>use-cases/gst_capi/helm</code>:</p> <pre><code>```bash\nmake run_capi_yolov8_ensemble\n```\n</code></pre> <p>and you should see all K8s pods running up.</p>"},{"location":"use-cases/capi-yolov8-ensemble/getting_started.html#verify-pods-running","title":"Verify Pods Running","text":"<pre><code>```bash\nkubectl get pod\n```\nResult:\n```console\nNAME                                  READY   STATUS    RESTARTS       AGE\ncamera-simulator-8cffdc4ff-nbcd2      1/1     Running   0              84s\ncamera-simulator0-65779f499-fkk7h     1/1     Running   2 (72s ago)    84s\ncamera-simulator1-8684b659cc-8b4lg    1/1     Running   2 (72s ago)    84s\ncapiyolov8ensemble-57b9ff7d6f-w5hj4   1/1     Running   0              83s\nintel-gpu-plugin-kzxlh                1/1     Running   44 (66m ago)   2d19h\nmqtt-broker-9b597cd94-vxprd           1/1     Running   0              83s\n```\n</code></pre>"},{"location":"use-cases/capi-yolov8-ensemble/getting_started.html#verify-results","title":"Verify Results","text":"<pre><code>After starting C-API YOLOv8 ensemble k8s deployment, you will begin to see the FPS results being published into MQTT-broker serice as it can be seen in the log file via running the command:\n\n```bash\nmake minikube_pod_log\n```\n\n```console\n...\ncid: 20241018174229464559017\nPIPELINE_PROFILE: capi_yolov8_ensemble  DEVICE: CPU\nDC: 0 INPUTSRC: rtsp://camera-simulator:8554/camera_1 USE_VPL: 0 RENDER_MODE: 0 RENDER_PORTRAIT_MODE: 0\nCODEC_TYPE: 1 WINDOW_WIDTH: 1280 WINDOW_HEIGHT: 720 DETECTION_THRESHOLD: 0.5\nRESULT_USE_MQTT=1 MQTT_BROKER=mqtt-broker MQTT_PORT=1883\npublish pipeline results using MQTT: mqtt-broker\n_videoStreamPipeline: rtsp://camera-simulator:8554/camera_1\n_use_onevpl: 0\n_render: 0\n_renderPortrait: 0\nvideoType: 1\n_window_width: 1280\n_window_height: 720\nuse mqtt: 1\nmqttBroker: mqtt-broker\nmqtt port =  1883\n...\nlibva info: VA-API version 1.22.0\nlibva info: Trying to open /usr/lib/x86_64-linux-gnu/dri/iHD_drv_video.so\nlibva info: Found init function __vaDriverInit_1_14\nlibva info: va_openDriver() returns 0\n--------------------------------------------------------------\nOpening Media Pipeline: rtspsrc location=rtsp://camera-simulator:8554/camera_1 ! rtph264depay ! h264parse ! vah264dec ! video/x-raw(memory:VAMemory),format=NV12  ! vapostproc !  video/x-raw, width=416, height=416  ! videoconvert ! video/x-raw,format=RGB ! queue ! appsink drop=1 sync=0\n--------------------------------------------------------------\novmsCofigJsonFilePath: /app/gst-ovms/pipelines/yolov8_ensemble/config-yolov8.json\n...\nStarting thread: 131360809276992\n2 object(s) detected at 2024-10-18.17:42:53\nAvg. Pipeline Throughput FPS: 30.000000\nAvg. Pipeline Latency (ms): 58\nMax. Pipeline Latency (ms): 72\nMin. Pipeline Latency (ms): 36\nConnected to MQTT broker: tcp://mqtt-broker:1883\npublishing messages to MQTT broker: tcp://mqtt-broker:1883\ntopic: capiyolov8ensemble/FPS messages: 30.000000\nwaiting for up to 10 seconds for publishing...\ndelivered token: 1 status code = 0\n2 object(s) detected at 2024-10-18.17:42:55\nAvg. Pipeline Throughput FPS: 20.000000\nAvg. Pipeline Latency (ms): 44\nMax. Pipeline Latency (ms): 55\nMin. Pipeline Latency (ms): 39\npublishing messages to MQTT broker: tcp://mqtt-broker:1883\ntopic: capiyolov8ensemble/FPS messages: 20.000000\nwaiting for up to 10 seconds for publishing...\ndelivered token: 2 status code = 0\n...\n```\n</code></pre>"},{"location":"use-cases/capi-yolov8-ensemble/getting_started.html#shutdown-running-containers","title":"Shutdown Running Containers","text":"<pre><code>```bash\nmake down_capi_yolov8_ensemble\n```\n</code></pre>"},{"location":"use-cases/loss-prevention/advanced.html","title":"Advanced Settings","text":""},{"location":"use-cases/loss-prevention/advanced.html#benchmark-quick-start-command","title":"Benchmark Quick Start command","text":"<p><pre><code>make update-submodules\n</code></pre> <code>update-submodules</code> ensures all submodules are initialized, updated to their latest remote versions, and ready for use.</p> <p><pre><code>make benchmark-quickstart\n</code></pre> The above command would: - Run headless (no display needed: <code>RENDER_MODE=0</code>) - Target GPU by default (<code>WORKLOAD_DIST=workload_to_pipeline_gpu.json</code>) - Run 6 streams, each with different workload (<code>CAMERA_STREAM=camera_to_workload_full.json</code>) - Generate benchmark metrics - Run <code>make consolidate-metrics</code> automatically</p>"},{"location":"use-cases/loss-prevention/advanced.html#understanding-benchmarking-types","title":"Understanding Benchmarking Types","text":""},{"location":"use-cases/loss-prevention/advanced.html#default-benchmark-command","title":"Default benchmark command","text":"<p><pre><code>make update-submodules\n</code></pre> <code>update-submodules</code> ensures all submodules are initialized, updated to their latest remote versions, and ready for use.</p> <p><pre><code>make benchmark\n</code></pre> Runs with: - <code>RENDER_MODE=0</code> - <code>CAMERA_STREAM=camera_to_workload.json</code> - <code>WORKLOAD_DIST=workload_to_pipeline.json</code> - <code>PIPELINE_COUNT=1</code></p> <p>You can override these values through the following Environment Variables.</p> Variable Description Values <code>RENDER_MODE</code> for displaying pipeline and overlay CV metadata 1, 0 <code>PIPELINE_COUNT</code> number of Loss Prevention Docker container instances to launch Ex: 1 <code>WORKLOAD_DIST</code> to define how each workload is assigned to a specific processing unit (CPU, GPU, NPU) workload_to_pipeline_cpu.json, workload_to_pipeline_gpu.json, workload_to_pipeline_gpu-npu.json, workload_to_pipeline_hetero.json, workload_to_pipeline.json <code>CAMERA_STREAM</code> to define camera settings and their associated workloads for the pipeline camera_to_workload.json, camera_to_workload_full.json <p>Note: Higher the <code>PIPELINE_COUNT</code>, higher the stress on the system. Increasing this value will run more parallel pipelines, increasing resource usage and testing system</p>"},{"location":"use-cases/loss-prevention/advanced.html#all-camera_stream-options","title":"All CAMERA_STREAM options","text":"<ul> <li> <p><code>camera_to_workload.json</code></p> Camera_ID Workload cam1 items_in_basket + multi_product_identification cam2 hidden_items + product_switching cam3 fake_scan_detection </li> <li> <p><code>camera_to_workload_full.json</code></p> Camera_ID Workload cam1 items_in_basket cam2 hidden_items cam3 fake_scan_detection cam4 multi_product_identification cam5 product_switching cam6 sweet_heartening </li> </ul>"},{"location":"use-cases/loss-prevention/advanced.html#all-workload_dist-options","title":"All WORKLOAD_DIST options","text":"<ul> <li><code>workload_to_pipeline_cpu.json</code> - All the workloads run on CPU.</li> <li><code>workload_to_pipeline_gpu.json</code> - All the workloads run on GPU.</li> <li><code>workload_to_pipeline_gpu-npu.json</code> - \u00a0 -  items_in_basket, hidden_items, multi_product_identification and product_switching run on GPU, \u00a0 -  fake_scan_detection and sweet_heartening run on NPU.</li> <li><code>workload_to_pipeline_hetero.json</code> -</li> </ul> Workload gvadetect gvaclassify gvainference items_in_basket GPU GPU - hidden_items GPU CPU - fake_scan_detection GPU CPU - multi_product_identification GPU CPU - product_switching GPU GPU - sweet_heartening NPU - NPU <ul> <li><code>workload_to_pipeline.json</code> -  \u00a0  - items_in_basket, multi_product_identification and sweet_heartening run on CPU, \u00a0  - product_switching and hidden_items run on GPU, \u00a0  - fake_scan_detection runs on NPU.</li> </ul> <p>Note</p> <p>The first time running this command may take few minutes. It will build all performance tools containers</p>"},{"location":"use-cases/loss-prevention/advanced.html#benchmark-command-with-environment-variable-overrides","title":"Benchmark command with environment variable overrides","text":"<pre><code>make benchmark WORKLOAD_DIST=workload_to_pipeline_gpu-npu.json CAMERA_STREAM=camera_to_workload_full.json\n</code></pre> <p>Runs with: - <code>RENDER_MODE=0</code> - <code>CAMERA_STREAM=camera_to_workload_full.json</code> - <code>WORKLOAD_DIST=workload_to_pipeline_gpu-npu.json</code> - <code>PIPELINE_COUNT=1</code></p>"},{"location":"use-cases/loss-prevention/advanced.html#see-the-benchmarking-results","title":"See the benchmarking results","text":"<pre><code>make  consolidate-metrics\n\ncat benchmark/metrics.csv\n</code></pre>"},{"location":"use-cases/loss-prevention/advanced.html#other-useful-make-commands","title":"\ud83d\udee0\ufe0f Other Useful Make Commands","text":"<ul> <li><code>make validate-all-configs</code> \u2014 Validate all configuration files</li> <li><code>make clean-images</code> \u2014 Remove dangling Docker images</li> <li><code>make clean-containers</code> \u2014 Remove stopped containers</li> <li><code>make clean-all</code> \u2014 Remove all unused Docker resources</li> </ul>"},{"location":"use-cases/loss-prevention/advanced.html#configuration","title":"\u2699\ufe0f Configuration","text":"<p>The application is highly configurable via JSON files in the <code>configs/</code> directory:</p> <ul> <li><code>camera_to_workload.json</code>: Maps each camera to one or more workloads. To add or remove a camera, edit the <code>lane_config.cameras</code> array in this file. Each camera entry can specify its video source, region of interest, and assigned workloads.<ul> <li>Example:   <pre><code>{\n  \"lane_config\": {\n    \"cameras\": [\n      {\n        \"camera_id\": \"cam1\",\n        \"fileSrc\": \"sample-media/video1.mp4\",              \n        \"workloads\": [\"items_in_basket\", \"multi_product_identification\"],\n        \"region_of_interest\": {\"x\": 100, \"y\": 100, \"x2\": 800, \"y2\": 600}\n      },\n      ...\n    ]\n  }\n}\n</code></pre></li> </ul> </li> <li><code>workload_to_pipeline.json</code>: Maps each workload name to a pipeline definition (sequence of GStreamer elements and models). To add or update a workload, edit the <code>workload_pipeline_map</code> in this file.<ul> <li>Example:   <pre><code>{\n  \"workload_pipeline_map\": {\n    \"items_in_basket\": [\n      {\"type\": \"gvadetect\", \"model\": \"yolo11n\", \"precision\": \"INT8\", \"device\": \"CPU\"},\n      {\"type\": \"gvaclassify\", \"model\": \"efficientnet-v2-b0\", \"precision\": \"INT8\", \"device\": \"CPU\"}\n    ],\n    ...\n  }\n}\n</code></pre></li> </ul> </li> </ul> <p>To try a new camera or workload: 1. Edit <code>configs/camera_to_workload.json</code> to add your camera and assign workloads. 2. Edit <code>configs/workload_to_pipeline.json</code> to define or update the pipeline for your workload. 3. (Optional) Place your video files in the appropriate directory and update the <code>fileSrc</code> path. 4. Re-run the pipeline as described above.</p>"},{"location":"use-cases/loss-prevention/advanced.html#project-structure","title":"\ud83d\udcc1 Project Structure","text":"<ul> <li><code>configs/</code> \u2014 Configuration files (camera/workload mapping, pipeline mapping)</li> <li><code>docker/</code> \u2014 Dockerfiles for downloader and pipeline containers</li> <li><code>docs/</code> \u2014 Documentation (HLD, LLD, system design)</li> <li><code>download-scripts/</code> \u2014 Scripts for downloading models and videos</li> <li><code>src/</code> \u2014 Main source code and pipeline runner scripts</li> <li><code>Makefile</code> \u2014 Build automation and workflow commands</li> </ul>"},{"location":"use-cases/loss-prevention/architecture.html","title":"Loss Prevention Pipeline System Architecture","text":""},{"location":"use-cases/loss-prevention/architecture.html#table-of-contents","title":"Table of Contents","text":"<ol> <li>System Overview</li> <li>Architecture Diagrams</li> <li>Configuration Management</li> <li>System Benefits</li> </ol>"},{"location":"use-cases/loss-prevention/architecture.html#system-overview","title":"System Overview","text":"<p>The Loss Prevention Pipeline System is a comprehensive video analytics platform designed for retail loss prevention using Intel hardware acceleration. The system processes multiple camera feeds simultaneously, applying AI-powered detection and classification models to identify potential security incidents.</p>"},{"location":"use-cases/loss-prevention/architecture.html#key-features","title":"Key Features","text":"<ul> <li>Multi-camera Support: Simultaneous processing of multiple camera feeds</li> <li>AI-Powered Analytics: Object detection and classification using YOLO and EfficientNet models</li> <li>Intel Hardware Optimization: Leverages CPU, GPU, and NPU acceleration</li> <li>Real-time Processing: GStreamer-based pipeline for low-latency video processing</li> <li>Flexible Configuration: JSON-based configuration for cameras, workloads, and pipelines</li> <li>Performance Monitoring: Comprehensive metrics collection and analysis</li> <li>Containerized Deployment: Docker-based deployment with Intel DL Streamer</li> </ul>"},{"location":"use-cases/loss-prevention/architecture.html#architecture-diagrams","title":"Architecture Diagrams","text":""},{"location":"use-cases/loss-prevention/architecture.html#component-architecture","title":"Component Architecture","text":"<pre><code>graph TB\n    subgraph \"Configuration Layer\"\n        CWC[camera_to_workload.json]\n        WPC[workload_to_pipeline.json]\n    end\n\n    subgraph \"Pipeline Generation Layer\"\n        PGL[Pipeline Generator]\n        CL[Config Loader]\n    end\n\n    subgraph \"Execution Layer\"\n        GST[GStreamer Runtime]\n        ENV[Environment Manager]\n        DEV[Device Manager]\n    end\n\n    subgraph \"Processing Components\"\n        subgraph \"Source\"\n            FS[filesrc]\n            DEC[decodebin]\n        end\n\n        subgraph \"AI Processing\"\n            ATT[gvaattachroi]\n            DET[gvadetect]\n            TRK[gvatrack]\n            CLS[gvaclassify]\n        end\n\n        subgraph \"Output\"\n            CNV[gvametaconvert]\n            PUB[gvametapublish]\n            TEE[tee]\n            SINK[fakesink/fpsdisplaysink]\n        end\n    end\n\n    subgraph \"Performance Tools\"\n        BM[Benchmark Scripts]\n        MC[Metrics Collection]\n        AN[Analysis Tools]\n    end\n\n    CWC --&gt; CL\n    WPC --&gt; CL\n    CL --&gt; PGL\n\n    PGL --&gt; GST\n    GST --&gt; ENV\n    ENV --&gt; DEV\n\n    GST --&gt; FS\n    FS --&gt; DEC\n    DEC --&gt; ATT\n    ATT --&gt; DET\n    DET --&gt; TRK\n    TRK --&gt; CLS\n    CLS --&gt; CNV\n    CNV --&gt; PUB\n    CNV --&gt; TEE\n    TEE --&gt; SINK\n\n    GST --&gt; BM\n    BM --&gt; MC\n    MC --&gt; AN\n</code></pre>"},{"location":"use-cases/loss-prevention/architecture.html#configuration-flow","title":"Configuration Flow","text":"<pre><code>flowchart LR\n    subgraph \"Configuration Files\"\n        CAM[camera_to_workload.json \\n\u2022 Camera definitions \\n\u2022 Workload assignments \\n\u2022 ROI specifications]\n        WORK[workload_to_pipeline.json \\n\u2022 Pipeline steps \\n\u2022 Model configurations \\n\u2022 Device assignments]\n    end\n\n    subgraph \"Processing\"\n        LOAD[Config Loader]\n        NORM[Normalization]\n        MERGE[Pipeline Consolidation]\n    end\n\n    subgraph \"Output\"\n        GST[GStreamer Command]\n        EXEC[Pipeline Execution]\n    end\n\n    CAM --&gt; LOAD\n    WORK --&gt; LOAD\n    LOAD --&gt; NORM\n    NORM --&gt; MERGE\n    MERGE --&gt; GST\n    GST --&gt; EXEC\n</code></pre>"},{"location":"use-cases/loss-prevention/architecture.html#component-architecture_1","title":"Component Architecture","text":""},{"location":"use-cases/loss-prevention/architecture.html#core-components","title":"Core Components","text":""},{"location":"use-cases/loss-prevention/architecture.html#1-pipeline-generator-gst-pipeline-generatorpy","title":"1. Pipeline Generator (<code>gst-pipeline-generator.py</code>)","text":"<p>Purpose: Dynamic GStreamer pipeline generation based on configuration Key Functions: - Configuration loading and validation - Pipeline signature generation for optimization - Dynamic element creation based on workload requirements - Device-specific environment variable management</p>"},{"location":"use-cases/loss-prevention/architecture.html#2-configuration-management","title":"2. Configuration Management","text":"<p>Files: - <code>camera_to_workload.json</code>: Camera definitions and workload assignments - <code>workload_to_pipeline.json</code>: Pipeline step definitions</p>"},{"location":"use-cases/loss-prevention/architecture.html#3-gstreamer-elements","title":"3. GStreamer Elements","text":"<p>Source Elements: - <code>filesrc</code>: Video file input - <code>decodebin</code>: Automatic video decoding</p> <p>AI Processing Elements: - <code>gvaattachroi</code>: Region of Interest attachment - <code>gvadetect</code>: Object detection using YOLO models - <code>gvatrack</code>: Object tracking across frames - <code>gvaclassify</code>: Object classification using EfficientNet</p> <p>Output Elements: - <code>gvametaconvert</code>: Metadata format conversion - <code>gvametapublish</code>: Results publishing to files - <code>tee</code>: Pipeline branching for multiple outputs</p>"},{"location":"use-cases/loss-prevention/architecture.html#environment-configuration","title":"Environment Configuration","text":"<p>Device-specific environment files: - <code>/res/all-cpu.env</code>: CPU optimization settings - <code>/res/all-gpu.env</code>: GPU acceleration settings - <code>/res/all-npu.env</code>: NPU configuration</p>"},{"location":"use-cases/loss-prevention/architecture.html#configuration-management","title":"Configuration Management","text":""},{"location":"use-cases/loss-prevention/architecture.html#camera-configuration","title":"Camera Configuration","text":"<pre><code>{\n  \"lane_config\": {\n    \"cameras\": [\n      {\n        \"camera_id\": \"cam1\",\n        \"fileSrc\": \"video-file.mp4\",\n        \"width\": 1920,\n        \"fps\": 15,\n        \"workloads\": [\"detection\", \"classification\"],\n        \"region_of_interest\": {\n          \"x\": 0.1,\n          \"y\": 0.1,\n          \"x2\": 0.9,\n          \"y2\": 0.9\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"use-cases/loss-prevention/architecture.html#workload-configuration","title":"Workload Configuration","text":"<pre><code>{\n  \"workload_pipeline_map\": {\n    \"detection\": [\n      {\n        \"type\": \"gvadetect\",\n        \"model\": \"yolov5s\",\n        \"device\": \"GPU\",\n        \"precision\": \"FP16\"\n      }\n    ],\n    \"classification\": [\n      {\n        \"type\": \"gvaclassify\",\n        \"model\": \"efficientnet-b0\",\n        \"device\": \"NPU\",\n        \"precision\": \"INT8\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"use-cases/loss-prevention/architecture.html#system-benefits","title":"System Benefits","text":""},{"location":"use-cases/loss-prevention/architecture.html#performance-advantages","title":"Performance Advantages","text":"<ul> <li>Hardware Optimization: Native Intel hardware acceleration</li> <li>Pipeline Efficiency: Optimized GStreamer pipelines with minimal overhead</li> <li>Parallel Processing: Multi-camera, multi-workload concurrent execution</li> <li>Resource Management: Intelligent device allocation and load balancing</li> </ul>"},{"location":"use-cases/loss-prevention/architecture.html#scalability-features","title":"Scalability Features","text":"<ul> <li>Configuration-Driven: Easy addition of new cameras and workloads</li> <li>Modular Design: Independent processing pipelines</li> <li>Container-Based: Horizontal scaling with Docker orchestration</li> <li>Performance Monitoring: Real-time metrics for optimization</li> </ul> <p>This architecture documentation provides a comprehensive overview of the Loss Prevention Pipeline System. For implementation details, refer to the source code and configuration files in the repository.</p>"},{"location":"use-cases/loss-prevention/getting_started.html","title":"Getting Started","text":""},{"location":"use-cases/loss-prevention/getting_started.html#step-by-step-instructions","title":"Step by step instructions:","text":"<ol> <li> <p>Download the models using download_models/downloadModels.sh</p> <pre><code>make download-models\n</code></pre> </li> <li> <p>Update github submodules</p> <pre><code>make update-submodules\n</code></pre> </li> <li> <p>Download sample videos used by the performance tools</p> <pre><code>make download-sample-videos\n</code></pre> </li> <li> <p>Run the LP application</p> <pre><code>make run-render-mode\n</code></pre> <p>NOTE:- User can directly run single make command that internally called all above command and run the Loss Prevention application.</p> </li> <li> <p>Run Loss Prevention appliaction with single command.   </p> <p><pre><code>make run-lp\n</code></pre> 6. View the Dynamically Generated GStreamer Pipeline.</p> <p>Since the GStreamer pipeline is generated dynamically based on the provided configuration(camera_to_workload and workload_to_pipeline json), the pipeline.sh file gets updated every time the user runs make run-lp or make benchmark. This ensures that the pipeline reflects the latest changes. <pre><code>src/pipelines/pipeline.sh\n</code></pre></p> </li> <li> <p>Verify Docker containers</p> <p><pre><code>docker ps --all\n</code></pre> Result: <pre><code>NAMES                    STATUS                     IMAGE\nsrc-pipeline-runner-1    Up 17 seconds (healthy)    pipeline-runner:lp\nmodel-downloader         Exited(0) 17 seconds       model-downloader:lp\n</code></pre></p> </li> <li> <p>Verify Results</p> <p>After starting Loss Prevention you will begin to see result files being written into the results/ directory. Here are example outputs from the 3 log files.</p> <p>gst-launch__gst.log <pre><code>/GstPipeline:pipeline0/GstGvaWatermark:gvawatermark0/GstCapsFilter:capsfilter1: caps = video/x-raw(memory:VASurface), format=(string)RGBA\n/GstPipeline:pipeline0/GstFPSDisplaySink:fpsdisplaysink0/GstXImageSink:ximagesink0: sync = true\nGot context from element 'vaapipostproc1': gst.vaapi.Display=context, gst.vaapi.Display=(GstVaapiDisplay)\"\\(GstVaapiDisplayGLX\\)\\ vaapidisplayglx0\", gst.vaapi.Display.GObject=(GstObject)\"\\(GstVaapiDisplayGLX\\)\\ vaapidisplayglx0\";\nProgress: (open) Opening Stream\nPipeline is PREROLLED ...\nPrerolled, waiting for progress to finish...\nProgress: (connect) Connecting to rtsp://localhost:8554/camera_0\nProgress: (open) Retrieving server options\nProgress: (open) Retrieving media info\nProgress: (request) SETUP stream 0\n</code></pre> <p>pipeline_gst.log <pre><code>14.58\n14.58\n15.47\n15.47\n15.10\n15.10\n14.60\n14.60\n14.88\n14.88\n</code></pre> <p>r_gst.jsonl <pre><code>{\n    \"objects\": [\n        {\n            \"detection\": {\n                \"bounding_box\": {\n                    \"x_max\": 0.7873924346958825,\n                    \"x_min\": 0.6701603093745723,\n                    \"y_max\": 0.7915918938548927,\n                    \"y_min\": 0.14599881349270305\n                },\n                \"confidence\": 0.8677337765693665,\n                \"label\": \"apple\",\n                \"label_id\": 39\n            },\n            \"h\": 697,\n            \"region_id\": 610,\n            \"roi_type\": \"apple\",\n            \"w\": 225,\n            \"x\": 1287,\n            \"y\": 158\n        },\n        {\n            \"detection\": {\n                \"bounding_box\": {\n                    \"x_max\": 0.3221945836811382,\n                    \"x_min\": 0.19950163649114616,\n                    \"y_max\": 0.7924592239981934,\n                    \"y_min\": 0.1336837231479251\n                },\n                \"confidence\": 0.8625879287719727,\n                \"label\": \"apple\",\n                \"label_id\": 39\n            },\n            \"h\": 711,\n            \"region_id\": 611,\n            \"roi_type\": \"apple\",\n            \"w\": 236,\n            \"x\": 383,\n            \"y\": 144\n        },\n        {\n            \"detection\": {\n                \"bounding_box\": {\n                    \"x_max\": 0.5730873789069046,\n                    \"x_min\": 0.42000878963365595,\n                    \"y_max\": 0.9749763191740435,\n                    \"y_min\": 0.12431765065780453\n                },\n                \"confidence\": 0.854443371295929,\n                \"label\": \"apple\",\n                \"label_id\": 39\n            },\n            \"h\": 919,\n            \"region_id\": 612,\n            \"roi_type\": \"apple\",\n            \"w\": 294,\n            \"x\": 806,\n            \"y\": 134\n        }\n    ],\n    \"resolution\": {\n        \"height\": 1080,\n        \"width\": 1920\n    },\n    \"timestamp\": 755106652\n}\n</code></pre> <li> <p>Stop the demo using docker compose down     <pre><code>make down-lp\n</code></pre></p> </li>"},{"location":"use-cases/loss-prevention/getting_started.html#proceed-to-advanced-settings","title":"Proceed to Advanced Settings","text":""},{"location":"use-cases/loss-prevention/loss-prevention.html","title":"Intel\u00ae Loss Prevention Reference Package","text":""},{"location":"use-cases/loss-prevention/loss-prevention.html#overview","title":"Overview","text":"<p>As computer vision technology becomes more mainstream in industrial and retail settings, using it for loss prevention is becoming increasingly complex. These vision workloads are substantial and require multiple stages of processing. For example, a typical loss prevention pipeline might capture video data, define regions of interest, implement tracking to monitor which products customers interact with, analyze the data using models like YOLOv5 and EfficientNet, and then post-process it to generate metadata that highlights which products are being purchased or potentially stolen. This is just one example of how such models and workflows can be utilized.</p> <p>Implementing loss prevention solutions in retail isn't straightforward. Retailers, independent software vendors (ISVs), and system integrators (SIs) need a solid understanding of both hardware and software, as well as the costs involved in setting up and scaling these systems. Given the data-intensive nature of vision workloads, systems must be carefully designed, built, and deployed with numerous considerations in mind. Effectively combating shrinkage requires the right mix of hardware, software, and optimized configurations.</p> <p>The Intel\u00ae Loss Prevention Reference Package is designed to help with this. It provides the essential components needed to develop and deploy a loss prevention solution using Intel\u00ae hardware, software, and open-source tools. This reference implementation includes a pre-configured pipeline that's optimized for Intel\u00ae hardware, simplifying the setup of an effective computer vision-based loss prevention system for retailers.</p>"},{"location":"use-cases/loss-prevention/loss-prevention.html#next-steps","title":"Next Steps","text":"<p>To begin using the loss prevention solution you can follow the Getting Started Guide. </p>"},{"location":"use-cases/order-accuracy/advanced.html","title":"Advanced Settings","text":""},{"location":"use-cases/order-accuracy/advanced.html#benchmark-quick-start-command","title":"Benchmark Quick Start command","text":"<p><pre><code>make update-submodules\n</code></pre> <code>update-submodules</code> ensures all submodules are initialized, updated to their latest remote versions, and ready for use.</p> <p><pre><code>make benchmark-quickstart\n</code></pre> The above command would: - Run headless (no display needed: <code>RENDER_MODE=0</code>) - Target GPU by default (<code>DEVICE_ENV=res/all-gpu.env</code>) - Generate benchmark metrics - Run <code>make consolidate-metrics</code> automatically</p>"},{"location":"use-cases/order-accuracy/advanced.html#understanding-benchmarking-types","title":"Understanding Benchmarking Types","text":""},{"location":"use-cases/order-accuracy/advanced.html#default-benchmark-command","title":"Default benchmark command","text":"<p><pre><code>make update-submodules\n</code></pre> <code>update-submodules</code> ensures all submodules are initialized, updated to their latest remote versions, and ready for use.</p> <p><pre><code>make benchmark\n</code></pre> Runs with: - <code>RENDER_MODE=0</code> - <code>DEVICE_ENV=res/all-cpu.env</code> - <code>PIPELINE_COUNT=1</code></p> <p>You can override these values through the following Environment Variables.</p> Variable Description Values <code>RENDER_MODE</code> for displaying pipeline and overlay CV metadata 1, 0 <code>PIPELINE_COUNT</code> number of Loss Prevention Docker container instances to launch Ex: 1 <code>DEVICE_ENV</code> path to device specific environment file that will be loaded into the pipeline container res/all-cpu.env, res/all-gpu.env, res/all-npu.env, res/all-dgpu.env"},{"location":"use-cases/order-accuracy/advanced.html#benchmark-command-for-gpu","title":"Benchmark command for GPU","text":"<pre><code>make DEVICE_ENV=res/all-gpu.env benchmark\n</code></pre>"},{"location":"use-cases/order-accuracy/advanced.html#benchmark-command-for-npu","title":"Benchmark command for NPU","text":"<pre><code>make DEVICE_ENV=res/all-npu.env benchmark\n</code></pre>"},{"location":"use-cases/order-accuracy/advanced.html#see-the-benchmarking-results","title":"See the benchmarking results.","text":"<pre><code>make  consolidate-metrics\n\ncat benchmark/metrics.csv\n</code></pre>"},{"location":"use-cases/order-accuracy/advanced.html#benchmark-stream-density","title":"Benchmark Stream Density","text":"<p>To test the maximum amount of Order Accuracy containers/pipelines that can run on a given system you can use the TARGET_FPS environment variable. Default is to find the container threshold over 7.95 FPS with the run-pipeline.sh pipeline. You can override these values through Environment Variables.</p> <p>List of EVs:</p> Variable Description Values <code>TARGET_FPS</code> threshold value for FPS to consider a valid stream Ex. 7.95 <code>OOM_PROTECTION</code> flag to enable/disable OOM checks before scaling the pipeline (enabled by default) 1, 0 <p>Note:</p> <p>An OOM crash occurs when a system or application tries to use more memory (RAM) than is available, causing the operating system to forcibly terminate processes to free up memory. If <code>OOM_PROTECTION</code> is set to 0, the system may crash or become unresponsive, requiring a hard reboot. </p> <pre><code>make benchmark-stream-density\n</code></pre> <p>You can check the output results for performance metrics in the <code>results</code> folder at the root level. Also, the stream density script will output the results in the console:</p>"},{"location":"use-cases/order-accuracy/advanced.html#change-the-target-fps-value","title":"Change the Target FPS value:","text":"<pre><code>make TARGET_FPS=6.5 benchmark-stream-density\n</code></pre> <p>Alternatively you can directly call the benchmark.py. This enables you to take advantage of all performance tools parameters. More details about the performance tools can be found HERE</p> <pre><code>cd performance-tools/benchmark-scripts &amp;&amp; python benchmark.py --compose_file ../../src/docker-compose.yml --target_fps 7\n</code></pre>"},{"location":"use-cases/order-accuracy/advanced.html#other-useful-make-commands","title":"\ud83d\udee0\ufe0f Other Useful Make Commands.","text":"<ul> <li><code>make clean-images</code> \u2014 Remove dangling Docker images</li> <li><code>make clean-models</code> \u2014 Remove all the downloaded models from the system</li> <li><code>make clean-all</code> \u2014 Remove all unused Docker resources</li> </ul>"},{"location":"use-cases/order-accuracy/advanced.html#project-structure","title":"\ud83d\udcc1 Project Structure","text":"<ul> <li><code>configs/</code> \u2014 Configuration files (workload videos URLs)</li> <li><code>docker/</code> \u2014 Dockerfiles for downloader and pipeline containers</li> <li><code>download-scripts/</code> \u2014 Scripts for downloading models and videos</li> <li><code>src/</code> \u2014 Main source code and pipeline runner scripts</li> <li><code>Makefile</code> \u2014 Build automation and workflow commands</li> </ul>"},{"location":"use-cases/order-accuracy/getting_started.html","title":"Getting Started","text":""},{"location":"use-cases/order-accuracy/getting_started.html#step-by-step-instructions","title":"Step by step instructions:","text":"<ol> <li> <p>Download the models using download_models/downloadModels.sh</p> <pre><code>make download-models\n</code></pre> </li> <li> <p>Update github submodules</p> <pre><code>make update-submodules\n</code></pre> </li> <li> <p>Download sample videos used by the performance tools</p> <pre><code>make download-sample-videos\n</code></pre> </li> <li> <p>Run the order accuracy application</p> <pre><code>make run-render-mode\n</code></pre> <p>NOTE:- User can directly run single make command that internally called all above command and run the Order Accuracy application.</p> </li> <li> <p>Run Order Accuracy appliaction with single command.   </p> <pre><code>make run-demo\n</code></pre> </li> <li> <p>Verify Docker containers</p> <p><pre><code>docker ps --all\n</code></pre> Result: <pre><code>NAMES                    STATUS                     IMAGE\nsrc-ClientGst-1          Up 17 seconds (healthy)    dlstreamer:dev\nmodel-downloader         Exited(0) 17 seconds       model-downloader:latest\n</code></pre></p> </li> <li> <p>Verify Results</p> <p>After starting Order Accuracy you will begin to see result files being written into the results/ directory. Here are example outputs from the 3 log files.</p> <p>gst-launch__gst.log <pre><code>/GstPipeline:pipeline0/GstGvaWatermark:gvawatermark0/GstCapsFilter:capsfilter1: caps = video/x-raw(memory:VASurface), format=(string)RGBA\n/GstPipeline:pipeline0/GstFPSDisplaySink:fpsdisplaysink0/GstXImageSink:ximagesink0: sync = true\nGot context from element 'vaapipostproc1': gst.vaapi.Display=context, gst.vaapi.Display=(GstVaapiDisplay)\"\\(GstVaapiDisplayGLX\\)\\ vaapidisplayglx0\", gst.vaapi.Display.GObject=(GstObject)\"\\(GstVaapiDisplayGLX\\)\\ vaapidisplayglx0\";\nProgress: (open) Opening Stream\nPipeline is PREROLLED ...\nPrerolled, waiting for progress to finish...\nProgress: (connect) Connecting to rtsp://localhost:8554/camera_0\nProgress: (open) Retrieving server options\nProgress: (open) Retrieving media info\nProgress: (request) SETUP stream 0\n</code></pre> <p>pipeline_gst.log <pre><code>14.58\n14.58\n15.47\n15.47\n15.10\n15.10\n14.60\n14.60\n14.88\n14.88\n</code></pre> <p>r_gst.jsonl <li> <p>Stop the demo using docker compose down     <pre><code>make down\n</code></pre></p> </li>"},{"location":"use-cases/order-accuracy/getting_started.html#proceed-to-advanced-settings","title":"Proceed to Advanced Settings","text":""},{"location":"use-cases/order-accuracy/order-accuracy.html","title":"Intel\u00ae Order Accuracy Reference Package","text":""},{"location":"use-cases/order-accuracy/order-accuracy.html#overview","title":"Overview","text":"<p>As computer vision technology becomes more mainstream in industrial and retail settings, using it for order accuracy in quick service restaurants is becoming increasingly complex. These vision workloads are substantial and require multiple stages of processing. For example, a typical order accuracy pipeline might capture video data and use an object detection model like YOLO V11 to identify items such as sandwiches, fries, and drinks placed on trays or tables. The results are then post-processed to generate metadata highlighting which food items were served.</p> <p>Implementing order accuracy solutions in retail isn't straightforward. Retailers, independent software vendors (ISVs), and system integrators (SIs) need a solid understanding of both hardware and software, as well as the costs involved in setting up and scaling these systems. Given the data-intensive nature of vision workloads, systems must be carefully designed, built, and deployed with numerous considerations in mind. Effectively combating shrinkage requires the right mix of hardware, software, and optimized configurations.</p> <p>The Intel\u00ae Order Accuracy Reference Package is designed to help with this. It provides the essential components needed to develop and deploy a order accuracy  solution using Intel\u00ae hardware, software, and open-source tools. This reference implementation includes a pre-configured pipeline that's optimized for Intel\u00ae hardware, simplifying the setup of an effective computer vision-based order accuracy system for retailers.</p>"},{"location":"use-cases/order-accuracy/order-accuracy.html#next-steps","title":"Next Steps","text":"<p>To begin using the order accuracy solution you can follow the Getting Started Guide. </p>"}]}